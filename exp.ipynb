{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************************\n",
      "*           Training Mode is  Downstream      *\n",
      "***********************************************\n",
      "Dataset Name:  /mnt/nas125_vol2/kanggilpark/child/PedXnet_Code_Factory/datasets\n",
      "---------- Model ----------\n",
      "Resume From:  \n",
      "Output Save To:  outputs\n",
      "Visible GPUs:  2\n",
      "---------- Optimizer ----------\n",
      "Learning Rate:  0.0005\n",
      "Batchsize:  72\n",
      "Loading dataset ....\n",
      "Train [Total]  number =  1832\n",
      "Valid [Total]  number =  226\n",
      "Creating model  : Downtask_General_Fracture\n",
      "Pretrained model: \n",
      "Number of Learnable Params: 26650561\n",
      "General_Fracture_Model(\n",
      "  (encoder): ResNet_Feature_Extractor(\n",
      "    (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (fc1): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (drop1): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (drop2): Dropout(p=0.5, inplace=False)\n",
      "  (head): Linear(in_features=1024, out_features=1, bias=True)\n",
      ")\n",
      "Start training for 500 epochs\n",
      "Freeze encoder ...!!\n",
      "Epoch: [0]  [ 0/25]  eta: 0:05:15  lr: 0.000000  loss: 0.7020 (0.7020)  BCE_Loss: 0.7020 (0.7020)  time: 12.6366  data: 11.3420  max mem: 1056\n",
      "Epoch: [0]  [ 1/25]  eta: 0:02:33  lr: 0.000000  loss: 0.6822 (0.6921)  BCE_Loss: 0.6822 (0.6921)  time: 6.4062  data: 5.6711  max mem: 1092\n",
      "Epoch: [0]  [ 2/25]  eta: 0:01:39  lr: 0.000000  loss: 0.7020 (0.7000)  BCE_Loss: 0.7020 (0.7000)  time: 4.3194  data: 3.7807  max mem: 1092\n",
      "Epoch: [0]  [ 3/25]  eta: 0:01:12  lr: 0.000000  loss: 0.6943 (0.6986)  BCE_Loss: 0.6943 (0.6986)  time: 3.2870  data: 2.8356  max mem: 1092\n",
      "Epoch: [0]  [ 4/25]  eta: 0:00:55  lr: 0.000000  loss: 0.6943 (0.6935)  BCE_Loss: 0.6943 (0.6935)  time: 2.6653  data: 2.2685  max mem: 1092\n",
      "Epoch: [0]  [ 5/25]  eta: 0:00:45  lr: 0.000000  loss: 0.6822 (0.6911)  BCE_Loss: 0.6822 (0.6911)  time: 2.2530  data: 1.8904  max mem: 1092\n",
      "Epoch: [0]  [ 6/25]  eta: 0:00:37  lr: 0.000000  loss: 0.6873 (0.6906)  BCE_Loss: 0.6873 (0.6906)  time: 1.9583  data: 1.6204  max mem: 1092\n",
      "Epoch: [0]  [ 7/25]  eta: 0:00:31  lr: 0.000000  loss: 0.6873 (0.6939)  BCE_Loss: 0.6873 (0.6939)  time: 1.7303  data: 1.4178  max mem: 1092\n",
      "Epoch: [0]  [ 8/25]  eta: 0:00:40  lr: 0.000000  loss: 0.6943 (0.6941)  BCE_Loss: 0.6943 (0.6941)  time: 2.3797  data: 2.0815  max mem: 1092\n",
      "Epoch: [0]  [ 9/25]  eta: 0:00:36  lr: 0.000000  loss: 0.6943 (0.6964)  BCE_Loss: 0.6943 (0.6964)  time: 2.2525  data: 1.9705  max mem: 1092\n",
      "Epoch: [0]  [10/25]  eta: 0:00:30  lr: 0.000000  loss: 0.6955 (0.6984)  BCE_Loss: 0.6955 (0.6984)  time: 2.0634  data: 1.7913  max mem: 1092\n",
      "Epoch: [0]  [11/25]  eta: 0:00:26  lr: 0.000000  loss: 0.6955 (0.6993)  BCE_Loss: 0.6955 (0.6993)  time: 1.9032  data: 1.6421  max mem: 1092\n",
      "Epoch: [0]  [12/25]  eta: 0:00:22  lr: 0.000000  loss: 0.6955 (0.6988)  BCE_Loss: 0.6955 (0.6988)  time: 1.7678  data: 1.5157  max mem: 1092\n",
      "Epoch: [0]  [13/25]  eta: 0:00:19  lr: 0.000000  loss: 0.6943 (0.6969)  BCE_Loss: 0.6943 (0.6969)  time: 1.6549  data: 1.4075  max mem: 1092\n",
      "Epoch: [0]  [14/25]  eta: 0:00:17  lr: 0.000000  loss: 0.6943 (0.6956)  BCE_Loss: 0.6943 (0.6956)  time: 1.5568  data: 1.3136  max mem: 1092\n",
      "Epoch: [0]  [15/25]  eta: 0:00:14  lr: 0.000000  loss: 0.6943 (0.6965)  BCE_Loss: 0.6943 (0.6965)  time: 1.4676  data: 1.2315  max mem: 1092\n",
      "Epoch: [0]  [16/25]  eta: 0:00:16  lr: 0.000000  loss: 0.6943 (0.6961)  BCE_Loss: 0.6943 (0.6961)  time: 1.8111  data: 1.5817  max mem: 1092\n",
      "Epoch: [0]  [17/25]  eta: 0:00:14  lr: 0.000000  loss: 0.6943 (0.6965)  BCE_Loss: 0.6943 (0.6965)  time: 1.8144  data: 1.5872  max mem: 1092\n",
      "Epoch: [0]  [18/25]  eta: 0:00:12  lr: 0.000000  loss: 0.6955 (0.6971)  BCE_Loss: 0.6955 (0.6971)  time: 1.7290  data: 1.5037  max mem: 1092\n",
      "Epoch: [0]  [19/25]  eta: 0:00:09  lr: 0.000000  loss: 0.6955 (0.6977)  BCE_Loss: 0.6955 (0.6977)  time: 1.6494  data: 1.4285  max mem: 1092\n",
      "Epoch: [0]  [20/25]  eta: 0:00:07  lr: 0.000000  loss: 0.6943 (0.6975)  BCE_Loss: 0.6943 (0.6975)  time: 1.0261  data: 0.8614  max mem: 1092\n",
      "Epoch: [0]  [21/25]  eta: 0:00:06  lr: 0.000000  loss: 0.6955 (0.6981)  BCE_Loss: 0.6955 (0.6981)  time: 1.0246  data: 0.8614  max mem: 1092\n",
      "Epoch: [0]  [22/25]  eta: 0:00:04  lr: 0.000000  loss: 0.6955 (0.6981)  BCE_Loss: 0.6955 (0.6981)  time: 1.0247  data: 0.8614  max mem: 1092\n",
      "Epoch: [0]  [23/25]  eta: 0:00:02  lr: 0.000000  loss: 0.6955 (0.6976)  BCE_Loss: 0.6955 (0.6976)  time: 1.0245  data: 0.8614  max mem: 1092\n",
      "Epoch: [0]  [24/25]  eta: 0:00:01  lr: 0.000000  loss: 0.6971 (0.6983)  BCE_Loss: 0.6971 (0.6983)  time: 1.3050  data: 1.1421  max mem: 1092\n",
      "Epoch: [0] Total time: 0:00:39 (1.5825 s / it)\n",
      "Averaged train_stats:  {'lr': 0.0, 'loss': 0.6983116, 'BCE_Loss': 0.6983116}\n",
      "Valid:  [0/4]  eta: 0:00:48  loss: 0.6864 (0.6864)  BCE_Loss: 0.6864 (0.6864)  time: 12.1447  data: 11.9758  max mem: 1092\n",
      "Valid:  [1/4]  eta: 0:00:18  loss: 0.6864 (0.6894)  BCE_Loss: 0.6864 (0.6894)  time: 6.1398  data: 5.9879  max mem: 1092\n",
      "Valid:  [2/4]  eta: 0:00:08  loss: 0.6923 (0.6929)  BCE_Loss: 0.6923 (0.6929)  time: 4.1497  data: 3.9919  max mem: 1092\n",
      "Valid:  [3/4]  eta: 0:00:03  loss: 0.6923 (0.6948)  BCE_Loss: 0.6923 (0.6948)  time: 3.1177  data: 2.9940  max mem: 1092\n",
      "Valid: Total time: 0:00:12 (3.1547 s / it)\n",
      "Averaged valid_stats:  {'loss': 0.6947626, 'BCE_Loss': 0.6947626, 'auc': 0.4675386, 'f1': 0.6666667, 'acc': 0.5, 'sen': 1.0, 'spe': 0.0}\n",
      "Freeze encoder ...!!\n",
      "Epoch: [1]  [ 0/25]  eta: 0:05:04  lr: 0.000000  loss: 0.6846 (0.6846)  BCE_Loss: 0.6846 (0.6846)  time: 12.1820  data: 11.9952  max mem: 1092\n",
      "Epoch: [1]  [ 1/25]  eta: 0:02:27  lr: 0.000000  loss: 0.6846 (0.6876)  BCE_Loss: 0.6846 (0.6876)  time: 6.1607  data: 5.9976  max mem: 1092\n",
      "Epoch: [1]  [ 2/25]  eta: 0:01:35  lr: 0.000000  loss: 0.6907 (0.6893)  BCE_Loss: 0.6907 (0.6893)  time: 4.1624  data: 3.9984  max mem: 1092\n",
      "Epoch: [1]  [ 3/25]  eta: 0:01:09  lr: 0.000000  loss: 0.6907 (0.6925)  BCE_Loss: 0.6907 (0.6925)  time: 3.1592  data: 2.9989  max mem: 1092\n",
      "Epoch: [1]  [ 4/25]  eta: 0:00:53  lr: 0.000000  loss: 0.6928 (0.6940)  BCE_Loss: 0.6928 (0.6940)  time: 2.5558  data: 2.3991  max mem: 1092\n",
      "Epoch: [1]  [ 5/25]  eta: 0:00:43  lr: 0.000000  loss: 0.6928 (0.6950)  BCE_Loss: 0.6928 (0.6950)  time: 2.1610  data: 1.9993  max mem: 1092\n",
      "Epoch: [1]  [ 6/25]  eta: 0:00:35  lr: 0.000000  loss: 0.6928 (0.6943)  BCE_Loss: 0.6928 (0.6943)  time: 1.8791  data: 1.7137  max mem: 1092\n",
      "Epoch: [1]  [ 7/25]  eta: 0:00:29  lr: 0.000000  loss: 0.6928 (0.6946)  BCE_Loss: 0.6928 (0.6946)  time: 1.6595  data: 1.4995  max mem: 1092\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 282, in <module>\n",
      "    main(args)\n",
      "  File \"train.py\", line 220, in main\n",
      "    train_stats = train_Downtask_General_Fracture(model, criterion, data_loader_train, optimizer, device, epoch, args.print_freq, args.batch_size, args.gradual_unfreeze)\n",
      "  File \"/mnt/nas125_vol2/kanggilpark/child/PedXnet_Code_Factory/engine.py\", line 464, in train_Downtask_General_Fracture\n",
      "    for batch_data in metric_logger.log_every(data_loader, print_freq, header):\n",
      "  File \"/mnt/nas125_vol2/kanggilpark/child/PedXnet_Code_Factory/utils.py\", line 119, in log_every\n",
      "    for obj in iterable:\n",
      "  File \"/home/pkg777774/anaconda3/envs/child_x/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 435, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/pkg777774/anaconda3/envs/child_x/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1068, in _next_data\n",
      "    idx, data = self._get_data()\n",
      "  File \"/home/pkg777774/anaconda3/envs/child_x/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1024, in _get_data\n",
      "    success, data = self._try_get_data()\n",
      "  File \"/home/pkg777774/anaconda3/envs/child_x/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 872, in _try_get_data\n",
      "    data = self._data_queue.get(timeout=timeout)\n",
      "  File \"/home/pkg777774/anaconda3/envs/child_x/lib/python3.8/queue.py\", line 179, in get\n",
      "    self.not_empty.wait(remaining)\n",
      "  File \"/home/pkg777774/anaconda3/envs/child_x/lib/python3.8/threading.py\", line 306, in wait\n",
      "    gotit = waiter.acquire(True, timeout)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python train.py \\\n",
    "--data_set 'General_Fracture' \\\n",
    "--model_name 'Downtask_General_Fracture' \\\n",
    "--batch-size 72 \\\n",
    "--num_workers 8 \\\n",
    "--multi-gpu-mode 'Single' \\\n",
    "--cuda-visible-devices '2' \\\n",
    "--gradual_unfreeze true \\\n",
    "--print-freq 1\\\n",
    "--output_dir outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************************\n",
      "*           Training Mode is  Downstream      *\n",
      "***********************************************\n",
      "Dataset Name:  /mnt/nas125_vol2/kanggilpark/child/bone_age/data\n",
      "---------- Model ----------\n",
      "Resume From:  \n",
      "Output Save To:  outputss\n",
      "Visible GPUs:  1\n",
      "---------- Optimizer ----------\n",
      "Learning Rate:  0.0005\n",
      "Batchsize:  100\n",
      "Loading dataset ....\n",
      "Train [Total]  number =  12611\n",
      "Valid [Total]  number =  1425\n",
      "Creating model  : Downtask_RSNA_Boneage\n",
      "Pretrained model: \n",
      "Number of Learnable Params: 26651585\n",
      "RSNA_BAA_Model(\n",
      "  (encoder): ResNet_Feature_Extractor(\n",
      "    (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (fc1): Linear(in_features=2049, out_features=1024, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (drop1): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (drop2): Dropout(p=0.5, inplace=False)\n",
      "  (head): Linear(in_features=1024, out_features=1, bias=True)\n",
      ")\n",
      "Start training for 500 epochs\n",
      "Epoch: [0]  [  0/126]  eta: 0:19:46  lr: 0.000000  loss: 17151.9453 (17151.9453)  L2_Loss: 17151.9453 (17151.9453)  time: 9.4140  data: 8.0648  max mem: 10883\n",
      "Epoch: [0]  [  1/126]  eta: 0:10:17  lr: 0.000000  loss: 16269.1299 (16710.5376)  L2_Loss: 16269.1299 (16710.5376)  time: 4.9377  data: 4.0324  max mem: 11179\n",
      "Epoch: [0]  [  2/126]  eta: 0:11:19  lr: 0.000000  loss: 17151.9453 (17620.3402)  L2_Loss: 17151.9453 (17620.3402)  time: 5.4788  data: 4.7214  max mem: 11179\n",
      "Epoch: [0]  [  3/126]  eta: 0:08:39  lr: 0.000000  loss: 17151.9453 (17901.0388)  L2_Loss: 17151.9453 (17901.0388)  time: 4.2246  data: 3.5411  max mem: 11179\n",
      "Epoch: [0]  [  4/126]  eta: 0:09:39  lr: 0.000000  loss: 17221.5215 (17765.1354)  L2_Loss: 17221.5215 (17765.1354)  time: 4.7509  data: 4.1114  max mem: 11179\n",
      "Epoch: [0]  [  5/126]  eta: 0:08:12  lr: 0.000000  loss: 17151.9453 (17632.4849)  L2_Loss: 17151.9453 (17632.4849)  time: 4.0682  data: 3.4577  max mem: 11179\n",
      "Epoch: [0]  [  6/126]  eta: 0:08:59  lr: 0.000000  loss: 17221.5215 (17728.5169)  L2_Loss: 17221.5215 (17728.5169)  time: 4.4975  data: 3.9076  max mem: 11179\n",
      "Epoch: [0]  [  7/126]  eta: 0:08:16  lr: 0.000000  loss: 17221.5215 (18023.2858)  L2_Loss: 17221.5215 (18023.2858)  time: 4.1715  data: 3.5970  max mem: 11179\n",
      "Epoch: [0]  [  8/126]  eta: 0:08:28  lr: 0.000000  loss: 17221.5215 (17884.4426)  L2_Loss: 17221.5215 (17884.4426)  time: 4.3126  data: 3.7504  max mem: 11179\n",
      "Epoch: [0]  [  9/126]  eta: 0:08:04  lr: 0.000000  loss: 17151.9453 (17809.9608)  L2_Loss: 17151.9453 (17809.9608)  time: 4.1439  data: 3.5915  max mem: 11179\n",
      "Epoch: [0]  [ 10/126]  eta: 0:08:09  lr: 0.000000  loss: 17221.5215 (17907.3577)  L2_Loss: 17221.5215 (17907.3577)  time: 4.2235  data: 3.6790  max mem: 11179\n",
      "Epoch: [0]  [ 11/126]  eta: 0:07:57  lr: 0.000000  loss: 17221.5215 (17967.9312)  L2_Loss: 17221.5215 (17967.9312)  time: 4.1547  data: 3.6167  max mem: 11179\n",
      "Epoch: [0]  [ 12/126]  eta: 0:07:49  lr: 0.000000  loss: 17221.5215 (17888.3524)  L2_Loss: 17221.5215 (17888.3524)  time: 4.1181  data: 3.5857  max mem: 11179\n",
      "Epoch: [0]  [ 13/126]  eta: 0:07:41  lr: 0.000000  loss: 17221.5215 (18015.7497)  L2_Loss: 17221.5215 (18015.7497)  time: 4.0812  data: 3.5537  max mem: 11179\n",
      "Epoch: [0]  [ 14/126]  eta: 0:07:32  lr: 0.000000  loss: 17497.0195 (17981.1676)  L2_Loss: 17497.0195 (17981.1676)  time: 4.0422  data: 3.5187  max mem: 11179\n",
      "Epoch: [0]  [ 15/126]  eta: 0:07:29  lr: 0.000000  loss: 17221.5215 (17931.3615)  L2_Loss: 17221.5215 (17931.3615)  time: 4.0499  data: 3.5298  max mem: 11179\n",
      "Epoch: [0]  [ 16/126]  eta: 0:07:23  lr: 0.000000  loss: 17471.6270 (17904.3183)  L2_Loss: 17471.6270 (17904.3183)  time: 4.0310  data: 3.5139  max mem: 11179\n",
      "Epoch: [0]  [ 17/126]  eta: 0:07:21  lr: 0.000000  loss: 17471.6270 (17930.3281)  L2_Loss: 17471.6270 (17930.3281)  time: 4.0470  data: 3.5327  max mem: 11179\n",
      "Epoch: [0]  [ 18/126]  eta: 0:07:14  lr: 0.000000  loss: 17471.6270 (17894.1325)  L2_Loss: 17471.6270 (17894.1325)  time: 4.0246  data: 3.5128  max mem: 11179\n",
      "Epoch: [0]  [ 19/126]  eta: 0:07:14  lr: 0.000000  loss: 17242.6113 (17834.3966)  L2_Loss: 17242.6113 (17834.3966)  time: 4.0562  data: 3.5467  max mem: 11179\n",
      "Epoch: [0]  [ 20/126]  eta: 0:07:03  lr: 0.000000  loss: 17276.7988 (17807.8444)  L2_Loss: 17276.7988 (17807.8444)  time: 3.7286  data: 3.2633  max mem: 11179\n",
      "Epoch: [0]  [ 21/126]  eta: 0:07:04  lr: 0.000000  loss: 17276.7988 (17778.0555)  L2_Loss: 17276.7988 (17778.0555)  time: 3.9574  data: 3.4918  max mem: 11179\n",
      "Epoch: [0]  [ 22/126]  eta: 0:06:53  lr: 0.000000  loss: 17276.7988 (17802.8878)  L2_Loss: 17276.7988 (17802.8878)  time: 3.7518  data: 3.2860  max mem: 11179\n",
      "Epoch: [0]  [ 23/126]  eta: 0:06:58  lr: 0.000000  loss: 17276.7988 (17878.1510)  L2_Loss: 17276.7988 (17878.1510)  time: 4.0270  data: 3.5611  max mem: 11179\n",
      "Epoch: [0]  [ 24/126]  eta: 0:06:44  lr: 0.000000  loss: 17471.6270 (17981.8707)  L2_Loss: 17471.6270 (17981.8707)  time: 3.7701  data: 3.3041  max mem: 11179\n",
      "Epoch: [0]  [ 25/126]  eta: 0:06:49  lr: 0.000000  loss: 17497.0195 (18058.3575)  L2_Loss: 17497.0195 (18058.3575)  time: 4.0546  data: 3.5886  max mem: 11179\n",
      "Epoch: [0]  [ 26/126]  eta: 0:06:36  lr: 0.000000  loss: 17471.6270 (18031.4268)  L2_Loss: 17471.6270 (18031.4268)  time: 3.7767  data: 3.3108  max mem: 11179\n",
      "Epoch: [0]  [ 27/126]  eta: 0:06:38  lr: 0.000000  loss: 17383.9160 (18008.3014)  L2_Loss: 17383.9160 (18008.3014)  time: 3.9678  data: 3.5019  max mem: 11179\n",
      "Epoch: [0]  [ 28/126]  eta: 0:06:26  lr: 0.000000  loss: 17383.9160 (17981.4200)  L2_Loss: 17383.9160 (17981.4200)  time: 3.7773  data: 3.3113  max mem: 11179\n",
      "Epoch: [0]  [ 29/126]  eta: 0:06:28  lr: 0.000000  loss: 17471.6270 (18022.9953)  L2_Loss: 17471.6270 (18022.9953)  time: 3.9346  data: 3.4684  max mem: 11179\n",
      "Epoch: [0]  [ 30/126]  eta: 0:06:16  lr: 0.000000  loss: 17383.9160 (17989.0713)  L2_Loss: 17383.9160 (17989.0713)  time: 3.7633  data: 3.2972  max mem: 11179\n",
      "Epoch: [0]  [ 31/126]  eta: 0:06:20  lr: 0.000000  loss: 17331.2266 (17965.8743)  L2_Loss: 17331.2266 (17965.8743)  time: 3.9081  data: 3.4420  max mem: 11179\n",
      "Epoch: [0]  [ 32/126]  eta: 0:06:07  lr: 0.000000  loss: 17383.9160 (17991.9453)  L2_Loss: 17383.9160 (17991.9453)  time: 3.7811  data: 3.3150  max mem: 11179\n",
      "Epoch: [0]  [ 33/126]  eta: 0:06:11  lr: 0.000000  loss: 17383.9160 (17998.6554)  L2_Loss: 17383.9160 (17998.6554)  time: 3.9314  data: 3.4650  max mem: 11179\n",
      "Epoch: [0]  [ 34/126]  eta: 0:05:59  lr: 0.000000  loss: 17331.2266 (17964.8660)  L2_Loss: 17331.2266 (17964.8660)  time: 3.8088  data: 3.3423  max mem: 11179\n",
      "Epoch: [0]  [ 35/126]  eta: 0:06:02  lr: 0.000000  loss: 17383.9160 (17953.5657)  L2_Loss: 17383.9160 (17953.5657)  time: 3.9399  data: 3.4735  max mem: 11179\n",
      "Epoch: [0]  [ 36/126]  eta: 0:05:51  lr: 0.000000  loss: 17383.9160 (17941.1453)  L2_Loss: 17383.9160 (17941.1453)  time: 3.7929  data: 3.3268  max mem: 11179\n",
      "Epoch: [0]  [ 37/126]  eta: 0:05:56  lr: 0.000000  loss: 17383.9160 (17978.6496)  L2_Loss: 17383.9160 (17978.6496)  time: 3.9694  data: 3.5033  max mem: 11179\n",
      "Epoch: [0]  [ 38/126]  eta: 0:05:44  lr: 0.000000  loss: 17494.0098 (18015.7723)  L2_Loss: 17494.0098 (18015.7723)  time: 3.8116  data: 3.3455  max mem: 11179\n",
      "Epoch: [0]  [ 39/126]  eta: 0:05:47  lr: 0.000000  loss: 17494.0098 (18001.2777)  L2_Loss: 17494.0098 (18001.2777)  time: 3.9434  data: 3.4773  max mem: 11179\n",
      "Epoch: [0]  [ 40/126]  eta: 0:05:36  lr: 0.000000  loss: 17558.0566 (18004.7444)  L2_Loss: 17558.0566 (18004.7444)  time: 3.8236  data: 3.3575  max mem: 11179\n",
      "Epoch: [0]  [ 41/126]  eta: 0:05:40  lr: 0.000000  loss: 18143.4102 (18045.8500)  L2_Loss: 18143.4102 (18045.8500)  time: 3.9537  data: 3.4874  max mem: 11179\n",
      "Epoch: [0]  [ 42/126]  eta: 0:05:29  lr: 0.000000  loss: 18143.4102 (18064.3371)  L2_Loss: 18143.4102 (18064.3371)  time: 3.8545  data: 3.3883  max mem: 11179\n",
      "Epoch: [0]  [ 43/126]  eta: 0:05:31  lr: 0.000000  loss: 17558.0566 (18044.4876)  L2_Loss: 17558.0566 (18044.4876)  time: 3.9204  data: 3.4540  max mem: 11179\n",
      "Epoch: [0]  [ 44/126]  eta: 0:05:21  lr: 0.000000  loss: 17494.0098 (18004.0763)  L2_Loss: 17494.0098 (18004.0763)  time: 3.8578  data: 3.3914  max mem: 11179\n",
      "Epoch: [0]  [ 45/126]  eta: 0:05:22  lr: 0.000000  loss: 17435.9902 (17990.0106)  L2_Loss: 17435.9902 (17990.0106)  time: 3.8725  data: 3.4060  max mem: 11179\n",
      "Epoch: [0]  [ 46/126]  eta: 0:05:12  lr: 0.000000  loss: 17494.0098 (17999.9863)  L2_Loss: 17494.0098 (17999.9863)  time: 3.8199  data: 3.3535  max mem: 11179\n",
      "Epoch: [0]  [ 47/126]  eta: 0:05:14  lr: 0.000000  loss: 17558.0566 (18039.6188)  L2_Loss: 17558.0566 (18039.6188)  time: 3.9105  data: 3.4440  max mem: 11179\n",
      "Epoch: [0]  [ 48/126]  eta: 0:05:04  lr: 0.000000  loss: 17558.0566 (17998.9763)  L2_Loss: 17558.0566 (17998.9763)  time: 3.8524  data: 3.3857  max mem: 11179\n",
      "Epoch: [0]  [ 49/126]  eta: 0:05:05  lr: 0.000000  loss: 17558.0566 (18009.0561)  L2_Loss: 17558.0566 (18009.0561)  time: 3.9241  data: 3.4575  max mem: 11179\n",
      "Epoch: [0]  [ 50/126]  eta: 0:04:56  lr: 0.000000  loss: 18120.9082 (18011.2493)  L2_Loss: 18120.9082 (18011.2493)  time: 3.8677  data: 3.4010  max mem: 11179\n",
      "Epoch: [0]  [ 51/126]  eta: 0:04:57  lr: 0.000000  loss: 18143.4102 (18015.3469)  L2_Loss: 18143.4102 (18015.3469)  time: 3.8997  data: 3.4331  max mem: 11179\n",
      "Epoch: [0]  [ 52/126]  eta: 0:04:48  lr: 0.000000  loss: 18143.4102 (18021.9586)  L2_Loss: 18143.4102 (18021.9586)  time: 3.8758  data: 3.4092  max mem: 11179\n",
      "Epoch: [0]  [ 53/126]  eta: 0:04:48  lr: 0.000000  loss: 18143.4102 (18024.4202)  L2_Loss: 18143.4102 (18024.4202)  time: 3.8994  data: 3.4330  max mem: 11179\n",
      "Epoch: [0]  [ 54/126]  eta: 0:04:41  lr: 0.000000  loss: 18143.4102 (18020.4903)  L2_Loss: 18143.4102 (18020.4903)  time: 3.9096  data: 3.4433  max mem: 11179\n",
      "Epoch: [0]  [ 55/126]  eta: 0:04:41  lr: 0.000000  loss: 18154.8867 (18023.3160)  L2_Loss: 18154.8867 (18023.3160)  time: 3.9036  data: 3.4372  max mem: 11179\n",
      "Epoch: [0]  [ 56/126]  eta: 0:04:33  lr: 0.000000  loss: 18178.7324 (18032.0539)  L2_Loss: 18178.7324 (18032.0539)  time: 3.9248  data: 3.4584  max mem: 11179\n",
      "Epoch: [0]  [ 57/126]  eta: 0:04:32  lr: 0.000000  loss: 18178.7324 (18061.2217)  L2_Loss: 18178.7324 (18061.2217)  time: 3.8582  data: 3.3917  max mem: 11179\n",
      "Epoch: [0]  [ 58/126]  eta: 0:04:25  lr: 0.000000  loss: 18154.8867 (18050.2268)  L2_Loss: 18154.8867 (18050.2268)  time: 3.8924  data: 3.4257  max mem: 11179\n",
      "Epoch: [0]  [ 59/126]  eta: 0:04:25  lr: 0.000000  loss: 18154.8867 (18044.0713)  L2_Loss: 18154.8867 (18044.0713)  time: 3.8799  data: 3.4134  max mem: 11179\n",
      "Epoch: [0]  [ 60/126]  eta: 0:04:18  lr: 0.000000  loss: 18154.8867 (18030.6314)  L2_Loss: 18154.8867 (18030.6314)  time: 3.9083  data: 3.4417  max mem: 11179\n",
      "Epoch: [0]  [ 61/126]  eta: 0:04:17  lr: 0.000000  loss: 18154.8867 (18035.3226)  L2_Loss: 18154.8867 (18035.3226)  time: 3.8599  data: 3.3934  max mem: 11179\n",
      "Epoch: [0]  [ 62/126]  eta: 0:04:10  lr: 0.000000  loss: 18154.8867 (18061.4815)  L2_Loss: 18154.8867 (18061.4815)  time: 3.9071  data: 3.4407  max mem: 11179\n",
      "Epoch: [0]  [ 63/126]  eta: 0:04:08  lr: 0.000000  loss: 18154.8867 (18060.7800)  L2_Loss: 18154.8867 (18060.7800)  time: 3.8494  data: 3.3831  max mem: 11179\n",
      "Epoch: [0]  [ 64/126]  eta: 0:04:03  lr: 0.000000  loss: 18154.8867 (18056.2256)  L2_Loss: 18154.8867 (18056.2256)  time: 3.9243  data: 3.4579  max mem: 11179\n",
      "Epoch: [0]  [ 65/126]  eta: 0:04:01  lr: 0.000000  loss: 18178.7324 (18059.3360)  L2_Loss: 18178.7324 (18059.3360)  time: 3.8987  data: 3.4321  max mem: 11179\n",
      "Epoch: [0]  [ 66/126]  eta: 0:03:54  lr: 0.000000  loss: 18178.7324 (18069.6944)  L2_Loss: 18178.7324 (18069.6944)  time: 3.9370  data: 3.4704  max mem: 11179\n",
      "Epoch: [0]  [ 67/126]  eta: 0:03:52  lr: 0.000000  loss: 18154.8867 (18064.3606)  L2_Loss: 18154.8867 (18064.3606)  time: 3.8796  data: 3.4129  max mem: 11179\n",
      "Epoch: [0]  [ 68/126]  eta: 0:03:47  lr: 0.000000  loss: 18154.8867 (18057.6171)  L2_Loss: 18154.8867 (18057.6171)  time: 3.9332  data: 3.4665  max mem: 11179\n",
      "Epoch: [0]  [ 69/126]  eta: 0:03:45  lr: 0.000000  loss: 18154.8867 (18069.5803)  L2_Loss: 18154.8867 (18069.5803)  time: 3.9024  data: 3.4357  max mem: 11179\n",
      "Epoch: [0]  [ 70/126]  eta: 0:03:39  lr: 0.000000  loss: 18154.8867 (18056.6086)  L2_Loss: 18154.8867 (18056.6086)  time: 3.9642  data: 3.4974  max mem: 11179\n",
      "Epoch: [0]  [ 71/126]  eta: 0:03:37  lr: 0.000000  loss: 18118.6562 (18057.4704)  L2_Loss: 18118.6562 (18057.4704)  time: 3.9171  data: 3.4503  max mem: 11179\n",
      "Epoch: [0]  [ 72/126]  eta: 0:03:31  lr: 0.000000  loss: 18016.5840 (18036.2805)  L2_Loss: 18016.5840 (18036.2805)  time: 3.9862  data: 3.5191  max mem: 11179\n",
      "Epoch: [0]  [ 73/126]  eta: 0:03:29  lr: 0.000000  loss: 18016.5840 (18044.9696)  L2_Loss: 18016.5840 (18044.9696)  time: 3.9106  data: 3.4436  max mem: 11179\n",
      "Epoch: [0]  [ 74/126]  eta: 0:03:23  lr: 0.000000  loss: 18118.6562 (18055.7639)  L2_Loss: 18118.6562 (18055.7639)  time: 3.9531  data: 3.4859  max mem: 11179\n",
      "Epoch: [0]  [ 75/126]  eta: 0:03:21  lr: 0.000000  loss: 18016.5840 (18032.6461)  L2_Loss: 18016.5840 (18032.6461)  time: 3.8963  data: 3.4291  max mem: 11179\n",
      "Epoch: [0]  [ 76/126]  eta: 0:03:16  lr: 0.000000  loss: 18016.5840 (18049.4488)  L2_Loss: 18016.5840 (18049.4488)  time: 3.9516  data: 3.4840  max mem: 11179\n",
      "Epoch: [0]  [ 77/126]  eta: 0:03:12  lr: 0.000000  loss: 17764.7441 (18030.4497)  L2_Loss: 17764.7441 (18030.4497)  time: 3.8503  data: 3.3828  max mem: 11179\n",
      "Epoch: [0]  [ 78/126]  eta: 0:03:08  lr: 0.000000  loss: 18016.5840 (18030.2888)  L2_Loss: 18016.5840 (18030.2888)  time: 3.9555  data: 3.4881  max mem: 11179\n",
      "Epoch: [0]  [ 79/126]  eta: 0:03:04  lr: 0.000000  loss: 18016.5840 (18027.2955)  L2_Loss: 18016.5840 (18027.2955)  time: 3.8176  data: 3.3502  max mem: 11179\n",
      "Epoch: [0]  [ 80/126]  eta: 0:03:00  lr: 0.000000  loss: 18017.7402 (18027.8442)  L2_Loss: 18017.7402 (18027.8442)  time: 3.9315  data: 3.4639  max mem: 11179\n",
      "Epoch: [0]  [ 81/126]  eta: 0:02:56  lr: 0.000000  loss: 18016.5840 (18024.0313)  L2_Loss: 18016.5840 (18024.0313)  time: 3.8567  data: 3.3892  max mem: 11179\n",
      "Epoch: [0]  [ 82/126]  eta: 0:02:52  lr: 0.000000  loss: 17790.8262 (18004.8239)  L2_Loss: 17790.8262 (18004.8239)  time: 3.8877  data: 3.4200  max mem: 11179\n",
      "Epoch: [0]  [ 83/126]  eta: 0:02:48  lr: 0.000000  loss: 17764.7441 (17982.0279)  L2_Loss: 17764.7441 (17982.0279)  time: 3.8498  data: 3.3821  max mem: 11179\n",
      "Epoch: [0]  [ 84/126]  eta: 0:02:43  lr: 0.000000  loss: 17715.1797 (17972.3910)  L2_Loss: 17715.1797 (17972.3910)  time: 3.8524  data: 3.3847  max mem: 11179\n",
      "Epoch: [0]  [ 85/126]  eta: 0:02:40  lr: 0.000000  loss: 17706.9902 (17951.9163)  L2_Loss: 17706.9902 (17951.9163)  time: 3.8263  data: 3.3587  max mem: 11179\n",
      "Epoch: [0]  [ 86/126]  eta: 0:02:36  lr: 0.000000  loss: 17706.9902 (17956.6931)  L2_Loss: 17706.9902 (17956.6931)  time: 3.8690  data: 3.4015  max mem: 11179\n",
      "Epoch: [0]  [ 87/126]  eta: 0:02:33  lr: 0.000000  loss: 17715.1797 (17954.9305)  L2_Loss: 17715.1797 (17954.9305)  time: 3.8357  data: 3.3683  max mem: 11179\n",
      "Epoch: [0]  [ 88/126]  eta: 0:02:28  lr: 0.000000  loss: 17790.8262 (17968.7675)  L2_Loss: 17790.8262 (17968.7675)  time: 3.8732  data: 3.4057  max mem: 11179\n",
      "Epoch: [0]  [ 89/126]  eta: 0:02:25  lr: 0.000000  loss: 17790.8262 (17982.5400)  L2_Loss: 17790.8262 (17982.5400)  time: 3.8570  data: 3.3895  max mem: 11179\n",
      "Epoch: [0]  [ 90/126]  eta: 0:02:20  lr: 0.000000  loss: 17801.5840 (17986.6600)  L2_Loss: 17801.5840 (17986.6600)  time: 3.8146  data: 3.3472  max mem: 11179\n",
      "Epoch: [0]  [ 91/126]  eta: 0:02:17  lr: 0.000000  loss: 17790.8262 (17959.9195)  L2_Loss: 17790.8262 (17959.9195)  time: 3.8555  data: 3.3881  max mem: 11179\n",
      "Epoch: [0]  [ 92/126]  eta: 0:02:12  lr: 0.000000  loss: 17790.8262 (17953.1708)  L2_Loss: 17790.8262 (17953.1708)  time: 3.7979  data: 3.3307  max mem: 11179\n",
      "Epoch: [0]  [ 93/126]  eta: 0:02:09  lr: 0.000000  loss: 17715.1797 (17944.0572)  L2_Loss: 17715.1797 (17944.0572)  time: 3.8499  data: 3.3827  max mem: 11179\n",
      "Epoch: [0]  [ 94/126]  eta: 0:02:04  lr: 0.000000  loss: 17403.0645 (17938.3626)  L2_Loss: 17403.0645 (17938.3626)  time: 3.8152  data: 3.3481  max mem: 11179\n",
      "Epoch: [0]  [ 95/126]  eta: 0:02:01  lr: 0.000000  loss: 17403.0645 (17931.2354)  L2_Loss: 17403.0645 (17931.2354)  time: 3.8424  data: 3.3754  max mem: 11179\n",
      "Epoch: [0]  [ 96/126]  eta: 0:01:57  lr: 0.000000  loss: 17332.2891 (17922.5255)  L2_Loss: 17332.2891 (17922.5255)  time: 3.8322  data: 3.3655  max mem: 11179\n",
      "Epoch: [0]  [ 97/126]  eta: 0:01:53  lr: 0.000000  loss: 17332.2891 (17912.2463)  L2_Loss: 17332.2891 (17912.2463)  time: 3.8947  data: 3.4278  max mem: 11179\n",
      "Epoch: [0]  [ 98/126]  eta: 0:01:49  lr: 0.000000  loss: 17332.2891 (17912.3309)  L2_Loss: 17332.2891 (17912.3309)  time: 3.8309  data: 3.3639  max mem: 11179\n",
      "Epoch: [0]  [ 99/126]  eta: 0:01:45  lr: 0.000000  loss: 17261.2988 (17905.8206)  L2_Loss: 17261.2988 (17905.8206)  time: 3.8764  data: 3.4093  max mem: 11179\n",
      "Epoch: [0]  [100/126]  eta: 0:01:41  lr: 0.000000  loss: 17261.2988 (17902.2574)  L2_Loss: 17261.2988 (17902.2574)  time: 3.7902  data: 3.3233  max mem: 11179\n",
      "Epoch: [0]  [101/126]  eta: 0:01:37  lr: 0.000000  loss: 17261.2988 (17901.4487)  L2_Loss: 17261.2988 (17901.4487)  time: 3.8165  data: 3.3494  max mem: 11179\n",
      "Epoch: [0]  [102/126]  eta: 0:01:33  lr: 0.000000  loss: 17332.2891 (17898.7142)  L2_Loss: 17332.2891 (17898.7142)  time: 3.7565  data: 3.2894  max mem: 11179\n",
      "Epoch: [0]  [103/126]  eta: 0:01:29  lr: 0.000000  loss: 17403.0645 (17906.8531)  L2_Loss: 17403.0645 (17906.8531)  time: 3.8238  data: 3.3569  max mem: 11179\n",
      "Epoch: [0]  [104/126]  eta: 0:01:25  lr: 0.000000  loss: 17545.9434 (17916.5612)  L2_Loss: 17545.9434 (17916.5612)  time: 3.7703  data: 3.3034  max mem: 11179\n",
      "Epoch: [0]  [105/126]  eta: 0:01:22  lr: 0.000000  loss: 17545.9434 (17912.1805)  L2_Loss: 17545.9434 (17912.1805)  time: 3.8448  data: 3.3778  max mem: 11179\n",
      "Epoch: [0]  [106/126]  eta: 0:01:17  lr: 0.000000  loss: 17545.9434 (17910.5513)  L2_Loss: 17545.9434 (17910.5513)  time: 3.8238  data: 3.3567  max mem: 11179\n",
      "Epoch: [0]  [107/126]  eta: 0:01:14  lr: 0.000000  loss: 17545.9434 (17907.7471)  L2_Loss: 17545.9434 (17907.7471)  time: 3.8549  data: 3.3879  max mem: 11179\n",
      "Epoch: [0]  [108/126]  eta: 0:01:10  lr: 0.000000  loss: 17545.9434 (17919.8030)  L2_Loss: 17545.9434 (17919.8030)  time: 3.8206  data: 3.3536  max mem: 11179\n",
      "Epoch: [0]  [109/126]  eta: 0:01:06  lr: 0.000000  loss: 17545.9434 (17931.9308)  L2_Loss: 17545.9434 (17931.9308)  time: 3.8373  data: 3.3702  max mem: 11179\n",
      "Epoch: [0]  [110/126]  eta: 0:01:02  lr: 0.000000  loss: 17545.9434 (17933.8129)  L2_Loss: 17545.9434 (17933.8129)  time: 3.8324  data: 3.3653  max mem: 11179\n",
      "Epoch: [0]  [111/126]  eta: 0:00:58  lr: 0.000000  loss: 17545.9434 (17926.7727)  L2_Loss: 17545.9434 (17926.7727)  time: 3.8400  data: 3.3727  max mem: 11179\n",
      "Epoch: [0]  [112/126]  eta: 0:00:54  lr: 0.000000  loss: 17607.6953 (17925.0286)  L2_Loss: 17607.6953 (17925.0286)  time: 3.8402  data: 3.3728  max mem: 11179\n",
      "Epoch: [0]  [113/126]  eta: 0:00:50  lr: 0.000000  loss: 17607.6953 (17920.0430)  L2_Loss: 17607.6953 (17920.0430)  time: 3.8405  data: 3.3728  max mem: 11179\n",
      "Epoch: [0]  [114/126]  eta: 0:00:46  lr: 0.000000  loss: 17619.7891 (17928.1937)  L2_Loss: 17619.7891 (17928.1937)  time: 3.8212  data: 3.3536  max mem: 11179\n",
      "Epoch: [0]  [115/126]  eta: 0:00:43  lr: 0.000000  loss: 17729.6875 (17950.2200)  L2_Loss: 17729.6875 (17950.2200)  time: 3.9079  data: 3.4400  max mem: 11179\n",
      "Epoch: [0]  [116/126]  eta: 0:00:38  lr: 0.000000  loss: 17729.6875 (17947.4596)  L2_Loss: 17729.6875 (17947.4596)  time: 3.8255  data: 3.3575  max mem: 11179\n",
      "Epoch: [0]  [117/126]  eta: 0:00:35  lr: 0.000000  loss: 17737.8516 (17953.1587)  L2_Loss: 17737.8516 (17953.1587)  time: 3.9572  data: 3.4892  max mem: 11179\n",
      "Epoch: [0]  [118/126]  eta: 0:00:31  lr: 0.000000  loss: 17729.6875 (17947.7007)  L2_Loss: 17729.6875 (17947.7007)  time: 3.8815  data: 3.4137  max mem: 11179\n",
      "Epoch: [0]  [119/126]  eta: 0:00:27  lr: 0.000000  loss: 17729.6875 (17941.4696)  L2_Loss: 17729.6875 (17941.4696)  time: 4.0246  data: 3.5569  max mem: 11179\n",
      "Epoch: [0]  [120/126]  eta: 0:00:23  lr: 0.000000  loss: 17737.8516 (17947.8017)  L2_Loss: 17737.8516 (17947.8017)  time: 3.9685  data: 3.5008  max mem: 11179\n",
      "Epoch: [0]  [121/126]  eta: 0:00:19  lr: 0.000000  loss: 17729.6875 (17936.9625)  L2_Loss: 17729.6875 (17936.9625)  time: 4.0632  data: 3.5956  max mem: 11179\n",
      "Epoch: [0]  [122/126]  eta: 0:00:15  lr: 0.000000  loss: 17737.8516 (17952.0591)  L2_Loss: 17737.8516 (17952.0591)  time: 4.0450  data: 3.5774  max mem: 11179\n",
      "Epoch: [0]  [123/126]  eta: 0:00:11  lr: 0.000000  loss: 17729.6875 (17942.3895)  L2_Loss: 17729.6875 (17942.3895)  time: 4.0542  data: 3.5867  max mem: 11179\n",
      "Epoch: [0]  [124/126]  eta: 0:00:07  lr: 0.000000  loss: 17627.2500 (17914.0989)  L2_Loss: 17627.2500 (17914.0989)  time: 4.0302  data: 3.5627  max mem: 11179\n",
      "Epoch: [0]  [125/126]  eta: 0:00:03  lr: 0.000000  loss: 17627.2500 (17897.0616)  L2_Loss: 17627.2500 (17897.0616)  time: 4.0146  data: 3.5472  max mem: 11179\n",
      "Epoch: [0] Total time: 0:08:14 (3.9269 s / it)\n",
      "Averaged train_stats:  {'lr': 0.0, 'loss': 17897.0616319, 'L2_Loss': 17897.0616319}\n",
      "\n",
      "=== Transform input info -- AddChanneld ===\n",
      "image statistics:\n",
      "Type: <class 'numpy.ndarray'> float32\n",
      "Shape: (256, 256)\n",
      "Value range: (0.003921568859368563, 1.0)\n",
      "label statistics:\n",
      "Type: <class 'numpy.float32'> float32\n",
      "Value: 168.0\n",
      "gender statistics:\n",
      "Type: <class 'numpy.float32'> float32\n",
      "Value: 0.0\n",
      "image_transforms statistics:\n",
      "Type: <class 'list'> None\n",
      "Value: [{'class': 'Lambdad', 'id': 139958967644560, 'orig_size': (2022,)}, {'class': 'Lambdad', 'id': 139958967646240, 'orig_size': (256,)}]\n",
      "\n",
      "=== Transform input info -- AddChanneld ===\n",
      "image statistics:\n",
      "Type: <class 'numpy.ndarray'> float32\n",
      "Shape: (256, 256)\n",
      "Value range: (0.0, 1.0)\n",
      "label statistics:\n",
      "Type: <class 'numpy.float32'> float32\n",
      "Value: 30.0\n",
      "gender statistics:\n",
      "Type: <class 'numpy.float32'> float32\n",
      "Value: 0.0\n",
      "image_transforms statistics:\n",
      "Type: <class 'list'> None\n",
      "Value: [{'class': 'Lambdad', 'id': 139958967644560, 'orig_size': (1514,)}, {'class': 'Lambdad', 'id': 139958967646240, 'orig_size': (256,)}]\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 282, in <module>\n",
      "    main(args)\n",
      "  File \"train.py\", line 228, in main\n",
      "    valid_stats = valid_Downtask_RSNA_BAA(model, criterion, data_loader_valid, device, args.print_freq, args.batch_size)\n",
      "  File \"/home/pkg777774/anaconda3/envs/child_x/lib/python3.8/site-packages/torch/autograd/grad_mode.py\", line 26, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/mnt/nas125_vol2/kanggilpark/child/PedXnet_Code_Factory/engine.py\", line 778, in valid_Downtask_RSNA_BAA\n",
      "    for batch_data in metric_logger.log_every(data_loader, print_freq, header):\n",
      "  File \"/mnt/nas125_vol2/kanggilpark/child/PedXnet_Code_Factory/utils.py\", line 119, in log_every\n",
      "    for obj in iterable:\n",
      "  File \"/home/pkg777774/anaconda3/envs/child_x/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 435, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/pkg777774/anaconda3/envs/child_x/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1085, in _next_data\n",
      "    return self._process_data(data)\n",
      "  File \"/home/pkg777774/anaconda3/envs/child_x/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1111, in _process_data\n",
      "    data.reraise()\n",
      "  File \"/home/pkg777774/anaconda3/envs/child_x/lib/python3.8/site-packages/torch/_utils.py\", line 428, in reraise\n",
      "    raise self.exc_type(msg)\n",
      "RuntimeError: Caught RuntimeError in DataLoader worker process 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/pkg777774/anaconda3/envs/child_x/lib/python3.8/site-packages/monai/transforms/transform.py\", line 82, in apply_transform\n",
      "    return _apply_transform(transform, data, unpack_items)\n",
      "  File \"/home/pkg777774/anaconda3/envs/child_x/lib/python3.8/site-packages/monai/transforms/transform.py\", line 53, in _apply_transform\n",
      "    return transform(parameters)\n",
      "  File \"/home/pkg777774/anaconda3/envs/child_x/lib/python3.8/site-packages/monai/transforms/utility/dictionary.py\", line 274, in __call__\n",
      "    for key in self.key_iterator(d):\n",
      "  File \"/home/pkg777774/anaconda3/envs/child_x/lib/python3.8/site-packages/monai/transforms/transform.py\", line 363, in key_iterator\n",
      "    raise KeyError(f\"Key was missing ({key}) and allow_missing_keys==False\")\n",
      "KeyError: 'Key was missing (imagelabel) and allow_missing_keys==False'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pkg777774/anaconda3/envs/child_x/lib/python3.8/site-packages/monai/transforms/transform.py\", line 82, in apply_transform\n",
      "    return _apply_transform(transform, data, unpack_items)\n",
      "  File \"/home/pkg777774/anaconda3/envs/child_x/lib/python3.8/site-packages/monai/transforms/transform.py\", line 53, in _apply_transform\n",
      "    return transform(parameters)\n",
      "  File \"/home/pkg777774/anaconda3/envs/child_x/lib/python3.8/site-packages/monai/transforms/compose.py\", line 160, in __call__\n",
      "    input_ = apply_transform(_transform, input_, self.map_items, self.unpack_items)\n",
      "  File \"/home/pkg777774/anaconda3/envs/child_x/lib/python3.8/site-packages/monai/transforms/transform.py\", line 106, in apply_transform\n",
      "    raise RuntimeError(f\"applying transform {transform}\") from e\n",
      "RuntimeError: applying transform <monai.transforms.utility.dictionary.AddChanneld object at 0x7f4abc8cb8e0>\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pkg777774/anaconda3/envs/child_x/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n",
      "    data = fetcher.fetch(index)\n",
      "  File \"/home/pkg777774/anaconda3/envs/child_x/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/home/pkg777774/anaconda3/envs/child_x/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/home/pkg777774/anaconda3/envs/child_x/lib/python3.8/site-packages/monai/data/dataset.py\", line 96, in __getitem__\n",
      "    return self._transform(index)\n",
      "  File \"/home/pkg777774/anaconda3/envs/child_x/lib/python3.8/site-packages/monai/data/dataset.py\", line 82, in _transform\n",
      "    return apply_transform(self.transform, data_i) if self.transform is not None else data_i\n",
      "  File \"/home/pkg777774/anaconda3/envs/child_x/lib/python3.8/site-packages/monai/transforms/transform.py\", line 106, in apply_transform\n",
      "    raise RuntimeError(f\"applying transform {transform}\") from e\n",
      "RuntimeError: applying transform <monai.transforms.compose.Compose object at 0x7f4abc8cbc40>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python train.py \\\n",
    "--data_set 'RSNA_BAA' \\\n",
    "--model_name 'Downtask_RSNA_Boneage' \\\n",
    "--data_folder_dir \"/mnt/nas125_vol2/kanggilpark/child/bone_age/data\" \\\n",
    "--batch-size 100 \\\n",
    "--num_workers 2 \\\n",
    "--multi-gpu-mode 'Single' \\\n",
    "--cuda-visible-devices '1' \\\n",
    "--gradual_unfreeze true \\\n",
    "--print-freq 1\\\n",
    "--output_dir outputss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************************\n",
      "*           Training Mode is  Downstream      *\n",
      "***********************************************\n",
      "Dataset Name:  /mnt/nas125_vol2/kanggilpark/child/jyp_child/data\n",
      "---------- Model ----------\n",
      "Resume From:  \n",
      "Output Save To:  outputssss\n",
      "Visible GPUs:  1\n",
      "---------- Optimizer ----------\n",
      "Learning Rate:  0.0005\n",
      "Batchsize:  120\n",
      "Loading dataset ....\n",
      "Train [Total]  number =  6838\n",
      "Valid [Total]  number =  624\n",
      "Creating model  : Downtask_Pneumonia\n",
      "Pretrained model: \n",
      "Number of Learnable Params: 26650561\n",
      "Pneumonia_Model(\n",
      "  (encoder): ResNet_Feature_Extractor(\n",
      "    (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (fc1): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (drop1): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (drop2): Dropout(p=0.5, inplace=False)\n",
      "  (head): Linear(in_features=1024, out_features=1, bias=True)\n",
      ")\n",
      "Start training for 500 epochs\n",
      "Freeze encoder ...!\n",
      "Epoch: [0]  [ 0/56]  eta: 0:16:39  lr: 0.000000  loss: 0.6918 (0.6918)  BCE_Loss: 0.6918 (0.6918)  time: 17.8556  data: 16.5438  max mem: 6462\n",
      "Epoch: [0]  [ 1/56]  eta: 0:08:26  lr: 0.000000  loss: 0.6838 (0.6878)  BCE_Loss: 0.6838 (0.6878)  time: 9.2110  data: 8.2719  max mem: 6498\n",
      "Epoch: [0]  [ 2/56]  eta: 0:05:41  lr: 0.000000  loss: 0.6881 (0.6879)  BCE_Loss: 0.6881 (0.6879)  time: 6.3299  data: 5.5146  max mem: 6498\n",
      "Epoch: [0]  [ 3/56]  eta: 0:04:19  lr: 0.000000  loss: 0.6881 (0.6897)  BCE_Loss: 0.6881 (0.6897)  time: 4.8883  data: 4.1360  max mem: 6498\n",
      "Epoch: [0]  [ 4/56]  eta: 0:05:27  lr: 0.000000  loss: 0.6918 (0.6921)  BCE_Loss: 0.6918 (0.6921)  time: 6.2966  data: 5.5817  max mem: 6498\n",
      "Epoch: [0]  [ 5/56]  eta: 0:04:32  lr: 0.000000  loss: 0.6918 (0.6935)  BCE_Loss: 0.6918 (0.6935)  time: 5.3413  data: 4.6515  max mem: 6498\n",
      "Epoch: [0]  [ 6/56]  eta: 0:03:52  lr: 0.000000  loss: 0.6949 (0.6948)  BCE_Loss: 0.6949 (0.6948)  time: 4.6590  data: 3.9870  max mem: 6498\n",
      "Epoch: [0]  [ 7/56]  eta: 0:03:23  lr: 0.000000  loss: 0.6949 (0.6952)  BCE_Loss: 0.6949 (0.6952)  time: 4.1472  data: 3.4886  max mem: 6498\n",
      "Epoch: [0]  [ 8/56]  eta: 0:04:09  lr: 0.000000  loss: 0.6984 (0.6960)  BCE_Loss: 0.6984 (0.6960)  time: 5.1876  data: 4.5394  max mem: 6498\n",
      "Epoch: [0]  [ 9/56]  eta: 0:03:42  lr: 0.000000  loss: 0.6949 (0.6959)  BCE_Loss: 0.6949 (0.6959)  time: 4.7255  data: 4.0855  max mem: 6498\n",
      "Epoch: [0]  [10/56]  eta: 0:03:19  lr: 0.000000  loss: 0.6949 (0.6951)  BCE_Loss: 0.6949 (0.6951)  time: 4.3473  data: 3.7141  max mem: 6498\n",
      "Epoch: [0]  [11/56]  eta: 0:03:01  lr: 0.000000  loss: 0.6946 (0.6946)  BCE_Loss: 0.6946 (0.6946)  time: 4.0322  data: 3.4046  max mem: 6498\n",
      "Epoch: [0]  [12/56]  eta: 0:03:27  lr: 0.000000  loss: 0.6949 (0.6951)  BCE_Loss: 0.6949 (0.6951)  time: 4.7242  data: 4.1011  max mem: 6498\n",
      "Epoch: [0]  [13/56]  eta: 0:03:10  lr: 0.000000  loss: 0.6949 (0.6956)  BCE_Loss: 0.6949 (0.6956)  time: 4.4273  data: 3.8082  max mem: 6498\n",
      "Epoch: [0]  [14/56]  eta: 0:02:55  lr: 0.000000  loss: 0.6984 (0.6962)  BCE_Loss: 0.6984 (0.6962)  time: 4.1699  data: 3.5543  max mem: 6498\n",
      "Epoch: [0]  [15/56]  eta: 0:02:41  lr: 0.000000  loss: 0.6949 (0.6960)  BCE_Loss: 0.6949 (0.6960)  time: 3.9450  data: 3.3322  max mem: 6498\n",
      "Epoch: [0]  [16/56]  eta: 0:02:58  lr: 0.000000  loss: 0.6949 (0.6953)  BCE_Loss: 0.6949 (0.6953)  time: 4.4712  data: 3.8611  max mem: 6498\n",
      "Epoch: [0]  [17/56]  eta: 0:02:45  lr: 0.000000  loss: 0.6946 (0.6949)  BCE_Loss: 0.6946 (0.6949)  time: 4.2542  data: 3.6466  max mem: 6498\n",
      "Epoch: [0]  [18/56]  eta: 0:02:34  lr: 0.000000  loss: 0.6946 (0.6948)  BCE_Loss: 0.6946 (0.6948)  time: 4.0779  data: 3.4725  max mem: 6498\n",
      "Epoch: [0]  [19/56]  eta: 0:02:24  lr: 0.000000  loss: 0.6936 (0.6945)  BCE_Loss: 0.6936 (0.6945)  time: 3.9023  data: 3.2989  max mem: 6498\n",
      "Epoch: [0]  [20/56]  eta: 0:02:34  lr: 0.000000  loss: 0.6936 (0.6943)  BCE_Loss: 0.6936 (0.6943)  time: 3.6016  data: 3.0354  max mem: 6498\n",
      "Epoch: [0]  [21/56]  eta: 0:02:23  lr: 0.000000  loss: 0.6946 (0.6945)  BCE_Loss: 0.6946 (0.6945)  time: 3.6016  data: 3.0354  max mem: 6498\n",
      "Epoch: [0]  [22/56]  eta: 0:02:16  lr: 0.000000  loss: 0.6946 (0.6944)  BCE_Loss: 0.6946 (0.6944)  time: 3.6749  data: 3.1087  max mem: 6498\n",
      "Epoch: [0]  [23/56]  eta: 0:02:07  lr: 0.000000  loss: 0.6946 (0.6945)  BCE_Loss: 0.6946 (0.6945)  time: 3.6750  data: 3.1087  max mem: 6498\n",
      "Epoch: [0]  [24/56]  eta: 0:02:13  lr: 0.000000  loss: 0.6946 (0.6948)  BCE_Loss: 0.6946 (0.6948)  time: 3.6574  data: 3.0910  max mem: 6498\n",
      "Epoch: [0]  [25/56]  eta: 0:02:05  lr: 0.000000  loss: 0.6936 (0.6948)  BCE_Loss: 0.6936 (0.6948)  time: 3.6615  data: 3.0951  max mem: 6498\n",
      "Epoch: [0]  [26/56]  eta: 0:01:59  lr: 0.000000  loss: 0.6936 (0.6954)  BCE_Loss: 0.6936 (0.6954)  time: 3.7484  data: 3.1819  max mem: 6498\n",
      "Epoch: [0]  [27/56]  eta: 0:01:52  lr: 0.000000  loss: 0.6936 (0.6958)  BCE_Loss: 0.6936 (0.6958)  time: 3.7485  data: 3.1819  max mem: 6498\n",
      "Epoch: [0]  [28/56]  eta: 0:01:55  lr: 0.000000  loss: 0.6936 (0.6962)  BCE_Loss: 0.6936 (0.6962)  time: 3.6504  data: 3.0837  max mem: 6498\n",
      "Epoch: [0]  [29/56]  eta: 0:01:48  lr: 0.000000  loss: 0.6936 (0.6958)  BCE_Loss: 0.6936 (0.6958)  time: 3.6675  data: 3.1009  max mem: 6498\n",
      "Epoch: [0]  [30/56]  eta: 0:01:42  lr: 0.000000  loss: 0.6936 (0.6961)  BCE_Loss: 0.6936 (0.6961)  time: 3.7100  data: 3.1433  max mem: 6498\n",
      "Epoch: [0]  [31/56]  eta: 0:01:35  lr: 0.000000  loss: 0.6962 (0.6964)  BCE_Loss: 0.6962 (0.6964)  time: 3.7100  data: 3.1434  max mem: 6498\n",
      "Epoch: [0]  [32/56]  eta: 0:01:38  lr: 0.000000  loss: 0.6938 (0.6963)  BCE_Loss: 0.6938 (0.6963)  time: 3.6999  data: 3.1334  max mem: 6498\n",
      "Epoch: [0]  [33/56]  eta: 0:01:32  lr: 0.000000  loss: 0.6938 (0.6964)  BCE_Loss: 0.6938 (0.6964)  time: 3.7461  data: 3.1796  max mem: 6498\n",
      "Epoch: [0]  [34/56]  eta: 0:01:26  lr: 0.000000  loss: 0.6936 (0.6962)  BCE_Loss: 0.6936 (0.6962)  time: 3.7461  data: 3.1796  max mem: 6498\n",
      "Epoch: [0]  [35/56]  eta: 0:01:20  lr: 0.000000  loss: 0.6936 (0.6961)  BCE_Loss: 0.6936 (0.6961)  time: 3.7459  data: 3.1796  max mem: 6498\n",
      "Epoch: [0]  [36/56]  eta: 0:01:20  lr: 0.000000  loss: 0.6936 (0.6957)  BCE_Loss: 0.6936 (0.6957)  time: 3.6744  data: 3.1081  max mem: 6498\n",
      "Epoch: [0]  [37/56]  eta: 0:01:16  lr: 0.000000  loss: 0.6938 (0.6960)  BCE_Loss: 0.6938 (0.6960)  time: 3.7747  data: 3.2084  max mem: 6498\n",
      "Epoch: [0]  [38/56]  eta: 0:01:10  lr: 0.000000  loss: 0.6950 (0.6959)  BCE_Loss: 0.6950 (0.6959)  time: 3.7581  data: 3.1914  max mem: 6498\n",
      "Epoch: [0]  [39/56]  eta: 0:01:05  lr: 0.000000  loss: 0.6962 (0.6960)  BCE_Loss: 0.6962 (0.6960)  time: 3.7581  data: 3.1914  max mem: 6498\n",
      "Epoch: [0]  [40/56]  eta: 0:01:04  lr: 0.000000  loss: 0.6962 (0.6960)  BCE_Loss: 0.6962 (0.6960)  time: 3.7239  data: 3.1573  max mem: 6498\n",
      "Epoch: [0]  [41/56]  eta: 0:00:59  lr: 0.000000  loss: 0.6953 (0.6959)  BCE_Loss: 0.6953 (0.6959)  time: 3.8480  data: 3.2813  max mem: 6498\n",
      "Epoch: [0]  [42/56]  eta: 0:00:54  lr: 0.000000  loss: 0.6953 (0.6957)  BCE_Loss: 0.6953 (0.6957)  time: 3.7747  data: 3.2080  max mem: 6498\n",
      "Epoch: [0]  [43/56]  eta: 0:00:49  lr: 0.000000  loss: 0.6953 (0.6958)  BCE_Loss: 0.6953 (0.6958)  time: 3.7748  data: 3.2080  max mem: 6498\n",
      "Epoch: [0]  [44/56]  eta: 0:00:47  lr: 0.000000  loss: 0.6953 (0.6959)  BCE_Loss: 0.6953 (0.6959)  time: 3.7560  data: 3.1891  max mem: 6498\n",
      "Epoch: [0]  [45/56]  eta: 0:00:43  lr: 0.000000  loss: 0.6958 (0.6959)  BCE_Loss: 0.6958 (0.6959)  time: 3.8686  data: 3.3010  max mem: 6498\n",
      "Epoch: [0]  [46/56]  eta: 0:00:38  lr: 0.000000  loss: 0.6953 (0.6958)  BCE_Loss: 0.6953 (0.6958)  time: 3.7817  data: 3.2142  max mem: 6498\n",
      "Epoch: [0]  [47/56]  eta: 0:00:34  lr: 0.000000  loss: 0.6952 (0.6955)  BCE_Loss: 0.6952 (0.6955)  time: 3.7818  data: 3.2142  max mem: 6498\n",
      "Epoch: [0]  [48/56]  eta: 0:00:31  lr: 0.000000  loss: 0.6950 (0.6955)  BCE_Loss: 0.6950 (0.6955)  time: 3.7390  data: 3.1712  max mem: 6498\n",
      "Epoch: [0]  [49/56]  eta: 0:00:27  lr: 0.000000  loss: 0.6950 (0.6954)  BCE_Loss: 0.6950 (0.6954)  time: 3.8451  data: 3.2771  max mem: 6498\n",
      "Epoch: [0]  [50/56]  eta: 0:00:23  lr: 0.000000  loss: 0.6938 (0.6953)  BCE_Loss: 0.6938 (0.6953)  time: 3.8204  data: 3.2523  max mem: 6498\n",
      "Epoch: [0]  [51/56]  eta: 0:00:19  lr: 0.000000  loss: 0.6932 (0.6952)  BCE_Loss: 0.6932 (0.6952)  time: 3.8205  data: 3.2523  max mem: 6498\n",
      "Epoch: [0]  [52/56]  eta: 0:00:15  lr: 0.000000  loss: 0.6932 (0.6953)  BCE_Loss: 0.6932 (0.6953)  time: 3.6865  data: 3.1182  max mem: 6498\n",
      "Epoch: [0]  [53/56]  eta: 0:00:11  lr: 0.000000  loss: 0.6932 (0.6954)  BCE_Loss: 0.6932 (0.6954)  time: 3.7415  data: 3.1733  max mem: 6498\n",
      "Epoch: [0]  [54/56]  eta: 0:00:07  lr: 0.000000  loss: 0.6950 (0.6955)  BCE_Loss: 0.6950 (0.6955)  time: 3.8163  data: 3.2480  max mem: 6498\n",
      "Epoch: [0]  [55/56]  eta: 0:00:03  lr: 0.000000  loss: 0.6952 (0.6955)  BCE_Loss: 0.6952 (0.6955)  time: 3.8163  data: 3.2480  max mem: 6498\n",
      "Epoch: [0] Total time: 0:03:34 (3.8299 s / it)\n",
      "Averaged train_stats:  {'lr': 0.0, 'loss': 0.6954591, 'BCE_Loss': 0.6954591}\n",
      "Valid:  [0/6]  eta: 0:01:27  loss: 0.6939 (0.6939)  BCE_Loss: 0.6939 (0.6939)  time: 14.5865  data: 14.0469  max mem: 6498\n",
      "Valid:  [1/6]  eta: 0:00:37  loss: 0.6936 (0.6938)  BCE_Loss: 0.6936 (0.6938)  time: 7.5593  data: 7.0235  max mem: 6498\n",
      "Valid:  [2/6]  eta: 0:00:20  loss: 0.6936 (0.6931)  BCE_Loss: 0.6936 (0.6931)  time: 5.2167  data: 4.6823  max mem: 6498\n",
      "Valid:  [3/6]  eta: 0:00:12  loss: 0.6918 (0.6928)  BCE_Loss: 0.6918 (0.6928)  time: 4.0452  data: 3.5117  max mem: 6498\n",
      "Valid:  [4/6]  eta: 0:00:08  loss: 0.6918 (0.6925)  BCE_Loss: 0.6918 (0.6925)  time: 4.3106  data: 3.7774  max mem: 6498\n",
      "Valid:  [5/6]  eta: 0:00:03  loss: 0.6918 (0.6923)  BCE_Loss: 0.6918 (0.6923)  time: 3.6105  data: 3.1479  max mem: 6498\n",
      "Valid: Total time: 0:00:21 (3.6223 s / it)\n",
      "Averaged valid_stats:  {'loss': 0.6922987, 'BCE_Loss': 0.6922987, 'auc': 0.5966743, 'f1': 0.6716233, 'acc': 0.5753205, 'sen': 0.6948718, 'spe': 0.3760684}\n",
      "Freeze encoder ...!\n",
      "Epoch: [1]  [ 0/56]  eta: 0:12:15  lr: 0.000000  loss: 0.6917 (0.6917)  BCE_Loss: 0.6917 (0.6917)  time: 13.1417  data: 12.5735  max mem: 6498\n",
      "Epoch: [1]  [ 1/56]  eta: 0:06:17  lr: 0.000000  loss: 0.6917 (0.6953)  BCE_Loss: 0.6917 (0.6953)  time: 6.8551  data: 6.2868  max mem: 6498\n",
      "Epoch: [1]  [ 2/56]  eta: 0:04:17  lr: 0.000000  loss: 0.6970 (0.6959)  BCE_Loss: 0.6970 (0.6959)  time: 4.7596  data: 4.1912  max mem: 6498\n",
      "Epoch: [1]  [ 3/56]  eta: 0:03:16  lr: 0.000000  loss: 0.6922 (0.6950)  BCE_Loss: 0.6922 (0.6950)  time: 3.7117  data: 3.1434  max mem: 6498\n",
      "Epoch: [1]  [ 4/56]  eta: 0:04:21  lr: 0.000000  loss: 0.6970 (0.6960)  BCE_Loss: 0.6970 (0.6960)  time: 5.0254  data: 4.4572  max mem: 6498\n",
      "Epoch: [1]  [ 5/56]  eta: 0:03:38  lr: 0.000000  loss: 0.6970 (0.6976)  BCE_Loss: 0.6970 (0.6976)  time: 4.2825  data: 3.7143  max mem: 6498\n",
      "Epoch: [1]  [ 6/56]  eta: 0:03:07  lr: 0.000000  loss: 0.6988 (0.6979)  BCE_Loss: 0.6988 (0.6979)  time: 3.7517  data: 3.1837  max mem: 6498\n",
      "Epoch: [1]  [ 7/56]  eta: 0:02:44  lr: 0.000000  loss: 0.6970 (0.6966)  BCE_Loss: 0.6970 (0.6966)  time: 3.3538  data: 2.7858  max mem: 6498\n",
      "Epoch: [1]  [ 8/56]  eta: 0:03:12  lr: 0.000000  loss: 0.6979 (0.6967)  BCE_Loss: 0.6979 (0.6967)  time: 4.0146  data: 3.4464  max mem: 6498\n",
      "Epoch: [1]  [ 9/56]  eta: 0:02:52  lr: 0.000000  loss: 0.6970 (0.6967)  BCE_Loss: 0.6970 (0.6967)  time: 3.6700  data: 3.1017  max mem: 6498\n",
      "Epoch: [1]  [10/56]  eta: 0:02:35  lr: 0.000000  loss: 0.6979 (0.6972)  BCE_Loss: 0.6979 (0.6972)  time: 3.3881  data: 2.8198  max mem: 6498\n",
      "Epoch: [1]  [11/56]  eta: 0:02:21  lr: 0.000000  loss: 0.6970 (0.6956)  BCE_Loss: 0.6970 (0.6956)  time: 3.1531  data: 2.5848  max mem: 6498\n",
      "Epoch: [1]  [12/56]  eta: 0:02:41  lr: 0.000000  loss: 0.6979 (0.6959)  BCE_Loss: 0.6979 (0.6959)  time: 3.6695  data: 3.1010  max mem: 6498\n",
      "Epoch: [1]  [13/56]  eta: 0:02:28  lr: 0.000000  loss: 0.6979 (0.6962)  BCE_Loss: 0.6979 (0.6962)  time: 3.4480  data: 2.8795  max mem: 6498\n",
      "Epoch: [1]  [14/56]  eta: 0:02:16  lr: 0.000000  loss: 0.6988 (0.6965)  BCE_Loss: 0.6988 (0.6965)  time: 3.2560  data: 2.6875  max mem: 6498\n",
      "Epoch: [1]  [15/56]  eta: 0:02:06  lr: 0.000000  loss: 0.6988 (0.6967)  BCE_Loss: 0.6988 (0.6967)  time: 3.0881  data: 2.5196  max mem: 6498\n",
      "Epoch: [1]  [16/56]  eta: 0:02:16  lr: 0.000000  loss: 0.6989 (0.6975)  BCE_Loss: 0.6989 (0.6975)  time: 3.4032  data: 2.8347  max mem: 6498\n",
      "Epoch: [1]  [17/56]  eta: 0:02:06  lr: 0.000000  loss: 0.6989 (0.6976)  BCE_Loss: 0.6989 (0.6976)  time: 3.2457  data: 2.6772  max mem: 6498\n",
      "Epoch: [1]  [18/56]  eta: 0:01:57  lr: 0.000000  loss: 0.6989 (0.6976)  BCE_Loss: 0.6989 (0.6976)  time: 3.1048  data: 2.5363  max mem: 6498\n",
      "Epoch: [1]  [19/56]  eta: 0:01:50  lr: 0.000000  loss: 0.6989 (0.6980)  BCE_Loss: 0.6989 (0.6980)  time: 2.9781  data: 2.4095  max mem: 6498\n",
      "Epoch: [1]  [20/56]  eta: 0:01:59  lr: 0.000000  loss: 0.6995 (0.6984)  BCE_Loss: 0.6995 (0.6984)  time: 2.8416  data: 2.2730  max mem: 6498\n",
      "Epoch: [1]  [21/56]  eta: 0:01:52  lr: 0.000000  loss: 0.6995 (0.6982)  BCE_Loss: 0.6995 (0.6982)  time: 2.8414  data: 2.2730  max mem: 6498\n",
      "Epoch: [1]  [22/56]  eta: 0:01:45  lr: 0.000000  loss: 0.6995 (0.6984)  BCE_Loss: 0.6995 (0.6984)  time: 2.8415  data: 2.2730  max mem: 6498\n",
      "Epoch: [1]  [23/56]  eta: 0:01:38  lr: 0.000000  loss: 0.6995 (0.6984)  BCE_Loss: 0.6995 (0.6984)  time: 2.8415  data: 2.2730  max mem: 6498\n",
      "Epoch: [1]  [24/56]  eta: 0:01:45  lr: 0.000000  loss: 0.6995 (0.6981)  BCE_Loss: 0.6995 (0.6981)  time: 2.8499  data: 2.2814  max mem: 6498\n",
      "Epoch: [1]  [25/56]  eta: 0:01:38  lr: 0.000000  loss: 0.6989 (0.6977)  BCE_Loss: 0.6989 (0.6977)  time: 2.8499  data: 2.2814  max mem: 6498\n",
      "Epoch: [1]  [26/56]  eta: 0:01:32  lr: 0.000000  loss: 0.6985 (0.6974)  BCE_Loss: 0.6985 (0.6974)  time: 2.8500  data: 2.2814  max mem: 6498\n",
      "Epoch: [1]  [27/56]  eta: 0:01:26  lr: 0.000000  loss: 0.6985 (0.6973)  BCE_Loss: 0.6985 (0.6973)  time: 2.8500  data: 2.2814  max mem: 6498\n",
      "Epoch: [1]  [28/56]  eta: 0:01:30  lr: 0.000000  loss: 0.6985 (0.6970)  BCE_Loss: 0.6985 (0.6970)  time: 2.8621  data: 2.2935  max mem: 6498\n",
      "Epoch: [1]  [29/56]  eta: 0:01:24  lr: 0.000000  loss: 0.6989 (0.6972)  BCE_Loss: 0.6989 (0.6972)  time: 2.8621  data: 2.2935  max mem: 6498\n",
      "Epoch: [1]  [30/56]  eta: 0:01:19  lr: 0.000000  loss: 0.6989 (0.6973)  BCE_Loss: 0.6989 (0.6973)  time: 2.8620  data: 2.2935  max mem: 6498\n",
      "Epoch: [1]  [31/56]  eta: 0:01:14  lr: 0.000000  loss: 0.6989 (0.6972)  BCE_Loss: 0.6989 (0.6972)  time: 2.8621  data: 2.2935  max mem: 6498\n",
      "Epoch: [1]  [32/56]  eta: 0:01:15  lr: 0.000000  loss: 0.6985 (0.6970)  BCE_Loss: 0.6985 (0.6970)  time: 2.8267  data: 2.2582  max mem: 6498\n",
      "Epoch: [1]  [33/56]  eta: 0:01:10  lr: 0.000000  loss: 0.6985 (0.6972)  BCE_Loss: 0.6985 (0.6972)  time: 2.8266  data: 2.2582  max mem: 6498\n",
      "Epoch: [1]  [34/56]  eta: 0:01:06  lr: 0.000000  loss: 0.6985 (0.6977)  BCE_Loss: 0.6985 (0.6977)  time: 2.8266  data: 2.2582  max mem: 6498\n",
      "Epoch: [1]  [35/56]  eta: 0:01:01  lr: 0.000000  loss: 0.6971 (0.6976)  BCE_Loss: 0.6971 (0.6976)  time: 2.8265  data: 2.2582  max mem: 6498\n",
      "Epoch: [1]  [36/56]  eta: 0:01:02  lr: 0.000000  loss: 0.6971 (0.6977)  BCE_Loss: 0.6971 (0.6977)  time: 2.8704  data: 2.3021  max mem: 6498\n",
      "Epoch: [1]  [37/56]  eta: 0:00:57  lr: 0.000000  loss: 0.6971 (0.6981)  BCE_Loss: 0.6971 (0.6981)  time: 2.8703  data: 2.3021  max mem: 6498\n",
      "Epoch: [1]  [38/56]  eta: 0:00:53  lr: 0.000000  loss: 0.6952 (0.6979)  BCE_Loss: 0.6952 (0.6979)  time: 2.8703  data: 2.3021  max mem: 6498\n",
      "Epoch: [1]  [39/56]  eta: 0:00:49  lr: 0.000000  loss: 0.6940 (0.6976)  BCE_Loss: 0.6940 (0.6976)  time: 2.8703  data: 2.3021  max mem: 6498\n",
      "Epoch: [1]  [40/56]  eta: 0:00:49  lr: 0.000000  loss: 0.6939 (0.6975)  BCE_Loss: 0.6939 (0.6975)  time: 2.8181  data: 2.2499  max mem: 6498\n",
      "Epoch: [1]  [41/56]  eta: 0:00:45  lr: 0.000000  loss: 0.6940 (0.6974)  BCE_Loss: 0.6940 (0.6974)  time: 2.8183  data: 2.2499  max mem: 6498\n",
      "Epoch: [1]  [42/56]  eta: 0:00:41  lr: 0.000000  loss: 0.6940 (0.6974)  BCE_Loss: 0.6940 (0.6974)  time: 2.8182  data: 2.2499  max mem: 6498\n",
      "Epoch: [1]  [43/56]  eta: 0:00:37  lr: 0.000000  loss: 0.6940 (0.6975)  BCE_Loss: 0.6940 (0.6975)  time: 2.8182  data: 2.2499  max mem: 6498\n",
      "Epoch: [1]  [44/56]  eta: 0:00:36  lr: 0.000000  loss: 0.6952 (0.6975)  BCE_Loss: 0.6952 (0.6975)  time: 2.7235  data: 2.1552  max mem: 6498\n",
      "Epoch: [1]  [45/56]  eta: 0:00:32  lr: 0.000000  loss: 0.6958 (0.6976)  BCE_Loss: 0.6958 (0.6976)  time: 2.7236  data: 2.1552  max mem: 6498\n",
      "Epoch: [1]  [46/56]  eta: 0:00:29  lr: 0.000000  loss: 0.6958 (0.6976)  BCE_Loss: 0.6958 (0.6976)  time: 2.7236  data: 2.1552  max mem: 6498\n",
      "Epoch: [1]  [47/56]  eta: 0:00:25  lr: 0.000000  loss: 0.6972 (0.6976)  BCE_Loss: 0.6972 (0.6976)  time: 2.7236  data: 2.1552  max mem: 6498\n",
      "Epoch: [1]  [48/56]  eta: 0:00:24  lr: 0.000000  loss: 0.6973 (0.6976)  BCE_Loss: 0.6973 (0.6976)  time: 2.7482  data: 2.1798  max mem: 6498\n",
      "Epoch: [1]  [49/56]  eta: 0:00:20  lr: 0.000000  loss: 0.6973 (0.6977)  BCE_Loss: 0.6973 (0.6977)  time: 2.7482  data: 2.1798  max mem: 6498\n",
      "Epoch: [1]  [50/56]  eta: 0:00:17  lr: 0.000000  loss: 0.6972 (0.6976)  BCE_Loss: 0.6972 (0.6976)  time: 2.7482  data: 2.1798  max mem: 6498\n",
      "Epoch: [1]  [51/56]  eta: 0:00:14  lr: 0.000000  loss: 0.6972 (0.6976)  BCE_Loss: 0.6972 (0.6976)  time: 2.7481  data: 2.1798  max mem: 6498\n",
      "Epoch: [1]  [52/56]  eta: 0:00:12  lr: 0.000000  loss: 0.6973 (0.6976)  BCE_Loss: 0.6973 (0.6976)  time: 2.7620  data: 2.1937  max mem: 6498\n",
      "Epoch: [1]  [53/56]  eta: 0:00:08  lr: 0.000000  loss: 0.6972 (0.6974)  BCE_Loss: 0.6972 (0.6974)  time: 2.7620  data: 2.1937  max mem: 6498\n",
      "Epoch: [1]  [54/56]  eta: 0:00:05  lr: 0.000000  loss: 0.6958 (0.6974)  BCE_Loss: 0.6958 (0.6974)  time: 2.7619  data: 2.1937  max mem: 6498\n",
      "Epoch: [1]  [55/56]  eta: 0:00:02  lr: 0.000000  loss: 0.6959 (0.6974)  BCE_Loss: 0.6959 (0.6974)  time: 2.7620  data: 2.1937  max mem: 6498\n",
      "Epoch: [1] Total time: 0:02:41 (2.8807 s / it)\n",
      "Averaged train_stats:  {'lr': 0.0, 'loss': 0.697353, 'BCE_Loss': 0.697353}\n",
      "Valid:  [0/6]  eta: 0:01:08  loss: 0.6807 (0.6807)  BCE_Loss: 0.6807 (0.6807)  time: 11.4706  data: 10.9348  max mem: 6498\n",
      "Valid:  [1/6]  eta: 0:00:30  loss: 0.6807 (0.6809)  BCE_Loss: 0.6807 (0.6809)  time: 6.0025  data: 5.4674  max mem: 6498\n",
      "Valid:  [2/6]  eta: 0:00:16  loss: 0.6812 (0.6894)  BCE_Loss: 0.6812 (0.6894)  time: 4.1792  data: 3.6450  max mem: 6498\n",
      "Valid:  [3/6]  eta: 0:00:09  loss: 0.6812 (0.6937)  BCE_Loss: 0.6812 (0.6937)  time: 3.2679  data: 2.7337  max mem: 6498\n",
      "Valid:  [4/6]  eta: 0:00:06  loss: 0.7064 (0.6966)  BCE_Loss: 0.7064 (0.6966)  time: 3.2758  data: 2.7421  max mem: 6498\n",
      "Valid:  [5/6]  eta: 0:00:02  loss: 0.7064 (0.6983)  BCE_Loss: 0.7064 (0.6983)  time: 2.7482  data: 2.2851  max mem: 6498\n",
      "Valid: Total time: 0:00:16 (2.7630 s / it)\n",
      "Averaged valid_stats:  {'loss': 0.6982528, 'BCE_Loss': 0.6982528, 'auc': 0.4990412, 'f1': 0.0203046, 'acc': 0.3814103, 'sen': 0.0102564, 'spe': 1.0}\n",
      "Freeze encoder ...!\n",
      "Epoch: [2]  [ 0/56]  eta: 0:12:03  lr: 0.000050  loss: 0.6893 (0.6893)  BCE_Loss: 0.6893 (0.6893)  time: 12.9214  data: 12.3494  max mem: 6498\n",
      "Epoch: [2]  [ 1/56]  eta: 0:06:10  lr: 0.000050  loss: 0.6828 (0.6861)  BCE_Loss: 0.6828 (0.6861)  time: 6.7450  data: 6.1747  max mem: 6498\n",
      "Epoch: [2]  [ 2/56]  eta: 0:04:13  lr: 0.000050  loss: 0.6828 (0.6847)  BCE_Loss: 0.6828 (0.6847)  time: 4.6874  data: 4.1165  max mem: 6498\n",
      "Epoch: [2]  [ 3/56]  eta: 0:03:13  lr: 0.000050  loss: 0.6828 (0.6853)  BCE_Loss: 0.6828 (0.6853)  time: 3.6590  data: 3.0875  max mem: 6498\n",
      "Epoch: [2]  [ 4/56]  eta: 0:04:05  lr: 0.000050  loss: 0.6828 (0.6831)  BCE_Loss: 0.6828 (0.6831)  time: 4.7290  data: 4.1583  max mem: 6498\n",
      "Epoch: [2]  [ 5/56]  eta: 0:03:28  lr: 0.000050  loss: 0.6828 (0.6852)  BCE_Loss: 0.6828 (0.6852)  time: 4.0952  data: 3.5251  max mem: 6498\n",
      "Epoch: [2]  [ 6/56]  eta: 0:02:59  lr: 0.000050  loss: 0.6872 (0.6889)  BCE_Loss: 0.6872 (0.6889)  time: 3.5915  data: 3.0215  max mem: 6498\n",
      "Epoch: [2]  [ 7/56]  eta: 0:02:37  lr: 0.000050  loss: 0.6872 (0.6918)  BCE_Loss: 0.6872 (0.6918)  time: 3.2138  data: 2.6438  max mem: 6498\n",
      "Epoch: [2]  [ 8/56]  eta: 0:03:03  lr: 0.000050  loss: 0.6872 (0.6905)  BCE_Loss: 0.6872 (0.6905)  time: 3.8194  data: 3.2497  max mem: 6498\n",
      "Epoch: [2]  [ 9/56]  eta: 0:02:45  lr: 0.000050  loss: 0.6828 (0.6888)  BCE_Loss: 0.6828 (0.6888)  time: 3.5280  data: 2.9584  max mem: 6498\n",
      "Epoch: [2]  [10/56]  eta: 0:02:29  lr: 0.000050  loss: 0.6872 (0.6903)  BCE_Loss: 0.6872 (0.6903)  time: 3.2590  data: 2.6895  max mem: 6498\n",
      "Epoch: [2]  [11/56]  eta: 0:02:16  lr: 0.000050  loss: 0.6847 (0.6898)  BCE_Loss: 0.6847 (0.6898)  time: 3.0348  data: 2.4653  max mem: 6498\n",
      "Epoch: [2]  [12/56]  eta: 0:02:35  lr: 0.000050  loss: 0.6872 (0.6908)  BCE_Loss: 0.6872 (0.6908)  time: 3.5229  data: 2.9535  max mem: 6498\n",
      "Epoch: [2]  [13/56]  eta: 0:02:24  lr: 0.000050  loss: 0.6849 (0.6904)  BCE_Loss: 0.6849 (0.6904)  time: 3.3500  data: 2.7806  max mem: 6498\n",
      "Epoch: [2]  [14/56]  eta: 0:02:12  lr: 0.000050  loss: 0.6856 (0.6901)  BCE_Loss: 0.6856 (0.6901)  time: 3.1647  data: 2.5952  max mem: 6498\n",
      "Epoch: [2]  [15/56]  eta: 0:02:03  lr: 0.000050  loss: 0.6849 (0.6897)  BCE_Loss: 0.6849 (0.6897)  time: 3.0025  data: 2.4330  max mem: 6498\n",
      "Epoch: [2]  [16/56]  eta: 0:02:15  lr: 0.000050  loss: 0.6856 (0.6903)  BCE_Loss: 0.6856 (0.6903)  time: 3.3845  data: 2.8151  max mem: 6498\n",
      "Epoch: [2]  [17/56]  eta: 0:02:06  lr: 0.000050  loss: 0.6856 (0.6905)  BCE_Loss: 0.6856 (0.6905)  time: 3.2335  data: 2.6642  max mem: 6498\n",
      "Epoch: [2]  [18/56]  eta: 0:01:57  lr: 0.000050  loss: 0.6856 (0.6898)  BCE_Loss: 0.6856 (0.6898)  time: 3.0932  data: 2.5239  max mem: 6498\n",
      "Epoch: [2]  [19/56]  eta: 0:01:49  lr: 0.000050  loss: 0.6856 (0.6901)  BCE_Loss: 0.6856 (0.6901)  time: 2.9670  data: 2.3978  max mem: 6498\n",
      "Epoch: [2]  [20/56]  eta: 0:01:57  lr: 0.000050  loss: 0.6856 (0.6899)  BCE_Loss: 0.6856 (0.6899)  time: 2.7854  data: 2.2161  max mem: 6498\n",
      "Epoch: [2]  [21/56]  eta: 0:01:51  lr: 0.000050  loss: 0.6858 (0.6902)  BCE_Loss: 0.6858 (0.6902)  time: 2.8351  data: 2.2658  max mem: 6498\n",
      "Epoch: [2]  [22/56]  eta: 0:01:44  lr: 0.000050  loss: 0.6858 (0.6899)  BCE_Loss: 0.6858 (0.6899)  time: 2.8348  data: 2.2658  max mem: 6498\n",
      "Epoch: [2]  [23/56]  eta: 0:01:38  lr: 0.000050  loss: 0.6858 (0.6909)  BCE_Loss: 0.6858 (0.6909)  time: 2.8346  data: 2.2658  max mem: 6498\n",
      "Epoch: [2]  [24/56]  eta: 0:01:41  lr: 0.000050  loss: 0.6858 (0.6906)  BCE_Loss: 0.6858 (0.6906)  time: 2.7912  data: 2.2223  max mem: 6498\n",
      "Epoch: [2]  [25/56]  eta: 0:01:36  lr: 0.000050  loss: 0.6856 (0.6903)  BCE_Loss: 0.6856 (0.6903)  time: 2.8339  data: 2.2650  max mem: 6498\n",
      "Epoch: [2]  [26/56]  eta: 0:01:31  lr: 0.000050  loss: 0.6849 (0.6901)  BCE_Loss: 0.6849 (0.6901)  time: 2.8418  data: 2.2729  max mem: 6498\n",
      "Epoch: [2]  [27/56]  eta: 0:01:25  lr: 0.000050  loss: 0.6849 (0.6908)  BCE_Loss: 0.6849 (0.6908)  time: 2.8418  data: 2.2729  max mem: 6498\n",
      "Epoch: [2]  [28/56]  eta: 0:01:27  lr: 0.000050  loss: 0.6853 (0.6906)  BCE_Loss: 0.6853 (0.6906)  time: 2.7957  data: 2.2268  max mem: 6498\n",
      "Epoch: [2]  [29/56]  eta: 0:01:23  lr: 0.000050  loss: 0.6856 (0.6912)  BCE_Loss: 0.6856 (0.6912)  time: 2.9026  data: 2.3338  max mem: 6498\n",
      "Epoch: [2]  [30/56]  eta: 0:01:18  lr: 0.000050  loss: 0.6856 (0.6914)  BCE_Loss: 0.6856 (0.6914)  time: 2.9027  data: 2.3338  max mem: 6498\n",
      "Epoch: [2]  [31/56]  eta: 0:01:13  lr: 0.000050  loss: 0.6856 (0.6908)  BCE_Loss: 0.6856 (0.6908)  time: 2.9026  data: 2.3338  max mem: 6498\n",
      "Epoch: [2]  [32/56]  eta: 0:01:13  lr: 0.000050  loss: 0.6853 (0.6903)  BCE_Loss: 0.6853 (0.6903)  time: 2.7493  data: 2.1804  max mem: 6498\n",
      "Epoch: [2]  [33/56]  eta: 0:01:11  lr: 0.000050  loss: 0.6856 (0.6905)  BCE_Loss: 0.6856 (0.6905)  time: 2.9082  data: 2.3394  max mem: 6498\n",
      "Epoch: [2]  [34/56]  eta: 0:01:06  lr: 0.000050  loss: 0.6858 (0.6905)  BCE_Loss: 0.6858 (0.6905)  time: 2.9082  data: 2.3394  max mem: 6498\n",
      "Epoch: [2]  [35/56]  eta: 0:01:01  lr: 0.000050  loss: 0.6858 (0.6904)  BCE_Loss: 0.6858 (0.6904)  time: 2.9081  data: 2.3394  max mem: 6498\n",
      "Epoch: [2]  [36/56]  eta: 0:01:00  lr: 0.000050  loss: 0.6853 (0.6901)  BCE_Loss: 0.6853 (0.6901)  time: 2.7197  data: 2.1511  max mem: 6498\n",
      "Epoch: [2]  [37/56]  eta: 0:00:58  lr: 0.000050  loss: 0.6850 (0.6899)  BCE_Loss: 0.6850 (0.6899)  time: 2.9471  data: 2.3785  max mem: 6498\n",
      "Epoch: [2]  [38/56]  eta: 0:00:54  lr: 0.000050  loss: 0.6853 (0.6899)  BCE_Loss: 0.6853 (0.6899)  time: 2.9471  data: 2.3785  max mem: 6498\n",
      "Epoch: [2]  [39/56]  eta: 0:00:50  lr: 0.000050  loss: 0.6850 (0.6897)  BCE_Loss: 0.6850 (0.6897)  time: 2.9471  data: 2.3785  max mem: 6498\n",
      "Epoch: [2]  [40/56]  eta: 0:00:47  lr: 0.000050  loss: 0.6850 (0.6897)  BCE_Loss: 0.6850 (0.6897)  time: 2.6782  data: 2.1099  max mem: 6498\n",
      "Epoch: [2]  [41/56]  eta: 0:00:45  lr: 0.000050  loss: 0.6850 (0.6900)  BCE_Loss: 0.6850 (0.6900)  time: 2.8905  data: 2.3221  max mem: 6498\n",
      "Epoch: [2]  [42/56]  eta: 0:00:41  lr: 0.000050  loss: 0.6853 (0.6901)  BCE_Loss: 0.6853 (0.6901)  time: 2.8907  data: 2.3221  max mem: 6498\n",
      "Epoch: [2]  [43/56]  eta: 0:00:38  lr: 0.000050  loss: 0.6850 (0.6899)  BCE_Loss: 0.6850 (0.6899)  time: 2.8907  data: 2.3221  max mem: 6498\n",
      "Epoch: [2]  [44/56]  eta: 0:00:35  lr: 0.000050  loss: 0.6853 (0.6901)  BCE_Loss: 0.6853 (0.6901)  time: 2.6717  data: 2.1031  max mem: 6498\n",
      "Epoch: [2]  [45/56]  eta: 0:00:33  lr: 0.000050  loss: 0.6853 (0.6897)  BCE_Loss: 0.6853 (0.6897)  time: 2.9346  data: 2.3659  max mem: 6498\n",
      "Epoch: [2]  [46/56]  eta: 0:00:29  lr: 0.000050  loss: 0.6895 (0.6903)  BCE_Loss: 0.6895 (0.6903)  time: 2.9266  data: 2.3580  max mem: 6498\n",
      "Epoch: [2]  [47/56]  eta: 0:00:26  lr: 0.000050  loss: 0.6895 (0.6906)  BCE_Loss: 0.6895 (0.6906)  time: 2.9267  data: 2.3580  max mem: 6498\n",
      "Epoch: [2]  [48/56]  eta: 0:00:23  lr: 0.000050  loss: 0.6906 (0.6909)  BCE_Loss: 0.6906 (0.6909)  time: 2.6575  data: 2.0888  max mem: 6498\n",
      "Epoch: [2]  [49/56]  eta: 0:00:21  lr: 0.000050  loss: 0.6906 (0.6911)  BCE_Loss: 0.6906 (0.6911)  time: 2.8654  data: 2.2966  max mem: 6498\n",
      "Epoch: [2]  [50/56]  eta: 0:00:17  lr: 0.000050  loss: 0.6895 (0.6910)  BCE_Loss: 0.6895 (0.6910)  time: 2.8654  data: 2.2966  max mem: 6498\n",
      "Epoch: [2]  [51/56]  eta: 0:00:14  lr: 0.000050  loss: 0.6895 (0.6906)  BCE_Loss: 0.6895 (0.6906)  time: 2.8654  data: 2.2966  max mem: 6498\n",
      "Epoch: [2]  [52/56]  eta: 0:00:11  lr: 0.000050  loss: 0.6895 (0.6905)  BCE_Loss: 0.6895 (0.6905)  time: 2.6757  data: 2.1068  max mem: 6498\n",
      "Epoch: [2]  [53/56]  eta: 0:00:08  lr: 0.000050  loss: 0.6895 (0.6905)  BCE_Loss: 0.6895 (0.6905)  time: 2.7984  data: 2.2295  max mem: 6498\n",
      "Epoch: [2]  [54/56]  eta: 0:00:05  lr: 0.000050  loss: 0.6895 (0.6905)  BCE_Loss: 0.6895 (0.6905)  time: 2.8056  data: 2.2367  max mem: 6498\n",
      "Epoch: [2]  [55/56]  eta: 0:00:02  lr: 0.000050  loss: 0.6895 (0.6904)  BCE_Loss: 0.6895 (0.6904)  time: 2.8056  data: 2.2367  max mem: 6498\n",
      "Epoch: [2] Total time: 0:02:42 (2.9008 s / it)\n",
      "Averaged train_stats:  {'lr': 5e-05, 'loss': 0.6904379, 'BCE_Loss': 0.6904379}\n",
      "Valid:  [0/6]  eta: 0:01:07  loss: 0.6519 (0.6519)  BCE_Loss: 0.6519 (0.6519)  time: 11.3067  data: 10.7727  max mem: 6498\n",
      "Valid:  [1/6]  eta: 0:00:29  loss: 0.6519 (0.6540)  BCE_Loss: 0.6519 (0.6540)  time: 5.9196  data: 5.3864  max mem: 6498\n",
      "Valid:  [2/6]  eta: 0:00:16  loss: 0.6561 (0.6733)  BCE_Loss: 0.6561 (0.6733)  time: 4.1240  data: 3.5909  max mem: 6498\n",
      "Valid:  [3/6]  eta: 0:00:09  loss: 0.6561 (0.6827)  BCE_Loss: 0.6561 (0.6827)  time: 3.2264  data: 2.6932  max mem: 6498\n",
      "Valid:  [4/6]  eta: 0:00:06  loss: 0.7059 (0.6873)  BCE_Loss: 0.7059 (0.6873)  time: 3.2457  data: 2.7126  max mem: 6498\n",
      "Valid:  [5/6]  eta: 0:00:02  loss: 0.7001 (0.6895)  BCE_Loss: 0.7001 (0.6895)  time: 2.7231  data: 2.2605  max mem: 6498\n",
      "Valid: Total time: 0:00:16 (2.7332 s / it)\n",
      "Averaged valid_stats:  {'loss': 0.6894615, 'BCE_Loss': 0.6894615, 'auc': 0.7050186, 'f1': 0.4503817, 'acc': 0.5384616, 'sen': 0.3025641, 'spe': 0.9316239}\n",
      "Freeze encoder ...!\n",
      "Epoch: [3]  [ 0/56]  eta: 0:11:14  lr: 0.000100  loss: 0.6791 (0.6791)  BCE_Loss: 0.6791 (0.6791)  time: 12.0360  data: 11.4640  max mem: 6498\n",
      "Epoch: [3]  [ 1/56]  eta: 0:05:46  lr: 0.000100  loss: 0.6744 (0.6768)  BCE_Loss: 0.6744 (0.6768)  time: 6.3032  data: 5.7321  max mem: 6498\n",
      "Epoch: [3]  [ 2/56]  eta: 0:03:57  lr: 0.000100  loss: 0.6791 (0.6855)  BCE_Loss: 0.6791 (0.6855)  time: 4.3913  data: 3.8214  max mem: 6498\n",
      "Epoch: [3]  [ 3/56]  eta: 0:03:02  lr: 0.000100  loss: 0.6791 (0.6889)  BCE_Loss: 0.6791 (0.6889)  time: 3.4356  data: 2.8661  max mem: 6498\n",
      "Epoch: [3]  [ 4/56]  eta: 0:04:04  lr: 0.000100  loss: 0.6980 (0.6907)  BCE_Loss: 0.6980 (0.6907)  time: 4.6953  data: 4.1259  max mem: 6498\n",
      "Epoch: [3]  [ 5/56]  eta: 0:03:24  lr: 0.000100  loss: 0.6936 (0.6912)  BCE_Loss: 0.6936 (0.6912)  time: 4.0075  data: 3.4383  max mem: 6498\n",
      "Epoch: [3]  [ 6/56]  eta: 0:02:55  lr: 0.000100  loss: 0.6938 (0.6915)  BCE_Loss: 0.6938 (0.6915)  time: 3.5162  data: 2.9471  max mem: 6498\n",
      "Epoch: [3]  [ 7/56]  eta: 0:02:34  lr: 0.000100  loss: 0.6936 (0.6905)  BCE_Loss: 0.6936 (0.6905)  time: 3.1478  data: 2.5787  max mem: 6498\n",
      "Epoch: [3]  [ 8/56]  eta: 0:03:00  lr: 0.000100  loss: 0.6936 (0.6903)  BCE_Loss: 0.6936 (0.6903)  time: 3.7588  data: 3.1895  max mem: 6498\n",
      "Epoch: [3]  [ 9/56]  eta: 0:02:41  lr: 0.000100  loss: 0.6892 (0.6900)  BCE_Loss: 0.6892 (0.6900)  time: 3.4397  data: 2.8706  max mem: 6498\n",
      "Epoch: [3]  [10/56]  eta: 0:02:26  lr: 0.000100  loss: 0.6936 (0.6907)  BCE_Loss: 0.6936 (0.6907)  time: 3.1788  data: 2.6096  max mem: 6498\n",
      "Epoch: [3]  [11/56]  eta: 0:02:13  lr: 0.000100  loss: 0.6936 (0.6916)  BCE_Loss: 0.6936 (0.6916)  time: 2.9613  data: 2.3922  max mem: 6498\n",
      "Epoch: [3]  [12/56]  eta: 0:02:30  lr: 0.000100  loss: 0.6936 (0.6907)  BCE_Loss: 0.6936 (0.6907)  time: 3.4144  data: 2.8454  max mem: 6498\n",
      "Epoch: [3]  [13/56]  eta: 0:02:18  lr: 0.000100  loss: 0.6892 (0.6891)  BCE_Loss: 0.6892 (0.6891)  time: 3.2112  data: 2.6422  max mem: 6498\n",
      "Epoch: [3]  [14/56]  eta: 0:02:07  lr: 0.000100  loss: 0.6936 (0.6909)  BCE_Loss: 0.6936 (0.6909)  time: 3.0350  data: 2.4660  max mem: 6498\n",
      "Epoch: [3]  [15/56]  eta: 0:01:58  lr: 0.000100  loss: 0.6905 (0.6909)  BCE_Loss: 0.6905 (0.6909)  time: 2.8809  data: 2.3119  max mem: 6498\n",
      "Epoch: [3]  [16/56]  eta: 0:02:08  lr: 0.000100  loss: 0.6905 (0.6900)  BCE_Loss: 0.6905 (0.6900)  time: 3.2221  data: 2.6532  max mem: 6498\n",
      "Epoch: [3]  [17/56]  eta: 0:02:01  lr: 0.000100  loss: 0.6905 (0.6911)  BCE_Loss: 0.6905 (0.6911)  time: 3.1220  data: 2.5531  max mem: 6498\n",
      "Epoch: [3]  [18/56]  eta: 0:01:53  lr: 0.000100  loss: 0.6905 (0.6908)  BCE_Loss: 0.6905 (0.6908)  time: 2.9876  data: 2.4187  max mem: 6498\n",
      "Epoch: [3]  [19/56]  eta: 0:01:46  lr: 0.000100  loss: 0.6905 (0.6912)  BCE_Loss: 0.6905 (0.6912)  time: 2.8779  data: 2.3090  max mem: 6498\n",
      "Epoch: [3]  [20/56]  eta: 0:01:53  lr: 0.000100  loss: 0.6905 (0.6911)  BCE_Loss: 0.6905 (0.6911)  time: 2.7150  data: 2.1463  max mem: 6498\n",
      "Epoch: [3]  [21/56]  eta: 0:01:46  lr: 0.000100  loss: 0.6936 (0.6915)  BCE_Loss: 0.6936 (0.6915)  time: 2.7150  data: 2.1463  max mem: 6498\n",
      "Epoch: [3]  [22/56]  eta: 0:01:39  lr: 0.000100  loss: 0.6905 (0.6910)  BCE_Loss: 0.6905 (0.6910)  time: 2.7150  data: 2.1463  max mem: 6498\n",
      "Epoch: [3]  [23/56]  eta: 0:01:34  lr: 0.000100  loss: 0.6905 (0.6910)  BCE_Loss: 0.6905 (0.6910)  time: 2.7352  data: 2.1666  max mem: 6498\n",
      "Epoch: [3]  [24/56]  eta: 0:01:38  lr: 0.000100  loss: 0.6905 (0.6912)  BCE_Loss: 0.6905 (0.6912)  time: 2.6740  data: 2.1054  max mem: 6498\n",
      "Epoch: [3]  [25/56]  eta: 0:01:32  lr: 0.000100  loss: 0.6899 (0.6904)  BCE_Loss: 0.6899 (0.6904)  time: 2.6740  data: 2.1054  max mem: 6498\n",
      "Epoch: [3]  [26/56]  eta: 0:01:27  lr: 0.000100  loss: 0.6892 (0.6899)  BCE_Loss: 0.6892 (0.6899)  time: 2.7178  data: 2.1492  max mem: 6498\n",
      "Epoch: [3]  [27/56]  eta: 0:01:22  lr: 0.000100  loss: 0.6892 (0.6892)  BCE_Loss: 0.6892 (0.6892)  time: 2.7294  data: 2.1608  max mem: 6498\n",
      "Epoch: [3]  [28/56]  eta: 0:01:25  lr: 0.000100  loss: 0.6899 (0.6896)  BCE_Loss: 0.6899 (0.6896)  time: 2.7182  data: 2.1497  max mem: 6498\n",
      "Epoch: [3]  [29/56]  eta: 0:01:19  lr: 0.000100  loss: 0.6905 (0.6902)  BCE_Loss: 0.6905 (0.6902)  time: 2.7182  data: 2.1497  max mem: 6498\n",
      "Epoch: [3]  [30/56]  eta: 0:01:16  lr: 0.000100  loss: 0.6905 (0.6903)  BCE_Loss: 0.6905 (0.6903)  time: 2.7958  data: 2.2273  max mem: 6498\n",
      "Epoch: [3]  [31/56]  eta: 0:01:11  lr: 0.000100  loss: 0.6905 (0.6905)  BCE_Loss: 0.6905 (0.6905)  time: 2.7958  data: 2.2273  max mem: 6498\n",
      "Epoch: [3]  [32/56]  eta: 0:01:11  lr: 0.000100  loss: 0.6924 (0.6911)  BCE_Loss: 0.6924 (0.6911)  time: 2.7304  data: 2.1617  max mem: 6498\n",
      "Epoch: [3]  [33/56]  eta: 0:01:07  lr: 0.000100  loss: 0.6943 (0.6914)  BCE_Loss: 0.6943 (0.6914)  time: 2.7303  data: 2.1617  max mem: 6498\n",
      "Epoch: [3]  [34/56]  eta: 0:01:04  lr: 0.000100  loss: 0.6924 (0.6913)  BCE_Loss: 0.6924 (0.6913)  time: 2.8536  data: 2.2851  max mem: 6498\n",
      "Epoch: [3]  [35/56]  eta: 0:01:00  lr: 0.000100  loss: 0.6924 (0.6913)  BCE_Loss: 0.6924 (0.6913)  time: 2.8535  data: 2.2851  max mem: 6498\n"
     ]
    }
   ],
   "source": [
    "!python train.py \\\n",
    "--data_set 'Pneumonia' \\\n",
    "--model_name 'Downtask_Pneumonia' \\\n",
    "--data_folder_dir \"/mnt/nas125_vol2/kanggilpark/child/jyp_child/data\" \\\n",
    "--batch-size 120 \\\n",
    "--num_workers 4 \\\n",
    "--multi-gpu-mode 'Single' \\\n",
    "--cuda-visible-devices '1' \\\n",
    "--gradual_unfreeze true \\\n",
    "--print-freq 1\\\n",
    "--output_dir outputssss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6838\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "a = glob.glob(\"/mnt/nas125_vol2/kanggilpark/child/jyp_child/data/train/*/*.jpeg\")\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import functools\n",
    "import numpy as np\n",
    "def get_label(x):\n",
    "    return np.float32(x)\n",
    "def get_path(mode, path, x):\n",
    "    return path + '/' + mode + '/' + str(x) + '.png'\n",
    "\n",
    "data_folder_dir = \"/mnt/nas125_vol2/kanggilpark/child/bone_age/data\"\n",
    "mode = 'train'\n",
    "train_df = pd.read_csv(data_folder_dir + \"/train.csv\")\n",
    "# print(train_df)\n",
    "img_list    = list(map(functools.partial(get_path, mode, data_folder_dir),train_df['id']))\n",
    "gender_list = list(map(get_label, train_df['male']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_list = list(map(get_label, train_df['boneage']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "a = 1\n",
    "ab = np.float32(a)\n",
    "print(ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bdaf8225ca8361a4501e29f7f35fc36f6baec4997917c7c75ec5999a985d7c37"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('child_x')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
