{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py \\\n",
    "--data_set 'General_Fracture' \\\n",
    "--model_name 'Downtask_General_Fracture' \\\n",
    "--batch-size 72 \\\n",
    "--num_workers 8 \\\n",
    "--multi-gpu-mode 'Single' \\\n",
    "--cuda-visible-devices '2' \\\n",
    "--gradual_unfreeze true \\\n",
    "--print-freq 1\\\n",
    "--output_dir outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************************\n",
      "*           Training Mode is  Downstream      *\n",
      "***********************************************\n",
      "Dataset Name:  /mnt/nas125_vol2/kanggilpark/child/bone_age/data\n",
      "---------- Model ----------\n",
      "Resume From:  \n",
      "Output Save To:  outputss\n",
      "Visible GPUs:  1\n",
      "---------- Optimizer ----------\n",
      "Learning Rate:  0.0005\n",
      "Batchsize:  140\n",
      "Loading dataset ....\n",
      "Train [Total]  number =  12611\n",
      "Valid [Total]  number =  1425\n",
      "Creating model  : Downtask_RSNA_Boneage\n",
      "Pretrained model: \n",
      "Number of Learnable Params: 26651585\n",
      "RSNA_BAA_Model(\n",
      "  (encoder): ResNet_Feature_Extractor(\n",
      "    (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (fc1): Linear(in_features=2049, out_features=1024, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (drop1): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (drop2): Dropout(p=0.5, inplace=False)\n",
      "  (head): Linear(in_features=1024, out_features=1, bias=True)\n",
      ")\n",
      "Start training for 500 epochs\n",
      "Epoch: [0]  [ 0/90]  eta: 0:17:30  lr: 0.000000  loss: 16225.9375 (16225.9375)  L2_Loss: 16225.9375 (16225.9375)  time: 11.6688  data: 10.2445  max mem: 15186\n",
      "Epoch: [0]  [ 1/90]  eta: 0:09:07  lr: 0.000000  loss: 16225.9375 (17478.2490)  L2_Loss: 16225.9375 (17478.2490)  time: 6.1526  data: 5.1223  max mem: 15480\n",
      "Epoch: [0]  [ 2/90]  eta: 0:10:13  lr: 0.000000  loss: 18730.5605 (17933.3626)  L2_Loss: 18730.5605 (17933.3626)  time: 6.9660  data: 6.0704  max mem: 15480\n",
      "Epoch: [0]  [ 3/90]  eta: 0:08:12  lr: 0.000000  loss: 16631.8203 (17607.9771)  L2_Loss: 16631.8203 (17607.9771)  time: 5.6553  data: 4.8260  max mem: 15480\n",
      "Epoch: [0]  [ 4/90]  eta: 0:08:55  lr: 0.000000  loss: 18215.1855 (17729.4188)  L2_Loss: 18215.1855 (17729.4188)  time: 6.2244  data: 5.4335  max mem: 15480\n",
      "Epoch: [0]  [ 5/90]  eta: 0:07:59  lr: 0.000000  loss: 18215.1855 (17951.3057)  L2_Loss: 18215.1855 (17951.3057)  time: 5.6429  data: 4.8769  max mem: 15480\n",
      "Epoch: [0]  [ 6/90]  eta: 0:08:13  lr: 0.000000  loss: 18215.1855 (17771.7310)  L2_Loss: 18215.1855 (17771.7310)  time: 5.8784  data: 5.1309  max mem: 15480\n",
      "Epoch: [0]  [ 7/90]  eta: 0:07:42  lr: 0.000000  loss: 18215.1855 (17860.0767)  L2_Loss: 18215.1855 (17860.0767)  time: 5.5742  data: 4.8402  max mem: 15480\n",
      "Epoch: [0]  [ 8/90]  eta: 0:08:07  lr: 0.000000  loss: 18215.1855 (17884.3079)  L2_Loss: 18215.1855 (17884.3079)  time: 5.9439  data: 5.2220  max mem: 15480\n",
      "Epoch: [0]  [ 9/90]  eta: 0:07:54  lr: 0.000000  loss: 18215.1855 (18016.7285)  L2_Loss: 18215.1855 (18016.7285)  time: 5.8585  data: 5.1447  max mem: 15480\n",
      "Epoch: [0]  [10/90]  eta: 0:08:34  lr: 0.000000  loss: 18215.1855 (17908.3546)  L2_Loss: 18215.1855 (17908.3546)  time: 6.4308  data: 5.7243  max mem: 15480\n",
      "Epoch: [0]  [11/90]  eta: 0:08:24  lr: 0.000000  loss: 18078.1582 (17892.5156)  L2_Loss: 18078.1582 (17892.5156)  time: 6.3919  data: 5.6914  max mem: 15480\n",
      "Epoch: [0]  [12/90]  eta: 0:08:59  lr: 0.000000  loss: 18215.1855 (17964.6740)  L2_Loss: 18215.1855 (17964.6740)  time: 6.9211  data: 6.2247  max mem: 15480\n",
      "Epoch: [0]  [13/90]  eta: 0:08:38  lr: 0.000000  loss: 18078.1582 (17870.9007)  L2_Loss: 18078.1582 (17870.9007)  time: 6.7367  data: 6.0444  max mem: 15480\n",
      "Epoch: [0]  [14/90]  eta: 0:09:17  lr: 0.000000  loss: 18078.1582 (17806.6194)  L2_Loss: 18078.1582 (17806.6194)  time: 7.3312  data: 6.6425  max mem: 15480\n",
      "Epoch: [0]  [15/90]  eta: 0:08:49  lr: 0.000000  loss: 17718.2871 (17787.7875)  L2_Loss: 17718.2871 (17787.7875)  time: 7.0659  data: 6.3803  max mem: 15480\n",
      "Epoch: [0]  [16/90]  eta: 0:09:12  lr: 0.000000  loss: 18078.1582 (17856.1383)  L2_Loss: 18078.1582 (17856.1383)  time: 7.4700  data: 6.7871  max mem: 15480\n",
      "Epoch: [0]  [17/90]  eta: 0:08:53  lr: 0.000000  loss: 18078.1582 (17985.4357)  L2_Loss: 18078.1582 (17985.4357)  time: 7.3079  data: 6.6271  max mem: 15480\n",
      "Epoch: [0]  [18/90]  eta: 0:09:10  lr: 0.000000  loss: 18215.1855 (18047.8430)  L2_Loss: 18215.1855 (18047.8430)  time: 7.6484  data: 6.9704  max mem: 15480\n",
      "Epoch: [0]  [19/90]  eta: 0:08:40  lr: 0.000000  loss: 18078.1582 (18007.5985)  L2_Loss: 18078.1582 (18007.5985)  time: 7.3291  data: 6.6532  max mem: 15480\n",
      "Epoch: [0]  [20/90]  eta: 0:09:05  lr: 0.000000  loss: 18078.1582 (17977.0739)  L2_Loss: 18078.1582 (17977.0739)  time: 7.6011  data: 6.9641  max mem: 15480\n",
      "Epoch: [0]  [21/90]  eta: 0:08:35  lr: 0.000000  loss: 18078.1582 (17999.2963)  L2_Loss: 18078.1582 (17999.2963)  time: 7.6012  data: 6.9641  max mem: 15480\n",
      "Epoch: [0]  [22/90]  eta: 0:09:00  lr: 0.000000  loss: 17718.2871 (17984.5786)  L2_Loss: 17718.2871 (17984.5786)  time: 8.0942  data: 7.4564  max mem: 15480\n",
      "Epoch: [0]  [23/90]  eta: 0:08:32  lr: 0.000000  loss: 18062.3398 (17987.8187)  L2_Loss: 18062.3398 (17987.8187)  time: 8.0406  data: 7.4018  max mem: 15480\n",
      "Epoch: [0]  [24/90]  eta: 0:08:49  lr: 0.000000  loss: 17718.2871 (17964.3818)  L2_Loss: 17718.2871 (17964.3818)  time: 8.4814  data: 7.8424  max mem: 15480\n",
      "Epoch: [0]  [25/90]  eta: 0:08:23  lr: 0.000000  loss: 17660.7910 (17944.5823)  L2_Loss: 17660.7910 (17944.5823)  time: 8.3766  data: 7.7376  max mem: 15480\n",
      "Epoch: [0]  [26/90]  eta: 0:08:34  lr: 0.000000  loss: 17718.2871 (17967.3591)  L2_Loss: 17718.2871 (17967.3591)  time: 8.7893  data: 8.1503  max mem: 15480\n",
      "Epoch: [0]  [27/90]  eta: 0:08:09  lr: 0.000000  loss: 17718.2871 (18004.7011)  L2_Loss: 17718.2871 (18004.7011)  time: 8.6488  data: 8.0100  max mem: 15480\n",
      "Epoch: [0]  [28/90]  eta: 0:08:18  lr: 0.000000  loss: 17660.7910 (17991.9731)  L2_Loss: 17660.7910 (17991.9731)  time: 8.9894  data: 8.3502  max mem: 15480\n",
      "Epoch: [0]  [29/90]  eta: 0:07:55  lr: 0.000000  loss: 17660.7910 (18045.2339)  L2_Loss: 17660.7910 (18045.2339)  time: 8.7665  data: 8.1277  max mem: 15480\n",
      "Epoch: [0]  [30/90]  eta: 0:08:04  lr: 0.000000  loss: 17718.2871 (18047.1670)  L2_Loss: 17718.2871 (18047.1670)  time: 8.9668  data: 8.3278  max mem: 15480\n",
      "Epoch: [0]  [31/90]  eta: 0:07:42  lr: 0.000000  loss: 17660.7910 (18024.9180)  L2_Loss: 17660.7910 (18024.9180)  time: 8.7006  data: 8.0613  max mem: 15480\n",
      "Epoch: [0]  [32/90]  eta: 0:07:41  lr: 0.000000  loss: 17635.5898 (17995.4950)  L2_Loss: 17635.5898 (17995.4950)  time: 8.6406  data: 8.0021  max mem: 15480\n",
      "Epoch: [0]  [33/90]  eta: 0:07:22  lr: 0.000000  loss: 17660.7910 (18031.1623)  L2_Loss: 17660.7910 (18031.1623)  time: 8.4837  data: 7.8452  max mem: 15480\n",
      "Epoch: [0]  [34/90]  eta: 0:07:16  lr: 0.000000  loss: 17660.7910 (17997.9708)  L2_Loss: 17660.7910 (17997.9708)  time: 8.1521  data: 7.5138  max mem: 15480\n",
      "Epoch: [0]  [35/90]  eta: 0:06:59  lr: 0.000000  loss: 18062.3398 (18023.9077)  L2_Loss: 18062.3398 (18023.9077)  time: 8.0817  data: 7.4433  max mem: 15480\n",
      "Epoch: [0]  [36/90]  eta: 0:06:53  lr: 0.000000  loss: 17660.7910 (18005.1623)  L2_Loss: 17660.7910 (18005.1623)  time: 7.8031  data: 7.1650  max mem: 15480\n",
      "Epoch: [0]  [37/90]  eta: 0:06:39  lr: 0.000000  loss: 17660.7910 (18027.8616)  L2_Loss: 17660.7910 (18027.8616)  time: 7.7299  data: 7.0923  max mem: 15480\n",
      "Epoch: [0]  [38/90]  eta: 0:06:30  lr: 0.000000  loss: 17635.5898 (18016.3616)  L2_Loss: 17635.5898 (18016.3616)  time: 7.3955  data: 6.7571  max mem: 15480\n",
      "Epoch: [0]  [39/90]  eta: 0:06:20  lr: 0.000000  loss: 17660.7910 (18022.5409)  L2_Loss: 17660.7910 (18022.5409)  time: 7.5756  data: 6.9371  max mem: 15480\n",
      "Epoch: [0]  [40/90]  eta: 0:06:10  lr: 0.000000  loss: 18062.3398 (18045.5167)  L2_Loss: 18062.3398 (18045.5167)  time: 7.0103  data: 6.3724  max mem: 15480\n",
      "Epoch: [0]  [41/90]  eta: 0:06:01  lr: 0.000000  loss: 18062.3398 (18050.9864)  L2_Loss: 18062.3398 (18050.9864)  time: 7.2733  data: 6.6352  max mem: 15480\n",
      "Epoch: [0]  [42/90]  eta: 0:05:51  lr: 0.000000  loss: 18062.3398 (18040.4643)  L2_Loss: 18062.3398 (18040.4643)  time: 6.6080  data: 5.9704  max mem: 15480\n",
      "Epoch: [0]  [43/90]  eta: 0:05:42  lr: 0.000000  loss: 18105.1602 (18042.2867)  L2_Loss: 18105.1602 (18042.2867)  time: 6.8609  data: 6.2244  max mem: 15480\n",
      "Epoch: [0]  [44/90]  eta: 0:05:32  lr: 0.000000  loss: 18120.6504 (18060.7843)  L2_Loss: 18120.6504 (18060.7843)  time: 6.2481  data: 5.6118  max mem: 15480\n",
      "Epoch: [0]  [45/90]  eta: 0:05:24  lr: 0.000000  loss: 18120.6504 (18059.0076)  L2_Loss: 18120.6504 (18059.0076)  time: 6.5220  data: 5.8853  max mem: 15480\n",
      "Epoch: [0]  [46/90]  eta: 0:05:14  lr: 0.000000  loss: 18105.1602 (18050.5206)  L2_Loss: 18105.1602 (18050.5206)  time: 5.9614  data: 5.3247  max mem: 15480\n",
      "Epoch: [0]  [47/90]  eta: 0:05:07  lr: 0.000000  loss: 18105.1602 (18059.6162)  L2_Loss: 18105.1602 (18059.6162)  time: 6.2688  data: 5.6321  max mem: 15480\n",
      "Epoch: [0]  [48/90]  eta: 0:04:57  lr: 0.000000  loss: 18120.6504 (18066.5959)  L2_Loss: 18120.6504 (18066.5959)  time: 5.7041  data: 5.0671  max mem: 15480\n",
      "Epoch: [0]  [49/90]  eta: 0:04:50  lr: 0.000000  loss: 18120.6504 (18069.0082)  L2_Loss: 18120.6504 (18069.0082)  time: 6.0111  data: 5.3739  max mem: 15480\n",
      "Epoch: [0]  [50/90]  eta: 0:04:41  lr: 0.000000  loss: 18120.6504 (18056.6737)  L2_Loss: 18120.6504 (18056.6737)  time: 5.4329  data: 4.7957  max mem: 15480\n",
      "Epoch: [0]  [51/90]  eta: 0:04:34  lr: 0.000000  loss: 18120.6504 (18040.0414)  L2_Loss: 18120.6504 (18040.0414)  time: 5.7449  data: 5.1078  max mem: 15480\n",
      "Epoch: [0]  [52/90]  eta: 0:04:24  lr: 0.000000  loss: 18187.2109 (18054.7175)  L2_Loss: 18187.2109 (18054.7175)  time: 5.3022  data: 4.6647  max mem: 15480\n",
      "Epoch: [0]  [53/90]  eta: 0:04:17  lr: 0.000000  loss: 18120.6504 (18036.0660)  L2_Loss: 18120.6504 (18036.0660)  time: 5.6038  data: 4.9660  max mem: 15480\n",
      "Epoch: [0]  [54/90]  eta: 0:04:08  lr: 0.000000  loss: 18187.2109 (18049.3504)  L2_Loss: 18187.2109 (18049.3504)  time: 5.3280  data: 4.6903  max mem: 15480\n",
      "Epoch: [0]  [55/90]  eta: 0:04:01  lr: 0.000000  loss: 18120.6504 (18039.4542)  L2_Loss: 18120.6504 (18039.4542)  time: 5.5636  data: 4.9263  max mem: 15480\n",
      "Epoch: [0]  [56/90]  eta: 0:03:52  lr: 0.000000  loss: 18120.6504 (18035.4600)  L2_Loss: 18120.6504 (18035.4600)  time: 5.3772  data: 4.7400  max mem: 15480\n",
      "Epoch: [0]  [57/90]  eta: 0:03:46  lr: 0.000000  loss: 17979.0566 (18024.4151)  L2_Loss: 17979.0566 (18024.4151)  time: 5.5688  data: 4.9316  max mem: 15480\n",
      "Epoch: [0]  [58/90]  eta: 0:03:37  lr: 0.000000  loss: 17979.0566 (18011.2521)  L2_Loss: 17979.0566 (18011.2521)  time: 5.4246  data: 4.7879  max mem: 15480\n",
      "Epoch: [0]  [59/90]  eta: 0:03:30  lr: 0.000000  loss: 17811.7832 (17981.5084)  L2_Loss: 17811.7832 (17981.5084)  time: 5.4606  data: 4.8241  max mem: 15480\n",
      "Epoch: [0]  [60/90]  eta: 0:03:22  lr: 0.000000  loss: 17660.1152 (17962.9338)  L2_Loss: 17660.1152 (17962.9338)  time: 5.4033  data: 4.7668  max mem: 15480\n",
      "Epoch: [0]  [61/90]  eta: 0:03:15  lr: 0.000000  loss: 17598.5332 (17954.5215)  L2_Loss: 17598.5332 (17954.5215)  time: 5.4043  data: 4.7679  max mem: 15480\n",
      "Epoch: [0]  [62/90]  eta: 0:03:07  lr: 0.000000  loss: 17660.1152 (17950.3372)  L2_Loss: 17660.1152 (17950.3372)  time: 5.3524  data: 4.7163  max mem: 15480\n",
      "Epoch: [0]  [63/90]  eta: 0:03:01  lr: 0.000000  loss: 17660.1152 (17978.0134)  L2_Loss: 17660.1152 (17978.0134)  time: 5.4367  data: 4.8004  max mem: 15480\n",
      "Epoch: [0]  [64/90]  eta: 0:02:52  lr: 0.000000  loss: 17660.1152 (17986.4166)  L2_Loss: 17660.1152 (17986.4166)  time: 5.3171  data: 4.6810  max mem: 15480\n",
      "Epoch: [0]  [65/90]  eta: 0:02:46  lr: 0.000000  loss: 17495.1660 (17957.4376)  L2_Loss: 17495.1660 (17957.4376)  time: 5.4410  data: 4.8059  max mem: 15480\n",
      "Epoch: [0]  [66/90]  eta: 0:02:38  lr: 0.000000  loss: 17495.1660 (17953.6349)  L2_Loss: 17495.1660 (17953.6349)  time: 5.3327  data: 4.6977  max mem: 15480\n",
      "Epoch: [0]  [67/90]  eta: 0:02:32  lr: 0.000000  loss: 17441.3691 (17937.3576)  L2_Loss: 17441.3691 (17937.3576)  time: 5.4317  data: 4.7966  max mem: 15480\n",
      "Epoch: [0]  [68/90]  eta: 0:02:24  lr: 0.000000  loss: 17439.9492 (17927.3617)  L2_Loss: 17439.9492 (17927.3617)  time: 5.2907  data: 4.6558  max mem: 15480\n",
      "Epoch: [0]  [69/90]  eta: 0:02:18  lr: 0.000000  loss: 17394.8594 (17911.7295)  L2_Loss: 17394.8594 (17911.7295)  time: 5.4516  data: 4.8164  max mem: 15480\n",
      "Epoch: [0]  [70/90]  eta: 0:02:10  lr: 0.000000  loss: 17394.8594 (17912.5041)  L2_Loss: 17394.8594 (17912.5041)  time: 5.2536  data: 4.6185  max mem: 15480\n",
      "Epoch: [0]  [71/90]  eta: 0:02:05  lr: 0.000000  loss: 17394.8594 (17905.2618)  L2_Loss: 17394.8594 (17905.2618)  time: 5.4041  data: 4.7693  max mem: 15480\n",
      "Epoch: [0]  [72/90]  eta: 0:01:56  lr: 0.000000  loss: 17391.0566 (17897.3407)  L2_Loss: 17391.0566 (17897.3407)  time: 5.2754  data: 4.6405  max mem: 15480\n",
      "Epoch: [0]  [73/90]  eta: 0:01:51  lr: 0.000000  loss: 17394.8594 (17900.7094)  L2_Loss: 17394.8594 (17900.7094)  time: 5.4196  data: 4.7845  max mem: 15480\n",
      "Epoch: [0]  [74/90]  eta: 0:01:43  lr: 0.000000  loss: 17394.8594 (17916.0505)  L2_Loss: 17394.8594 (17916.0505)  time: 5.2761  data: 4.6409  max mem: 15480\n",
      "Epoch: [0]  [75/90]  eta: 0:01:37  lr: 0.000000  loss: 17394.8594 (17911.2299)  L2_Loss: 17394.8594 (17911.2299)  time: 5.4595  data: 4.8242  max mem: 15480\n",
      "Epoch: [0]  [76/90]  eta: 0:01:30  lr: 0.000000  loss: 17394.8594 (17908.6163)  L2_Loss: 17394.8594 (17908.6163)  time: 5.2597  data: 4.6241  max mem: 15480\n",
      "Epoch: [0]  [77/90]  eta: 0:01:24  lr: 0.000000  loss: 17441.3691 (17916.1896)  L2_Loss: 17441.3691 (17916.1896)  time: 5.4279  data: 4.7926  max mem: 15480\n",
      "Epoch: [0]  [78/90]  eta: 0:01:16  lr: 0.000000  loss: 17549.6895 (17926.6008)  L2_Loss: 17549.6895 (17926.6008)  time: 5.2496  data: 4.6139  max mem: 15480\n",
      "Epoch: [0]  [79/90]  eta: 0:01:11  lr: 0.000000  loss: 17690.9141 (17926.4547)  L2_Loss: 17690.9141 (17926.4547)  time: 5.5090  data: 4.8731  max mem: 15480\n",
      "Epoch: [0]  [80/90]  eta: 0:01:03  lr: 0.000000  loss: 17702.6602 (17925.7131)  L2_Loss: 17702.6602 (17925.7131)  time: 5.3079  data: 4.6719  max mem: 15480\n",
      "Epoch: [0]  [81/90]  eta: 0:00:57  lr: 0.000000  loss: 17709.9805 (17924.8218)  L2_Loss: 17709.9805 (17924.8218)  time: 5.4869  data: 4.8511  max mem: 15480\n",
      "Epoch: [0]  [82/90]  eta: 0:00:50  lr: 0.000000  loss: 17852.6270 (17945.0528)  L2_Loss: 17852.6270 (17945.0528)  time: 5.3131  data: 4.6770  max mem: 15480\n",
      "Epoch: [0]  [83/90]  eta: 0:00:44  lr: 0.000000  loss: 17852.6270 (17945.4665)  L2_Loss: 17852.6270 (17945.4665)  time: 5.4925  data: 4.8564  max mem: 15480\n",
      "Epoch: [0]  [84/90]  eta: 0:00:38  lr: 0.000000  loss: 17852.6270 (17947.2773)  L2_Loss: 17852.6270 (17947.2773)  time: 5.3910  data: 4.7546  max mem: 15480\n",
      "Epoch: [0]  [85/90]  eta: 0:00:32  lr: 0.000000  loss: 17866.3828 (17947.6885)  L2_Loss: 17866.3828 (17947.6885)  time: 5.4909  data: 4.8542  max mem: 15480\n",
      "Epoch: [0]  [86/90]  eta: 0:00:25  lr: 0.000000  loss: 17866.3828 (17941.6583)  L2_Loss: 17866.3828 (17941.6583)  time: 5.4143  data: 4.7777  max mem: 15480\n",
      "Epoch: [0]  [87/90]  eta: 0:00:19  lr: 0.000000  loss: 17914.9180 (17950.6667)  L2_Loss: 17914.9180 (17950.6667)  time: 5.5017  data: 4.8653  max mem: 15480\n",
      "Epoch: [0]  [88/90]  eta: 0:00:12  lr: 0.000000  loss: 17914.9180 (17928.2430)  L2_Loss: 17914.9180 (17928.2430)  time: 5.4532  data: 4.8170  max mem: 15480\n",
      "Epoch: [0]  [89/90]  eta: 0:00:06  lr: 0.000000  loss: 17914.9180 (17896.6649)  L2_Loss: 17914.9180 (17896.6649)  time: 5.4554  data: 4.8194  max mem: 15480\n",
      "Epoch: [0] Total time: 0:09:32 (6.3602 s / it)\n",
      "Averaged train_stats:  {'lr': 0.0, 'loss': 17896.6648546, 'L2_Loss': 17896.6648546}\n",
      "tensor([[0.0504],\n",
      "        [0.0560],\n",
      "        [0.0541],\n",
      "        [0.0634],\n",
      "        [0.0603],\n",
      "        [0.0579],\n",
      "        [0.0629],\n",
      "        [0.0617],\n",
      "        [0.0617],\n",
      "        [0.0589],\n",
      "        [0.0620],\n",
      "        [0.0741],\n",
      "        [0.0599],\n",
      "        [0.0547],\n",
      "        [0.0686],\n",
      "        [0.0438],\n",
      "        [0.0725],\n",
      "        [0.0644],\n",
      "        [0.0639],\n",
      "        [0.0559],\n",
      "        [0.0601],\n",
      "        [0.0689],\n",
      "        [0.0477],\n",
      "        [0.0574],\n",
      "        [0.0609],\n",
      "        [0.0527],\n",
      "        [0.0583],\n",
      "        [0.0634],\n",
      "        [0.0441],\n",
      "        [0.0600],\n",
      "        [0.0624],\n",
      "        [0.0559],\n",
      "        [0.0674],\n",
      "        [0.0707],\n",
      "        [0.0617],\n",
      "        [0.0547],\n",
      "        [0.0472],\n",
      "        [0.0583],\n",
      "        [0.0582],\n",
      "        [0.0719],\n",
      "        [0.0647],\n",
      "        [0.0697],\n",
      "        [0.0699],\n",
      "        [0.0661],\n",
      "        [0.0638],\n",
      "        [0.0616],\n",
      "        [0.0608],\n",
      "        [0.0629],\n",
      "        [0.0628],\n",
      "        [0.0550],\n",
      "        [0.0569],\n",
      "        [0.0608],\n",
      "        [0.0625],\n",
      "        [0.0664],\n",
      "        [0.0678],\n",
      "        [0.0587],\n",
      "        [0.0557],\n",
      "        [0.0591],\n",
      "        [0.0572],\n",
      "        [0.0446],\n",
      "        [0.0651],\n",
      "        [0.0565],\n",
      "        [0.0578],\n",
      "        [0.0653],\n",
      "        [0.0565],\n",
      "        [0.0656],\n",
      "        [0.0586],\n",
      "        [0.0545],\n",
      "        [0.0629],\n",
      "        [0.0715],\n",
      "        [0.0677],\n",
      "        [0.0777],\n",
      "        [0.0584],\n",
      "        [0.0618],\n",
      "        [0.0541],\n",
      "        [0.0541],\n",
      "        [0.0479],\n",
      "        [0.0683],\n",
      "        [0.0703],\n",
      "        [0.0648],\n",
      "        [0.0589],\n",
      "        [0.0753],\n",
      "        [0.0682],\n",
      "        [0.0545],\n",
      "        [0.0598],\n",
      "        [0.0626],\n",
      "        [0.0559],\n",
      "        [0.0702],\n",
      "        [0.0631],\n",
      "        [0.0651],\n",
      "        [0.0632],\n",
      "        [0.0610],\n",
      "        [0.0599],\n",
      "        [0.0691],\n",
      "        [0.0593],\n",
      "        [0.0592],\n",
      "        [0.0652],\n",
      "        [0.0669],\n",
      "        [0.0657],\n",
      "        [0.0625],\n",
      "        [0.0678],\n",
      "        [0.0645],\n",
      "        [0.0545],\n",
      "        [0.0637],\n",
      "        [0.0613],\n",
      "        [0.0655],\n",
      "        [0.0716],\n",
      "        [0.0439],\n",
      "        [0.0545],\n",
      "        [0.0798],\n",
      "        [0.0626],\n",
      "        [0.0529],\n",
      "        [0.0621],\n",
      "        [0.0647],\n",
      "        [0.0600],\n",
      "        [0.0625],\n",
      "        [0.0732],\n",
      "        [0.0695],\n",
      "        [0.0422],\n",
      "        [0.0668],\n",
      "        [0.0652],\n",
      "        [0.0612],\n",
      "        [0.0691],\n",
      "        [0.0604],\n",
      "        [0.0703],\n",
      "        [0.0709],\n",
      "        [0.0601],\n",
      "        [0.0616],\n",
      "        [0.0577],\n",
      "        [0.0750],\n",
      "        [0.0757],\n",
      "        [0.0626],\n",
      "        [0.0601],\n",
      "        [0.0628],\n",
      "        [0.0648],\n",
      "        [0.0518],\n",
      "        [0.0663],\n",
      "        [0.0582],\n",
      "        [0.0468],\n",
      "        [0.0531]], device='cuda:0')\n",
      "Valid:  [ 0/11]  eta: 0:03:21  loss: 18253.8789 (18253.8789)  metric: 125.6456 (125.6456)  L2_Loss: 18253.8789 (18253.8789)  time: 18.3043  data: 18.1229  max mem: 15480\n",
      "tensor([[0.0553],\n",
      "        [0.0666],\n",
      "        [0.0570],\n",
      "        [0.0631],\n",
      "        [0.0642],\n",
      "        [0.0621],\n",
      "        [0.0668],\n",
      "        [0.0730],\n",
      "        [0.0564],\n",
      "        [0.0704],\n",
      "        [0.0547],\n",
      "        [0.0685],\n",
      "        [0.0583],\n",
      "        [0.0579],\n",
      "        [0.0716],\n",
      "        [0.0498],\n",
      "        [0.0536],\n",
      "        [0.0656],\n",
      "        [0.0617],\n",
      "        [0.0637],\n",
      "        [0.0695],\n",
      "        [0.0608],\n",
      "        [0.0589],\n",
      "        [0.0673],\n",
      "        [0.0412],\n",
      "        [0.0580],\n",
      "        [0.0608],\n",
      "        [0.0611],\n",
      "        [0.0721],\n",
      "        [0.0651],\n",
      "        [0.0555],\n",
      "        [0.0657],\n",
      "        [0.0715],\n",
      "        [0.0507],\n",
      "        [0.0672],\n",
      "        [0.0498],\n",
      "        [0.0635],\n",
      "        [0.0696],\n",
      "        [0.0593],\n",
      "        [0.0599],\n",
      "        [0.0664],\n",
      "        [0.0643],\n",
      "        [0.0619],\n",
      "        [0.0695],\n",
      "        [0.0690],\n",
      "        [0.0536],\n",
      "        [0.0595],\n",
      "        [0.0610],\n",
      "        [0.0573],\n",
      "        [0.0651],\n",
      "        [0.0686],\n",
      "        [0.0564],\n",
      "        [0.0679],\n",
      "        [0.0649],\n",
      "        [0.0580],\n",
      "        [0.0648],\n",
      "        [0.0568],\n",
      "        [0.0616],\n",
      "        [0.0508],\n",
      "        [0.0643],\n",
      "        [0.0649],\n",
      "        [0.0617],\n",
      "        [0.0469],\n",
      "        [0.0724],\n",
      "        [0.0898],\n",
      "        [0.0503],\n",
      "        [0.0646],\n",
      "        [0.0553],\n",
      "        [0.0589],\n",
      "        [0.0475],\n",
      "        [0.0595],\n",
      "        [0.0603],\n",
      "        [0.0600],\n",
      "        [0.0571],\n",
      "        [0.0400],\n",
      "        [0.0630],\n",
      "        [0.0734],\n",
      "        [0.0601],\n",
      "        [0.0598],\n",
      "        [0.0521],\n",
      "        [0.0523],\n",
      "        [0.0493],\n",
      "        [0.0592],\n",
      "        [0.0401],\n",
      "        [0.0610],\n",
      "        [0.0670],\n",
      "        [0.0656],\n",
      "        [0.0710],\n",
      "        [0.0811],\n",
      "        [0.0472],\n",
      "        [0.0629],\n",
      "        [0.0615],\n",
      "        [0.0543],\n",
      "        [0.0637],\n",
      "        [0.0539],\n",
      "        [0.0661],\n",
      "        [0.0521],\n",
      "        [0.0513],\n",
      "        [0.0716],\n",
      "        [0.0538],\n",
      "        [0.0652],\n",
      "        [0.0674],\n",
      "        [0.0521],\n",
      "        [0.0602],\n",
      "        [0.0417],\n",
      "        [0.0670],\n",
      "        [0.0596],\n",
      "        [0.0560],\n",
      "        [0.0704],\n",
      "        [0.0650],\n",
      "        [0.0531],\n",
      "        [0.0530],\n",
      "        [0.0519],\n",
      "        [0.0586],\n",
      "        [0.0617],\n",
      "        [0.0510],\n",
      "        [0.0612],\n",
      "        [0.0613],\n",
      "        [0.0700],\n",
      "        [0.0547],\n",
      "        [0.0611],\n",
      "        [0.0751],\n",
      "        [0.0626],\n",
      "        [0.0496],\n",
      "        [0.0634],\n",
      "        [0.0634],\n",
      "        [0.0661],\n",
      "        [0.0684],\n",
      "        [0.0667],\n",
      "        [0.0748],\n",
      "        [0.0641],\n",
      "        [0.0569],\n",
      "        [0.0677],\n",
      "        [0.0355],\n",
      "        [0.0618],\n",
      "        [0.0767],\n",
      "        [0.0715],\n",
      "        [0.0600],\n",
      "        [0.0702],\n",
      "        [0.0615]], device='cuda:0')\n",
      "Valid:  [ 1/11]  eta: 0:01:32  loss: 18253.8789 (19214.2021)  metric: 125.6456 (129.9994)  L2_Loss: 18253.8789 (19214.2021)  time: 9.2333  data: 9.0615  max mem: 15480\n",
      "tensor([[0.0696],\n",
      "        [0.0650],\n",
      "        [0.0722],\n",
      "        [0.0597],\n",
      "        [0.0687],\n",
      "        [0.0542],\n",
      "        [0.0686],\n",
      "        [0.0693],\n",
      "        [0.0685],\n",
      "        [0.0633],\n",
      "        [0.0638],\n",
      "        [0.0610],\n",
      "        [0.0635],\n",
      "        [0.0610],\n",
      "        [0.0640],\n",
      "        [0.0515],\n",
      "        [0.0549],\n",
      "        [0.0617],\n",
      "        [0.0584],\n",
      "        [0.0611],\n",
      "        [0.0478],\n",
      "        [0.0594],\n",
      "        [0.0555],\n",
      "        [0.0629],\n",
      "        [0.0656],\n",
      "        [0.0589],\n",
      "        [0.0632],\n",
      "        [0.0559],\n",
      "        [0.0589],\n",
      "        [0.0618],\n",
      "        [0.0472],\n",
      "        [0.0642],\n",
      "        [0.0585],\n",
      "        [0.0494],\n",
      "        [0.0658],\n",
      "        [0.0526],\n",
      "        [0.0484],\n",
      "        [0.0612],\n",
      "        [0.0567],\n",
      "        [0.0606],\n",
      "        [0.0550],\n",
      "        [0.0547],\n",
      "        [0.0552],\n",
      "        [0.0575],\n",
      "        [0.0486],\n",
      "        [0.0578],\n",
      "        [0.0611],\n",
      "        [0.0503],\n",
      "        [0.0646],\n",
      "        [0.0557],\n",
      "        [0.0588],\n",
      "        [0.0532],\n",
      "        [0.0669],\n",
      "        [0.0590],\n",
      "        [0.0798],\n",
      "        [0.0636],\n",
      "        [0.0801],\n",
      "        [0.0625],\n",
      "        [0.0454],\n",
      "        [0.0547],\n",
      "        [0.0556],\n",
      "        [0.0538],\n",
      "        [0.0539],\n",
      "        [0.0598],\n",
      "        [0.0543],\n",
      "        [0.0655],\n",
      "        [0.0593],\n",
      "        [0.0578],\n",
      "        [0.0583],\n",
      "        [0.0621],\n",
      "        [0.0569],\n",
      "        [0.0545],\n",
      "        [0.0656],\n",
      "        [0.0581],\n",
      "        [0.0534],\n",
      "        [0.0493],\n",
      "        [0.0610],\n",
      "        [0.0597],\n",
      "        [0.0604],\n",
      "        [0.0635],\n",
      "        [0.0516],\n",
      "        [0.0560],\n",
      "        [0.0542],\n",
      "        [0.0461],\n",
      "        [0.0596],\n",
      "        [0.0538],\n",
      "        [0.0542],\n",
      "        [0.0777],\n",
      "        [0.0577],\n",
      "        [0.0624],\n",
      "        [0.0688],\n",
      "        [0.0593],\n",
      "        [0.0620],\n",
      "        [0.0680],\n",
      "        [0.0772],\n",
      "        [0.0523],\n",
      "        [0.0567],\n",
      "        [0.0436],\n",
      "        [0.0533],\n",
      "        [0.0566],\n",
      "        [0.0614],\n",
      "        [0.0549],\n",
      "        [0.0501],\n",
      "        [0.0588],\n",
      "        [0.0610],\n",
      "        [0.0544],\n",
      "        [0.0714],\n",
      "        [0.0414],\n",
      "        [0.0779],\n",
      "        [0.0689],\n",
      "        [0.0764],\n",
      "        [0.0550],\n",
      "        [0.0603],\n",
      "        [0.0524],\n",
      "        [0.0566],\n",
      "        [0.0643],\n",
      "        [0.0601],\n",
      "        [0.0581],\n",
      "        [0.0639],\n",
      "        [0.0677],\n",
      "        [0.0656],\n",
      "        [0.0454],\n",
      "        [0.0556],\n",
      "        [0.0617],\n",
      "        [0.0650],\n",
      "        [0.0618],\n",
      "        [0.0581],\n",
      "        [0.0682],\n",
      "        [0.0650],\n",
      "        [0.0605],\n",
      "        [0.0507],\n",
      "        [0.0588],\n",
      "        [0.0573],\n",
      "        [0.0582],\n",
      "        [0.0510],\n",
      "        [0.0603],\n",
      "        [0.0587],\n",
      "        [0.0568],\n",
      "        [0.0505],\n",
      "        [0.0690]], device='cuda:0')\n",
      "Valid:  [ 2/11]  eta: 0:01:27  loss: 18253.8789 (18390.9440)  metric: 125.6456 (127.4679)  L2_Loss: 18253.8789 (18390.9440)  time: 9.7681  data: 9.5983  max mem: 15480\n",
      "tensor([[0.0536],\n",
      "        [0.0716],\n",
      "        [0.0715],\n",
      "        [0.0787],\n",
      "        [0.0635],\n",
      "        [0.0721],\n",
      "        [0.0655],\n",
      "        [0.0525],\n",
      "        [0.0507],\n",
      "        [0.0608],\n",
      "        [0.0544],\n",
      "        [0.0557],\n",
      "        [0.0534],\n",
      "        [0.0542],\n",
      "        [0.0531],\n",
      "        [0.0716],\n",
      "        [0.0625],\n",
      "        [0.0565],\n",
      "        [0.0529],\n",
      "        [0.0501],\n",
      "        [0.0618],\n",
      "        [0.0474],\n",
      "        [0.0586],\n",
      "        [0.0656],\n",
      "        [0.0603],\n",
      "        [0.0459],\n",
      "        [0.0580],\n",
      "        [0.0468],\n",
      "        [0.0658],\n",
      "        [0.0539],\n",
      "        [0.0505],\n",
      "        [0.0543],\n",
      "        [0.0480],\n",
      "        [0.0518],\n",
      "        [0.0511],\n",
      "        [0.0545],\n",
      "        [0.0545],\n",
      "        [0.0578],\n",
      "        [0.0539],\n",
      "        [0.0525],\n",
      "        [0.0440],\n",
      "        [0.0591],\n",
      "        [0.0542],\n",
      "        [0.0543],\n",
      "        [0.0579],\n",
      "        [0.0459],\n",
      "        [0.0594],\n",
      "        [0.0627],\n",
      "        [0.0658],\n",
      "        [0.0728],\n",
      "        [0.0530],\n",
      "        [0.0622],\n",
      "        [0.0507],\n",
      "        [0.0558],\n",
      "        [0.0626],\n",
      "        [0.0509],\n",
      "        [0.0573],\n",
      "        [0.0469],\n",
      "        [0.0550],\n",
      "        [0.0601],\n",
      "        [0.0571],\n",
      "        [0.0586],\n",
      "        [0.0559],\n",
      "        [0.0694],\n",
      "        [0.0695],\n",
      "        [0.0531],\n",
      "        [0.0890],\n",
      "        [0.0598],\n",
      "        [0.0553],\n",
      "        [0.0584],\n",
      "        [0.0583],\n",
      "        [0.0483],\n",
      "        [0.0688],\n",
      "        [0.0629],\n",
      "        [0.0485],\n",
      "        [0.0489],\n",
      "        [0.0477],\n",
      "        [0.0481],\n",
      "        [0.0755],\n",
      "        [0.0605],\n",
      "        [0.0565],\n",
      "        [0.0538],\n",
      "        [0.0500],\n",
      "        [0.0643],\n",
      "        [0.0534],\n",
      "        [0.0519],\n",
      "        [0.0601],\n",
      "        [0.0567],\n",
      "        [0.0514],\n",
      "        [0.0669],\n",
      "        [0.0639],\n",
      "        [0.0657],\n",
      "        [0.0553],\n",
      "        [0.0522],\n",
      "        [0.0544],\n",
      "        [0.0713],\n",
      "        [0.0553],\n",
      "        [0.0735],\n",
      "        [0.0548],\n",
      "        [0.0595],\n",
      "        [0.0559],\n",
      "        [0.0529],\n",
      "        [0.0498],\n",
      "        [0.0567],\n",
      "        [0.0416],\n",
      "        [0.0558],\n",
      "        [0.0645],\n",
      "        [0.0657],\n",
      "        [0.0533],\n",
      "        [0.0621],\n",
      "        [0.0614],\n",
      "        [0.0598],\n",
      "        [0.0631],\n",
      "        [0.0530],\n",
      "        [0.0525],\n",
      "        [0.0571],\n",
      "        [0.0779],\n",
      "        [0.0607],\n",
      "        [0.0466],\n",
      "        [0.0667],\n",
      "        [0.0694],\n",
      "        [0.0626],\n",
      "        [0.0556],\n",
      "        [0.0737],\n",
      "        [0.0481],\n",
      "        [0.0489],\n",
      "        [0.0486],\n",
      "        [0.0570],\n",
      "        [0.0534],\n",
      "        [0.0440],\n",
      "        [0.0486],\n",
      "        [0.0482],\n",
      "        [0.0583],\n",
      "        [0.0641],\n",
      "        [0.0647],\n",
      "        [0.0526],\n",
      "        [0.0597],\n",
      "        [0.0517],\n",
      "        [0.0529],\n",
      "        [0.0615]], device='cuda:0')\n",
      "Valid:  [ 3/11]  eta: 0:00:58  loss: 18253.8789 (18371.3115)  metric: 125.6456 (127.8079)  L2_Loss: 18253.8789 (18371.3115)  time: 7.3676  data: 7.1988  max mem: 15480\n",
      "tensor([[0.0595],\n",
      "        [0.0613],\n",
      "        [0.0644],\n",
      "        [0.0635],\n",
      "        [0.0526],\n",
      "        [0.0564],\n",
      "        [0.0576],\n",
      "        [0.0618],\n",
      "        [0.0607],\n",
      "        [0.0571],\n",
      "        [0.0531],\n",
      "        [0.0620],\n",
      "        [0.0493],\n",
      "        [0.0493],\n",
      "        [0.0495],\n",
      "        [0.0581],\n",
      "        [0.0474],\n",
      "        [0.0642],\n",
      "        [0.0764],\n",
      "        [0.0638],\n",
      "        [0.0660],\n",
      "        [0.0543],\n",
      "        [0.0551],\n",
      "        [0.0663],\n",
      "        [0.0511],\n",
      "        [0.0458],\n",
      "        [0.0595],\n",
      "        [0.0480],\n",
      "        [0.0506],\n",
      "        [0.0495],\n",
      "        [0.0648],\n",
      "        [0.0626],\n",
      "        [0.0572],\n",
      "        [0.0712],\n",
      "        [0.0525],\n",
      "        [0.0511],\n",
      "        [0.0639],\n",
      "        [0.0463],\n",
      "        [0.0628],\n",
      "        [0.0538],\n",
      "        [0.0518],\n",
      "        [0.0484],\n",
      "        [0.0698],\n",
      "        [0.0562],\n",
      "        [0.0489],\n",
      "        [0.0529],\n",
      "        [0.0618],\n",
      "        [0.0577],\n",
      "        [0.0519],\n",
      "        [0.0600],\n",
      "        [0.0558],\n",
      "        [0.0525],\n",
      "        [0.0576],\n",
      "        [0.0496],\n",
      "        [0.0502],\n",
      "        [0.0616],\n",
      "        [0.0515],\n",
      "        [0.0558],\n",
      "        [0.0609],\n",
      "        [0.0600],\n",
      "        [0.0675],\n",
      "        [0.0445],\n",
      "        [0.0542],\n",
      "        [0.0735],\n",
      "        [0.0718],\n",
      "        [0.0493],\n",
      "        [0.0679],\n",
      "        [0.0521],\n",
      "        [0.0596],\n",
      "        [0.0621],\n",
      "        [0.0466],\n",
      "        [0.0505],\n",
      "        [0.0519],\n",
      "        [0.0689],\n",
      "        [0.0611],\n",
      "        [0.0598],\n",
      "        [0.0467],\n",
      "        [0.0661],\n",
      "        [0.0688],\n",
      "        [0.0668],\n",
      "        [0.0658],\n",
      "        [0.0559],\n",
      "        [0.0636],\n",
      "        [0.0570],\n",
      "        [0.0523],\n",
      "        [0.0534],\n",
      "        [0.0657],\n",
      "        [0.0621],\n",
      "        [0.0558],\n",
      "        [0.0557],\n",
      "        [0.0545],\n",
      "        [0.0545],\n",
      "        [0.0509],\n",
      "        [0.0592],\n",
      "        [0.0615],\n",
      "        [0.0478],\n",
      "        [0.0636],\n",
      "        [0.0616],\n",
      "        [0.0605],\n",
      "        [0.0489],\n",
      "        [0.0605],\n",
      "        [0.0562],\n",
      "        [0.0599],\n",
      "        [0.0564],\n",
      "        [0.0527],\n",
      "        [0.0486],\n",
      "        [0.0516],\n",
      "        [0.0780],\n",
      "        [0.0623],\n",
      "        [0.0504],\n",
      "        [0.0639],\n",
      "        [0.0422],\n",
      "        [0.0565],\n",
      "        [0.0537],\n",
      "        [0.0722],\n",
      "        [0.0616],\n",
      "        [0.0546],\n",
      "        [0.0601],\n",
      "        [0.0487],\n",
      "        [0.0738],\n",
      "        [0.0661],\n",
      "        [0.0454],\n",
      "        [0.0433],\n",
      "        [0.0662],\n",
      "        [0.0632],\n",
      "        [0.0638],\n",
      "        [0.0603],\n",
      "        [0.0574],\n",
      "        [0.0557],\n",
      "        [0.0498],\n",
      "        [0.0443],\n",
      "        [0.0668],\n",
      "        [0.0559],\n",
      "        [0.0544],\n",
      "        [0.0527],\n",
      "        [0.0678],\n",
      "        [0.0637],\n",
      "        [0.0510],\n",
      "        [0.0478],\n",
      "        [0.0594]], device='cuda:0')\n",
      "Valid:  [ 4/11]  eta: 0:01:00  loss: 18253.8789 (18157.8832)  metric: 125.6456 (127.3048)  L2_Loss: 18253.8789 (18157.8832)  time: 8.6671  data: 8.4993  max mem: 15480\n",
      "tensor([[0.0662],\n",
      "        [0.0632],\n",
      "        [0.0561],\n",
      "        [0.0586],\n",
      "        [0.0550],\n",
      "        [0.0553],\n",
      "        [0.0627],\n",
      "        [0.0560],\n",
      "        [0.0553],\n",
      "        [0.0630],\n",
      "        [0.0606],\n",
      "        [0.0628],\n",
      "        [0.0683],\n",
      "        [0.0553],\n",
      "        [0.0647],\n",
      "        [0.0427],\n",
      "        [0.0563],\n",
      "        [0.0530],\n",
      "        [0.0562],\n",
      "        [0.0657],\n",
      "        [0.0548],\n",
      "        [0.0512],\n",
      "        [0.0607],\n",
      "        [0.0530],\n",
      "        [0.0591],\n",
      "        [0.0378],\n",
      "        [0.0676],\n",
      "        [0.0672],\n",
      "        [0.0473],\n",
      "        [0.0561],\n",
      "        [0.0721],\n",
      "        [0.0470],\n",
      "        [0.0620],\n",
      "        [0.0590],\n",
      "        [0.0662],\n",
      "        [0.0565],\n",
      "        [0.0476],\n",
      "        [0.0578],\n",
      "        [0.0552],\n",
      "        [0.0570],\n",
      "        [0.0589],\n",
      "        [0.0519],\n",
      "        [0.0591],\n",
      "        [0.0561],\n",
      "        [0.0543],\n",
      "        [0.0370],\n",
      "        [0.0614],\n",
      "        [0.0692],\n",
      "        [0.0540],\n",
      "        [0.0544],\n",
      "        [0.0630],\n",
      "        [0.0643],\n",
      "        [0.0585],\n",
      "        [0.0598],\n",
      "        [0.0690],\n",
      "        [0.0653],\n",
      "        [0.0537],\n",
      "        [0.0661],\n",
      "        [0.0587],\n",
      "        [0.0533],\n",
      "        [0.0558],\n",
      "        [0.0568],\n",
      "        [0.0544],\n",
      "        [0.0627],\n",
      "        [0.0568],\n",
      "        [0.0493],\n",
      "        [0.0633],\n",
      "        [0.0480],\n",
      "        [0.0496],\n",
      "        [0.0597],\n",
      "        [0.0686],\n",
      "        [0.0594],\n",
      "        [0.0485],\n",
      "        [0.0784],\n",
      "        [0.0610],\n",
      "        [0.0521],\n",
      "        [0.0612],\n",
      "        [0.0531],\n",
      "        [0.0530],\n",
      "        [0.0472],\n",
      "        [0.0690],\n",
      "        [0.0550],\n",
      "        [0.0547],\n",
      "        [0.0554],\n",
      "        [0.0534],\n",
      "        [0.0588],\n",
      "        [0.0452],\n",
      "        [0.0584],\n",
      "        [0.0514],\n",
      "        [0.0616],\n",
      "        [0.0614],\n",
      "        [0.0562],\n",
      "        [0.0695],\n",
      "        [0.0548],\n",
      "        [0.0572],\n",
      "        [0.0561],\n",
      "        [0.0592],\n",
      "        [0.0590],\n",
      "        [0.0627],\n",
      "        [0.0725],\n",
      "        [0.0656],\n",
      "        [0.0597],\n",
      "        [0.0488],\n",
      "        [0.0709],\n",
      "        [0.0749],\n",
      "        [0.0431],\n",
      "        [0.0606],\n",
      "        [0.0650],\n",
      "        [0.0596],\n",
      "        [0.0553],\n",
      "        [0.0523],\n",
      "        [0.0615],\n",
      "        [0.0503],\n",
      "        [0.0561],\n",
      "        [0.0467],\n",
      "        [0.0615],\n",
      "        [0.0535],\n",
      "        [0.0529],\n",
      "        [0.0559],\n",
      "        [0.0603],\n",
      "        [0.0650],\n",
      "        [0.0494],\n",
      "        [0.0674],\n",
      "        [0.0580],\n",
      "        [0.0531],\n",
      "        [0.0591],\n",
      "        [0.0488],\n",
      "        [0.0712],\n",
      "        [0.0609],\n",
      "        [0.0599],\n",
      "        [0.0663],\n",
      "        [0.0687],\n",
      "        [0.0629],\n",
      "        [0.0586],\n",
      "        [0.0616],\n",
      "        [0.0628],\n",
      "        [0.0555],\n",
      "        [0.0615],\n",
      "        [0.0386],\n",
      "        [0.0635]], device='cuda:0')\n",
      "Valid:  [ 5/11]  eta: 0:00:43  loss: 17800.6328 (18098.3415)  metric: 125.6456 (127.4860)  L2_Loss: 17800.6328 (18098.3415)  time: 7.2507  data: 7.0828  max mem: 15480\n",
      "tensor([[0.0608],\n",
      "        [0.0550],\n",
      "        [0.0629],\n",
      "        [0.0604],\n",
      "        [0.0568],\n",
      "        [0.0501],\n",
      "        [0.0559],\n",
      "        [0.0509],\n",
      "        [0.0708],\n",
      "        [0.0590],\n",
      "        [0.0574],\n",
      "        [0.0522],\n",
      "        [0.0583],\n",
      "        [0.0656],\n",
      "        [0.0723],\n",
      "        [0.0583],\n",
      "        [0.0707],\n",
      "        [0.0560],\n",
      "        [0.0655],\n",
      "        [0.0607],\n",
      "        [0.0584],\n",
      "        [0.0555],\n",
      "        [0.0564],\n",
      "        [0.0634],\n",
      "        [0.0523],\n",
      "        [0.0452],\n",
      "        [0.0636],\n",
      "        [0.0546],\n",
      "        [0.0598],\n",
      "        [0.0576],\n",
      "        [0.0578],\n",
      "        [0.0516],\n",
      "        [0.0609],\n",
      "        [0.0594],\n",
      "        [0.0614],\n",
      "        [0.0607],\n",
      "        [0.0565],\n",
      "        [0.0414],\n",
      "        [0.0583],\n",
      "        [0.0619],\n",
      "        [0.0653],\n",
      "        [0.0592],\n",
      "        [0.0472],\n",
      "        [0.0511],\n",
      "        [0.0519],\n",
      "        [0.0597],\n",
      "        [0.0575],\n",
      "        [0.0612],\n",
      "        [0.0609],\n",
      "        [0.0570],\n",
      "        [0.0480],\n",
      "        [0.0554],\n",
      "        [0.0618],\n",
      "        [0.0499],\n",
      "        [0.0521],\n",
      "        [0.0529],\n",
      "        [0.0576],\n",
      "        [0.0600],\n",
      "        [0.0607],\n",
      "        [0.0620],\n",
      "        [0.0596],\n",
      "        [0.0512],\n",
      "        [0.0416],\n",
      "        [0.0549],\n",
      "        [0.0589],\n",
      "        [0.0448],\n",
      "        [0.0634],\n",
      "        [0.0524],\n",
      "        [0.0672],\n",
      "        [0.0568],\n",
      "        [0.0536],\n",
      "        [0.0593],\n",
      "        [0.0498],\n",
      "        [0.0615],\n",
      "        [0.0549],\n",
      "        [0.0557],\n",
      "        [0.0575],\n",
      "        [0.0541],\n",
      "        [0.0586],\n",
      "        [0.0582],\n",
      "        [0.0536],\n",
      "        [0.0594],\n",
      "        [0.0580],\n",
      "        [0.0561],\n",
      "        [0.0573],\n",
      "        [0.0501],\n",
      "        [0.0643],\n",
      "        [0.0553],\n",
      "        [0.0591],\n",
      "        [0.0624],\n",
      "        [0.0565],\n",
      "        [0.0643],\n",
      "        [0.0516],\n",
      "        [0.0712],\n",
      "        [0.0667],\n",
      "        [0.0487],\n",
      "        [0.0561],\n",
      "        [0.0461],\n",
      "        [0.0506],\n",
      "        [0.0623],\n",
      "        [0.0587],\n",
      "        [0.0589],\n",
      "        [0.0466],\n",
      "        [0.0602],\n",
      "        [0.0616],\n",
      "        [0.0496],\n",
      "        [0.0430],\n",
      "        [0.0604],\n",
      "        [0.0603],\n",
      "        [0.0569],\n",
      "        [0.0590],\n",
      "        [0.0586],\n",
      "        [0.0499],\n",
      "        [0.0727],\n",
      "        [0.0597],\n",
      "        [0.0584],\n",
      "        [0.0728],\n",
      "        [0.0632],\n",
      "        [0.0610],\n",
      "        [0.0766],\n",
      "        [0.0579],\n",
      "        [0.0629],\n",
      "        [0.0553],\n",
      "        [0.0460],\n",
      "        [0.0522],\n",
      "        [0.0671],\n",
      "        [0.0550],\n",
      "        [0.0615],\n",
      "        [0.0538],\n",
      "        [0.0597],\n",
      "        [0.0579],\n",
      "        [0.0618],\n",
      "        [0.0555],\n",
      "        [0.0656],\n",
      "        [0.0612],\n",
      "        [0.0580],\n",
      "        [0.0689],\n",
      "        [0.0525],\n",
      "        [0.0586],\n",
      "        [0.0452]], device='cuda:0')\n",
      "Valid:  [ 6/11]  eta: 0:00:39  loss: 18165.3926 (18107.9202)  metric: 128.3919 (127.6451)  L2_Loss: 18165.3926 (18107.9202)  time: 7.9290  data: 7.7614  max mem: 15480\n",
      "tensor([[0.0460],\n",
      "        [0.0606],\n",
      "        [0.0531],\n",
      "        [0.0598],\n",
      "        [0.0573],\n",
      "        [0.0502],\n",
      "        [0.0493],\n",
      "        [0.0601],\n",
      "        [0.0739],\n",
      "        [0.0530],\n",
      "        [0.0559],\n",
      "        [0.0562],\n",
      "        [0.0590],\n",
      "        [0.0625],\n",
      "        [0.0578],\n",
      "        [0.0624],\n",
      "        [0.0607],\n",
      "        [0.0598],\n",
      "        [0.0554],\n",
      "        [0.0604],\n",
      "        [0.0575],\n",
      "        [0.0550],\n",
      "        [0.0653],\n",
      "        [0.0650],\n",
      "        [0.0614],\n",
      "        [0.0661],\n",
      "        [0.0547],\n",
      "        [0.0604],\n",
      "        [0.0662],\n",
      "        [0.0617],\n",
      "        [0.0555],\n",
      "        [0.0514],\n",
      "        [0.0487],\n",
      "        [0.0554],\n",
      "        [0.0552],\n",
      "        [0.0635],\n",
      "        [0.0482],\n",
      "        [0.0669],\n",
      "        [0.0538],\n",
      "        [0.0615],\n",
      "        [0.0430],\n",
      "        [0.0641],\n",
      "        [0.0725],\n",
      "        [0.0618],\n",
      "        [0.0585],\n",
      "        [0.0598],\n",
      "        [0.0517],\n",
      "        [0.0499],\n",
      "        [0.0512],\n",
      "        [0.0558],\n",
      "        [0.0666],\n",
      "        [0.0648],\n",
      "        [0.0633],\n",
      "        [0.0614],\n",
      "        [0.0458],\n",
      "        [0.0665],\n",
      "        [0.0531],\n",
      "        [0.0554],\n",
      "        [0.0655],\n",
      "        [0.0669],\n",
      "        [0.0597],\n",
      "        [0.0655],\n",
      "        [0.0558],\n",
      "        [0.0593],\n",
      "        [0.0448],\n",
      "        [0.0572],\n",
      "        [0.0575],\n",
      "        [0.0541],\n",
      "        [0.0578],\n",
      "        [0.0604],\n",
      "        [0.0729],\n",
      "        [0.0588],\n",
      "        [0.0513],\n",
      "        [0.0678],\n",
      "        [0.0474],\n",
      "        [0.0572],\n",
      "        [0.0537],\n",
      "        [0.0609],\n",
      "        [0.0610],\n",
      "        [0.0666],\n",
      "        [0.0465],\n",
      "        [0.0544],\n",
      "        [0.0579],\n",
      "        [0.0771],\n",
      "        [0.0423],\n",
      "        [0.0645],\n",
      "        [0.0689],\n",
      "        [0.0850],\n",
      "        [0.0677],\n",
      "        [0.0540],\n",
      "        [0.0525],\n",
      "        [0.0553],\n",
      "        [0.0547],\n",
      "        [0.0709],\n",
      "        [0.0534],\n",
      "        [0.0512],\n",
      "        [0.0669],\n",
      "        [0.0636],\n",
      "        [0.0543],\n",
      "        [0.0704],\n",
      "        [0.0612],\n",
      "        [0.0592],\n",
      "        [0.0524],\n",
      "        [0.0449],\n",
      "        [0.0505],\n",
      "        [0.0465],\n",
      "        [0.0436],\n",
      "        [0.0500],\n",
      "        [0.0550],\n",
      "        [0.0568],\n",
      "        [0.0654],\n",
      "        [0.0523],\n",
      "        [0.0509],\n",
      "        [0.0584],\n",
      "        [0.0610],\n",
      "        [0.0622],\n",
      "        [0.0457],\n",
      "        [0.0748],\n",
      "        [0.0643],\n",
      "        [0.0560],\n",
      "        [0.0577],\n",
      "        [0.0542],\n",
      "        [0.0613],\n",
      "        [0.0729],\n",
      "        [0.0602],\n",
      "        [0.0656],\n",
      "        [0.0751],\n",
      "        [0.0559],\n",
      "        [0.0514],\n",
      "        [0.0605],\n",
      "        [0.0571],\n",
      "        [0.0505],\n",
      "        [0.0616],\n",
      "        [0.0713],\n",
      "        [0.0595],\n",
      "        [0.0563],\n",
      "        [0.0498],\n",
      "        [0.0518],\n",
      "        [0.0655],\n",
      "        [0.0690]], device='cuda:0')\n",
      "Valid:  [ 7/11]  eta: 0:00:27  loss: 17920.2715 (18084.4641)  metric: 127.3057 (127.6026)  L2_Loss: 17920.2715 (18084.4641)  time: 6.9583  data: 6.7912  max mem: 15480\n",
      "tensor([[0.0588],\n",
      "        [0.0646],\n",
      "        [0.0581],\n",
      "        [0.0569],\n",
      "        [0.0694],\n",
      "        [0.0612],\n",
      "        [0.0531],\n",
      "        [0.0571],\n",
      "        [0.0482],\n",
      "        [0.0626],\n",
      "        [0.0599],\n",
      "        [0.0620],\n",
      "        [0.0578],\n",
      "        [0.0705],\n",
      "        [0.0548],\n",
      "        [0.0550],\n",
      "        [0.0663],\n",
      "        [0.0496],\n",
      "        [0.0650],\n",
      "        [0.0560],\n",
      "        [0.0515],\n",
      "        [0.0837],\n",
      "        [0.0556],\n",
      "        [0.0705],\n",
      "        [0.0643],\n",
      "        [0.0659],\n",
      "        [0.0476],\n",
      "        [0.0536],\n",
      "        [0.0562],\n",
      "        [0.0575],\n",
      "        [0.0498],\n",
      "        [0.0599],\n",
      "        [0.0623],\n",
      "        [0.0759],\n",
      "        [0.0686],\n",
      "        [0.0622],\n",
      "        [0.0544],\n",
      "        [0.0520],\n",
      "        [0.0558],\n",
      "        [0.0654],\n",
      "        [0.0623],\n",
      "        [0.0620],\n",
      "        [0.0573],\n",
      "        [0.0637],\n",
      "        [0.0672],\n",
      "        [0.0778],\n",
      "        [0.0566],\n",
      "        [0.0617],\n",
      "        [0.0455],\n",
      "        [0.0587],\n",
      "        [0.0573],\n",
      "        [0.0696],\n",
      "        [0.0676],\n",
      "        [0.0625],\n",
      "        [0.0580],\n",
      "        [0.0537],\n",
      "        [0.0636],\n",
      "        [0.0532],\n",
      "        [0.0431],\n",
      "        [0.0786],\n",
      "        [0.0650],\n",
      "        [0.0554],\n",
      "        [0.0669],\n",
      "        [0.0610],\n",
      "        [0.0527],\n",
      "        [0.0558],\n",
      "        [0.0554],\n",
      "        [0.0501],\n",
      "        [0.0535],\n",
      "        [0.0535],\n",
      "        [0.0553],\n",
      "        [0.0508],\n",
      "        [0.0623],\n",
      "        [0.0615],\n",
      "        [0.0498],\n",
      "        [0.0706],\n",
      "        [0.0527],\n",
      "        [0.0552],\n",
      "        [0.0505],\n",
      "        [0.0580],\n",
      "        [0.0551],\n",
      "        [0.0642],\n",
      "        [0.0640],\n",
      "        [0.0668],\n",
      "        [0.0514],\n",
      "        [0.0526],\n",
      "        [0.0674],\n",
      "        [0.0618],\n",
      "        [0.0601],\n",
      "        [0.0620],\n",
      "        [0.0514],\n",
      "        [0.0581],\n",
      "        [0.0566],\n",
      "        [0.0757],\n",
      "        [0.0569],\n",
      "        [0.0490],\n",
      "        [0.0600],\n",
      "        [0.0778],\n",
      "        [0.0602],\n",
      "        [0.0568],\n",
      "        [0.0553],\n",
      "        [0.0541],\n",
      "        [0.0615],\n",
      "        [0.0550],\n",
      "        [0.0536],\n",
      "        [0.0570],\n",
      "        [0.0490],\n",
      "        [0.0577],\n",
      "        [0.0589],\n",
      "        [0.0598],\n",
      "        [0.0584],\n",
      "        [0.0590],\n",
      "        [0.0605],\n",
      "        [0.0492],\n",
      "        [0.0559],\n",
      "        [0.0587],\n",
      "        [0.0441],\n",
      "        [0.0624],\n",
      "        [0.0662],\n",
      "        [0.0651],\n",
      "        [0.0490],\n",
      "        [0.0532],\n",
      "        [0.0620],\n",
      "        [0.0575],\n",
      "        [0.0547],\n",
      "        [0.0588],\n",
      "        [0.0647],\n",
      "        [0.0641],\n",
      "        [0.0471],\n",
      "        [0.0662],\n",
      "        [0.0626],\n",
      "        [0.0589],\n",
      "        [0.0512],\n",
      "        [0.0627],\n",
      "        [0.0666],\n",
      "        [0.0578],\n",
      "        [0.0527],\n",
      "        [0.0552],\n",
      "        [0.0583],\n",
      "        [0.0519]], device='cuda:0')\n",
      "Valid:  [ 8/11]  eta: 0:00:23  loss: 17920.2715 (17974.5484)  metric: 127.3057 (127.2553)  L2_Loss: 17920.2715 (17974.5484)  time: 7.6690  data: 7.5022  max mem: 15480\n",
      "tensor([[0.0558],\n",
      "        [0.0586],\n",
      "        [0.0504],\n",
      "        [0.0548],\n",
      "        [0.0516],\n",
      "        [0.0540],\n",
      "        [0.0725],\n",
      "        [0.0621],\n",
      "        [0.0489],\n",
      "        [0.0494],\n",
      "        [0.0574],\n",
      "        [0.0529],\n",
      "        [0.0571],\n",
      "        [0.0685],\n",
      "        [0.0535],\n",
      "        [0.0486],\n",
      "        [0.0556],\n",
      "        [0.0509],\n",
      "        [0.0591],\n",
      "        [0.0537],\n",
      "        [0.0573],\n",
      "        [0.0571],\n",
      "        [0.0547],\n",
      "        [0.0607],\n",
      "        [0.0457],\n",
      "        [0.0562],\n",
      "        [0.0637],\n",
      "        [0.0681],\n",
      "        [0.0706],\n",
      "        [0.0662],\n",
      "        [0.0695],\n",
      "        [0.0570],\n",
      "        [0.0598],\n",
      "        [0.0563],\n",
      "        [0.0758],\n",
      "        [0.0433],\n",
      "        [0.0523],\n",
      "        [0.0449],\n",
      "        [0.0540],\n",
      "        [0.0536],\n",
      "        [0.0572],\n",
      "        [0.0579],\n",
      "        [0.0629],\n",
      "        [0.0465],\n",
      "        [0.0625],\n",
      "        [0.0549],\n",
      "        [0.0575],\n",
      "        [0.0509],\n",
      "        [0.0671],\n",
      "        [0.0608],\n",
      "        [0.0436],\n",
      "        [0.0429],\n",
      "        [0.0673],\n",
      "        [0.0594],\n",
      "        [0.0746],\n",
      "        [0.0607],\n",
      "        [0.0631],\n",
      "        [0.0432],\n",
      "        [0.0653],\n",
      "        [0.0588],\n",
      "        [0.0584],\n",
      "        [0.0515],\n",
      "        [0.0555],\n",
      "        [0.0583],\n",
      "        [0.0663],\n",
      "        [0.0536],\n",
      "        [0.0648],\n",
      "        [0.0579],\n",
      "        [0.0566],\n",
      "        [0.0500],\n",
      "        [0.0406],\n",
      "        [0.0582],\n",
      "        [0.0547],\n",
      "        [0.0643],\n",
      "        [0.0464],\n",
      "        [0.0603],\n",
      "        [0.0610],\n",
      "        [0.0669],\n",
      "        [0.0481],\n",
      "        [0.0556],\n",
      "        [0.0483],\n",
      "        [0.0715],\n",
      "        [0.0608],\n",
      "        [0.0621],\n",
      "        [0.0608],\n",
      "        [0.0566],\n",
      "        [0.0502],\n",
      "        [0.0539],\n",
      "        [0.0575],\n",
      "        [0.0541],\n",
      "        [0.0579],\n",
      "        [0.0442],\n",
      "        [0.0594],\n",
      "        [0.0476],\n",
      "        [0.0555],\n",
      "        [0.0474],\n",
      "        [0.0533],\n",
      "        [0.0650],\n",
      "        [0.0455],\n",
      "        [0.0585],\n",
      "        [0.0445],\n",
      "        [0.0720],\n",
      "        [0.0649],\n",
      "        [0.0638],\n",
      "        [0.0566],\n",
      "        [0.0488],\n",
      "        [0.0642],\n",
      "        [0.0641],\n",
      "        [0.0431],\n",
      "        [0.0503],\n",
      "        [0.0526],\n",
      "        [0.0534],\n",
      "        [0.0795],\n",
      "        [0.0555],\n",
      "        [0.0556],\n",
      "        [0.0565],\n",
      "        [0.0537],\n",
      "        [0.0518],\n",
      "        [0.0540],\n",
      "        [0.0626],\n",
      "        [0.0523],\n",
      "        [0.0766],\n",
      "        [0.0682],\n",
      "        [0.0655],\n",
      "        [0.0636],\n",
      "        [0.0616],\n",
      "        [0.0462],\n",
      "        [0.0639],\n",
      "        [0.0796],\n",
      "        [0.0636],\n",
      "        [0.0607],\n",
      "        [0.0550],\n",
      "        [0.0634],\n",
      "        [0.0559],\n",
      "        [0.0687],\n",
      "        [0.0583],\n",
      "        [0.0620],\n",
      "        [0.0542],\n",
      "        [0.0482],\n",
      "        [0.0708]], device='cuda:0')\n",
      "Valid:  [ 9/11]  eta: 0:00:13  loss: 17800.6328 (17840.3439)  metric: 125.6456 (126.9097)  L2_Loss: 17800.6328 (17840.3439)  time: 6.9184  data: 6.7520  max mem: 15480\n",
      "tensor([[0.0593],\n",
      "        [0.0687],\n",
      "        [0.0572],\n",
      "        [0.0574],\n",
      "        [0.0502],\n",
      "        [0.0621],\n",
      "        [0.0748],\n",
      "        [0.0560],\n",
      "        [0.0641],\n",
      "        [0.0647],\n",
      "        [0.0595],\n",
      "        [0.0703],\n",
      "        [0.0670],\n",
      "        [0.0538],\n",
      "        [0.0491],\n",
      "        [0.0709],\n",
      "        [0.0522],\n",
      "        [0.0498],\n",
      "        [0.0573],\n",
      "        [0.0542],\n",
      "        [0.0566],\n",
      "        [0.0593],\n",
      "        [0.0660],\n",
      "        [0.0535],\n",
      "        [0.0566]], device='cuda:0')\n",
      "Valid:  [10/11]  eta: 0:00:06  loss: 17920.2715 (18114.5911)  metric: 127.3057 (127.8834)  L2_Loss: 17920.2715 (18114.5911)  time: 6.4744  data: 6.3200  max mem: 15480\n",
      "Valid: Total time: 0:01:11 (6.4829 s / it)\n",
      "* Loss:18114.591 | Metric:127.883 \n",
      "Averaged valid_stats:  {'loss': 18114.591086647728, 'MAE': 127.88343533602628}\n"
     ]
    }
   ],
   "source": [
    "!python train.py \\\n",
    "--data_set 'RSNA_BAA' \\\n",
    "--model_name 'Downtask_RSNA_Boneage' \\\n",
    "--data_folder_dir \"/mnt/nas125_vol2/kanggilpark/child/bone_age/data\" \\\n",
    "--batch-size 140 \\\n",
    "--num_workers 2 \\\n",
    "--multi-gpu-mode 'Single' \\\n",
    "--cuda-visible-devices '1' \\\n",
    "--gradual_unfreeze true \\\n",
    "--print-freq 1\\\n",
    "--output_dir outputss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************************\n",
      "*           Training Mode is  Downstream      *\n",
      "***********************************************\n",
      "Dataset Name:  /mnt/nas125_vol2/kanggilpark/child/jyp_child/data\n",
      "---------- Model ----------\n",
      "Resume From:  \n",
      "Output Save To:  outputssss\n",
      "Visible GPUs:  1\n",
      "---------- Optimizer ----------\n",
      "Learning Rate:  0.0005\n",
      "Batchsize:  120\n",
      "Loading dataset ....\n",
      "Train [Total]  number =  6838\n",
      "Valid [Total]  number =  624\n",
      "Creating model  : Downtask_Pneumonia\n",
      "Pretrained model: \n",
      "Number of Learnable Params: 26650561\n",
      "Pneumonia_Model(\n",
      "  (encoder): ResNet_Feature_Extractor(\n",
      "    (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (fc1): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (drop1): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (drop2): Dropout(p=0.5, inplace=False)\n",
      "  (head): Linear(in_features=1024, out_features=1, bias=True)\n",
      ")\n",
      "Start training for 500 epochs\n",
      "Freeze encoder ...!\n",
      "Epoch: [0]  [ 0/56]  eta: 0:16:39  lr: 0.000000  loss: 0.6918 (0.6918)  BCE_Loss: 0.6918 (0.6918)  time: 17.8556  data: 16.5438  max mem: 6462\n",
      "Epoch: [0]  [ 1/56]  eta: 0:08:26  lr: 0.000000  loss: 0.6838 (0.6878)  BCE_Loss: 0.6838 (0.6878)  time: 9.2110  data: 8.2719  max mem: 6498\n",
      "Epoch: [0]  [ 2/56]  eta: 0:05:41  lr: 0.000000  loss: 0.6881 (0.6879)  BCE_Loss: 0.6881 (0.6879)  time: 6.3299  data: 5.5146  max mem: 6498\n",
      "Epoch: [0]  [ 3/56]  eta: 0:04:19  lr: 0.000000  loss: 0.6881 (0.6897)  BCE_Loss: 0.6881 (0.6897)  time: 4.8883  data: 4.1360  max mem: 6498\n",
      "Epoch: [0]  [ 4/56]  eta: 0:05:27  lr: 0.000000  loss: 0.6918 (0.6921)  BCE_Loss: 0.6918 (0.6921)  time: 6.2966  data: 5.5817  max mem: 6498\n",
      "Epoch: [0]  [ 5/56]  eta: 0:04:32  lr: 0.000000  loss: 0.6918 (0.6935)  BCE_Loss: 0.6918 (0.6935)  time: 5.3413  data: 4.6515  max mem: 6498\n",
      "Epoch: [0]  [ 6/56]  eta: 0:03:52  lr: 0.000000  loss: 0.6949 (0.6948)  BCE_Loss: 0.6949 (0.6948)  time: 4.6590  data: 3.9870  max mem: 6498\n",
      "Epoch: [0]  [ 7/56]  eta: 0:03:23  lr: 0.000000  loss: 0.6949 (0.6952)  BCE_Loss: 0.6949 (0.6952)  time: 4.1472  data: 3.4886  max mem: 6498\n",
      "Epoch: [0]  [ 8/56]  eta: 0:04:09  lr: 0.000000  loss: 0.6984 (0.6960)  BCE_Loss: 0.6984 (0.6960)  time: 5.1876  data: 4.5394  max mem: 6498\n",
      "Epoch: [0]  [ 9/56]  eta: 0:03:42  lr: 0.000000  loss: 0.6949 (0.6959)  BCE_Loss: 0.6949 (0.6959)  time: 4.7255  data: 4.0855  max mem: 6498\n",
      "Epoch: [0]  [10/56]  eta: 0:03:19  lr: 0.000000  loss: 0.6949 (0.6951)  BCE_Loss: 0.6949 (0.6951)  time: 4.3473  data: 3.7141  max mem: 6498\n",
      "Epoch: [0]  [11/56]  eta: 0:03:01  lr: 0.000000  loss: 0.6946 (0.6946)  BCE_Loss: 0.6946 (0.6946)  time: 4.0322  data: 3.4046  max mem: 6498\n",
      "Epoch: [0]  [12/56]  eta: 0:03:27  lr: 0.000000  loss: 0.6949 (0.6951)  BCE_Loss: 0.6949 (0.6951)  time: 4.7242  data: 4.1011  max mem: 6498\n",
      "Epoch: [0]  [13/56]  eta: 0:03:10  lr: 0.000000  loss: 0.6949 (0.6956)  BCE_Loss: 0.6949 (0.6956)  time: 4.4273  data: 3.8082  max mem: 6498\n",
      "Epoch: [0]  [14/56]  eta: 0:02:55  lr: 0.000000  loss: 0.6984 (0.6962)  BCE_Loss: 0.6984 (0.6962)  time: 4.1699  data: 3.5543  max mem: 6498\n",
      "Epoch: [0]  [15/56]  eta: 0:02:41  lr: 0.000000  loss: 0.6949 (0.6960)  BCE_Loss: 0.6949 (0.6960)  time: 3.9450  data: 3.3322  max mem: 6498\n",
      "Epoch: [0]  [16/56]  eta: 0:02:58  lr: 0.000000  loss: 0.6949 (0.6953)  BCE_Loss: 0.6949 (0.6953)  time: 4.4712  data: 3.8611  max mem: 6498\n",
      "Epoch: [0]  [17/56]  eta: 0:02:45  lr: 0.000000  loss: 0.6946 (0.6949)  BCE_Loss: 0.6946 (0.6949)  time: 4.2542  data: 3.6466  max mem: 6498\n",
      "Epoch: [0]  [18/56]  eta: 0:02:34  lr: 0.000000  loss: 0.6946 (0.6948)  BCE_Loss: 0.6946 (0.6948)  time: 4.0779  data: 3.4725  max mem: 6498\n",
      "Epoch: [0]  [19/56]  eta: 0:02:24  lr: 0.000000  loss: 0.6936 (0.6945)  BCE_Loss: 0.6936 (0.6945)  time: 3.9023  data: 3.2989  max mem: 6498\n",
      "Epoch: [0]  [20/56]  eta: 0:02:34  lr: 0.000000  loss: 0.6936 (0.6943)  BCE_Loss: 0.6936 (0.6943)  time: 3.6016  data: 3.0354  max mem: 6498\n",
      "Epoch: [0]  [21/56]  eta: 0:02:23  lr: 0.000000  loss: 0.6946 (0.6945)  BCE_Loss: 0.6946 (0.6945)  time: 3.6016  data: 3.0354  max mem: 6498\n",
      "Epoch: [0]  [22/56]  eta: 0:02:16  lr: 0.000000  loss: 0.6946 (0.6944)  BCE_Loss: 0.6946 (0.6944)  time: 3.6749  data: 3.1087  max mem: 6498\n",
      "Epoch: [0]  [23/56]  eta: 0:02:07  lr: 0.000000  loss: 0.6946 (0.6945)  BCE_Loss: 0.6946 (0.6945)  time: 3.6750  data: 3.1087  max mem: 6498\n",
      "Epoch: [0]  [24/56]  eta: 0:02:13  lr: 0.000000  loss: 0.6946 (0.6948)  BCE_Loss: 0.6946 (0.6948)  time: 3.6574  data: 3.0910  max mem: 6498\n",
      "Epoch: [0]  [25/56]  eta: 0:02:05  lr: 0.000000  loss: 0.6936 (0.6948)  BCE_Loss: 0.6936 (0.6948)  time: 3.6615  data: 3.0951  max mem: 6498\n",
      "Epoch: [0]  [26/56]  eta: 0:01:59  lr: 0.000000  loss: 0.6936 (0.6954)  BCE_Loss: 0.6936 (0.6954)  time: 3.7484  data: 3.1819  max mem: 6498\n",
      "Epoch: [0]  [27/56]  eta: 0:01:52  lr: 0.000000  loss: 0.6936 (0.6958)  BCE_Loss: 0.6936 (0.6958)  time: 3.7485  data: 3.1819  max mem: 6498\n",
      "Epoch: [0]  [28/56]  eta: 0:01:55  lr: 0.000000  loss: 0.6936 (0.6962)  BCE_Loss: 0.6936 (0.6962)  time: 3.6504  data: 3.0837  max mem: 6498\n",
      "Epoch: [0]  [29/56]  eta: 0:01:48  lr: 0.000000  loss: 0.6936 (0.6958)  BCE_Loss: 0.6936 (0.6958)  time: 3.6675  data: 3.1009  max mem: 6498\n",
      "Epoch: [0]  [30/56]  eta: 0:01:42  lr: 0.000000  loss: 0.6936 (0.6961)  BCE_Loss: 0.6936 (0.6961)  time: 3.7100  data: 3.1433  max mem: 6498\n",
      "Epoch: [0]  [31/56]  eta: 0:01:35  lr: 0.000000  loss: 0.6962 (0.6964)  BCE_Loss: 0.6962 (0.6964)  time: 3.7100  data: 3.1434  max mem: 6498\n",
      "Epoch: [0]  [32/56]  eta: 0:01:38  lr: 0.000000  loss: 0.6938 (0.6963)  BCE_Loss: 0.6938 (0.6963)  time: 3.6999  data: 3.1334  max mem: 6498\n",
      "Epoch: [0]  [33/56]  eta: 0:01:32  lr: 0.000000  loss: 0.6938 (0.6964)  BCE_Loss: 0.6938 (0.6964)  time: 3.7461  data: 3.1796  max mem: 6498\n",
      "Epoch: [0]  [34/56]  eta: 0:01:26  lr: 0.000000  loss: 0.6936 (0.6962)  BCE_Loss: 0.6936 (0.6962)  time: 3.7461  data: 3.1796  max mem: 6498\n",
      "Epoch: [0]  [35/56]  eta: 0:01:20  lr: 0.000000  loss: 0.6936 (0.6961)  BCE_Loss: 0.6936 (0.6961)  time: 3.7459  data: 3.1796  max mem: 6498\n",
      "Epoch: [0]  [36/56]  eta: 0:01:20  lr: 0.000000  loss: 0.6936 (0.6957)  BCE_Loss: 0.6936 (0.6957)  time: 3.6744  data: 3.1081  max mem: 6498\n",
      "Epoch: [0]  [37/56]  eta: 0:01:16  lr: 0.000000  loss: 0.6938 (0.6960)  BCE_Loss: 0.6938 (0.6960)  time: 3.7747  data: 3.2084  max mem: 6498\n",
      "Epoch: [0]  [38/56]  eta: 0:01:10  lr: 0.000000  loss: 0.6950 (0.6959)  BCE_Loss: 0.6950 (0.6959)  time: 3.7581  data: 3.1914  max mem: 6498\n",
      "Epoch: [0]  [39/56]  eta: 0:01:05  lr: 0.000000  loss: 0.6962 (0.6960)  BCE_Loss: 0.6962 (0.6960)  time: 3.7581  data: 3.1914  max mem: 6498\n",
      "Epoch: [0]  [40/56]  eta: 0:01:04  lr: 0.000000  loss: 0.6962 (0.6960)  BCE_Loss: 0.6962 (0.6960)  time: 3.7239  data: 3.1573  max mem: 6498\n",
      "Epoch: [0]  [41/56]  eta: 0:00:59  lr: 0.000000  loss: 0.6953 (0.6959)  BCE_Loss: 0.6953 (0.6959)  time: 3.8480  data: 3.2813  max mem: 6498\n",
      "Epoch: [0]  [42/56]  eta: 0:00:54  lr: 0.000000  loss: 0.6953 (0.6957)  BCE_Loss: 0.6953 (0.6957)  time: 3.7747  data: 3.2080  max mem: 6498\n",
      "Epoch: [0]  [43/56]  eta: 0:00:49  lr: 0.000000  loss: 0.6953 (0.6958)  BCE_Loss: 0.6953 (0.6958)  time: 3.7748  data: 3.2080  max mem: 6498\n",
      "Epoch: [0]  [44/56]  eta: 0:00:47  lr: 0.000000  loss: 0.6953 (0.6959)  BCE_Loss: 0.6953 (0.6959)  time: 3.7560  data: 3.1891  max mem: 6498\n",
      "Epoch: [0]  [45/56]  eta: 0:00:43  lr: 0.000000  loss: 0.6958 (0.6959)  BCE_Loss: 0.6958 (0.6959)  time: 3.8686  data: 3.3010  max mem: 6498\n",
      "Epoch: [0]  [46/56]  eta: 0:00:38  lr: 0.000000  loss: 0.6953 (0.6958)  BCE_Loss: 0.6953 (0.6958)  time: 3.7817  data: 3.2142  max mem: 6498\n",
      "Epoch: [0]  [47/56]  eta: 0:00:34  lr: 0.000000  loss: 0.6952 (0.6955)  BCE_Loss: 0.6952 (0.6955)  time: 3.7818  data: 3.2142  max mem: 6498\n",
      "Epoch: [0]  [48/56]  eta: 0:00:31  lr: 0.000000  loss: 0.6950 (0.6955)  BCE_Loss: 0.6950 (0.6955)  time: 3.7390  data: 3.1712  max mem: 6498\n",
      "Epoch: [0]  [49/56]  eta: 0:00:27  lr: 0.000000  loss: 0.6950 (0.6954)  BCE_Loss: 0.6950 (0.6954)  time: 3.8451  data: 3.2771  max mem: 6498\n",
      "Epoch: [0]  [50/56]  eta: 0:00:23  lr: 0.000000  loss: 0.6938 (0.6953)  BCE_Loss: 0.6938 (0.6953)  time: 3.8204  data: 3.2523  max mem: 6498\n",
      "Epoch: [0]  [51/56]  eta: 0:00:19  lr: 0.000000  loss: 0.6932 (0.6952)  BCE_Loss: 0.6932 (0.6952)  time: 3.8205  data: 3.2523  max mem: 6498\n",
      "Epoch: [0]  [52/56]  eta: 0:00:15  lr: 0.000000  loss: 0.6932 (0.6953)  BCE_Loss: 0.6932 (0.6953)  time: 3.6865  data: 3.1182  max mem: 6498\n",
      "Epoch: [0]  [53/56]  eta: 0:00:11  lr: 0.000000  loss: 0.6932 (0.6954)  BCE_Loss: 0.6932 (0.6954)  time: 3.7415  data: 3.1733  max mem: 6498\n",
      "Epoch: [0]  [54/56]  eta: 0:00:07  lr: 0.000000  loss: 0.6950 (0.6955)  BCE_Loss: 0.6950 (0.6955)  time: 3.8163  data: 3.2480  max mem: 6498\n",
      "Epoch: [0]  [55/56]  eta: 0:00:03  lr: 0.000000  loss: 0.6952 (0.6955)  BCE_Loss: 0.6952 (0.6955)  time: 3.8163  data: 3.2480  max mem: 6498\n",
      "Epoch: [0] Total time: 0:03:34 (3.8299 s / it)\n",
      "Averaged train_stats:  {'lr': 0.0, 'loss': 0.6954591, 'BCE_Loss': 0.6954591}\n",
      "Valid:  [0/6]  eta: 0:01:27  loss: 0.6939 (0.6939)  BCE_Loss: 0.6939 (0.6939)  time: 14.5865  data: 14.0469  max mem: 6498\n",
      "Valid:  [1/6]  eta: 0:00:37  loss: 0.6936 (0.6938)  BCE_Loss: 0.6936 (0.6938)  time: 7.5593  data: 7.0235  max mem: 6498\n",
      "Valid:  [2/6]  eta: 0:00:20  loss: 0.6936 (0.6931)  BCE_Loss: 0.6936 (0.6931)  time: 5.2167  data: 4.6823  max mem: 6498\n",
      "Valid:  [3/6]  eta: 0:00:12  loss: 0.6918 (0.6928)  BCE_Loss: 0.6918 (0.6928)  time: 4.0452  data: 3.5117  max mem: 6498\n",
      "Valid:  [4/6]  eta: 0:00:08  loss: 0.6918 (0.6925)  BCE_Loss: 0.6918 (0.6925)  time: 4.3106  data: 3.7774  max mem: 6498\n",
      "Valid:  [5/6]  eta: 0:00:03  loss: 0.6918 (0.6923)  BCE_Loss: 0.6918 (0.6923)  time: 3.6105  data: 3.1479  max mem: 6498\n",
      "Valid: Total time: 0:00:21 (3.6223 s / it)\n",
      "Averaged valid_stats:  {'loss': 0.6922987, 'BCE_Loss': 0.6922987, 'auc': 0.5966743, 'f1': 0.6716233, 'acc': 0.5753205, 'sen': 0.6948718, 'spe': 0.3760684}\n",
      "Freeze encoder ...!\n",
      "Epoch: [1]  [ 0/56]  eta: 0:12:15  lr: 0.000000  loss: 0.6917 (0.6917)  BCE_Loss: 0.6917 (0.6917)  time: 13.1417  data: 12.5735  max mem: 6498\n",
      "Epoch: [1]  [ 1/56]  eta: 0:06:17  lr: 0.000000  loss: 0.6917 (0.6953)  BCE_Loss: 0.6917 (0.6953)  time: 6.8551  data: 6.2868  max mem: 6498\n",
      "Epoch: [1]  [ 2/56]  eta: 0:04:17  lr: 0.000000  loss: 0.6970 (0.6959)  BCE_Loss: 0.6970 (0.6959)  time: 4.7596  data: 4.1912  max mem: 6498\n",
      "Epoch: [1]  [ 3/56]  eta: 0:03:16  lr: 0.000000  loss: 0.6922 (0.6950)  BCE_Loss: 0.6922 (0.6950)  time: 3.7117  data: 3.1434  max mem: 6498\n",
      "Epoch: [1]  [ 4/56]  eta: 0:04:21  lr: 0.000000  loss: 0.6970 (0.6960)  BCE_Loss: 0.6970 (0.6960)  time: 5.0254  data: 4.4572  max mem: 6498\n",
      "Epoch: [1]  [ 5/56]  eta: 0:03:38  lr: 0.000000  loss: 0.6970 (0.6976)  BCE_Loss: 0.6970 (0.6976)  time: 4.2825  data: 3.7143  max mem: 6498\n",
      "Epoch: [1]  [ 6/56]  eta: 0:03:07  lr: 0.000000  loss: 0.6988 (0.6979)  BCE_Loss: 0.6988 (0.6979)  time: 3.7517  data: 3.1837  max mem: 6498\n",
      "Epoch: [1]  [ 7/56]  eta: 0:02:44  lr: 0.000000  loss: 0.6970 (0.6966)  BCE_Loss: 0.6970 (0.6966)  time: 3.3538  data: 2.7858  max mem: 6498\n",
      "Epoch: [1]  [ 8/56]  eta: 0:03:12  lr: 0.000000  loss: 0.6979 (0.6967)  BCE_Loss: 0.6979 (0.6967)  time: 4.0146  data: 3.4464  max mem: 6498\n",
      "Epoch: [1]  [ 9/56]  eta: 0:02:52  lr: 0.000000  loss: 0.6970 (0.6967)  BCE_Loss: 0.6970 (0.6967)  time: 3.6700  data: 3.1017  max mem: 6498\n",
      "Epoch: [1]  [10/56]  eta: 0:02:35  lr: 0.000000  loss: 0.6979 (0.6972)  BCE_Loss: 0.6979 (0.6972)  time: 3.3881  data: 2.8198  max mem: 6498\n",
      "Epoch: [1]  [11/56]  eta: 0:02:21  lr: 0.000000  loss: 0.6970 (0.6956)  BCE_Loss: 0.6970 (0.6956)  time: 3.1531  data: 2.5848  max mem: 6498\n",
      "Epoch: [1]  [12/56]  eta: 0:02:41  lr: 0.000000  loss: 0.6979 (0.6959)  BCE_Loss: 0.6979 (0.6959)  time: 3.6695  data: 3.1010  max mem: 6498\n",
      "Epoch: [1]  [13/56]  eta: 0:02:28  lr: 0.000000  loss: 0.6979 (0.6962)  BCE_Loss: 0.6979 (0.6962)  time: 3.4480  data: 2.8795  max mem: 6498\n",
      "Epoch: [1]  [14/56]  eta: 0:02:16  lr: 0.000000  loss: 0.6988 (0.6965)  BCE_Loss: 0.6988 (0.6965)  time: 3.2560  data: 2.6875  max mem: 6498\n",
      "Epoch: [1]  [15/56]  eta: 0:02:06  lr: 0.000000  loss: 0.6988 (0.6967)  BCE_Loss: 0.6988 (0.6967)  time: 3.0881  data: 2.5196  max mem: 6498\n",
      "Epoch: [1]  [16/56]  eta: 0:02:16  lr: 0.000000  loss: 0.6989 (0.6975)  BCE_Loss: 0.6989 (0.6975)  time: 3.4032  data: 2.8347  max mem: 6498\n",
      "Epoch: [1]  [17/56]  eta: 0:02:06  lr: 0.000000  loss: 0.6989 (0.6976)  BCE_Loss: 0.6989 (0.6976)  time: 3.2457  data: 2.6772  max mem: 6498\n",
      "Epoch: [1]  [18/56]  eta: 0:01:57  lr: 0.000000  loss: 0.6989 (0.6976)  BCE_Loss: 0.6989 (0.6976)  time: 3.1048  data: 2.5363  max mem: 6498\n",
      "Epoch: [1]  [19/56]  eta: 0:01:50  lr: 0.000000  loss: 0.6989 (0.6980)  BCE_Loss: 0.6989 (0.6980)  time: 2.9781  data: 2.4095  max mem: 6498\n",
      "Epoch: [1]  [20/56]  eta: 0:01:59  lr: 0.000000  loss: 0.6995 (0.6984)  BCE_Loss: 0.6995 (0.6984)  time: 2.8416  data: 2.2730  max mem: 6498\n",
      "Epoch: [1]  [21/56]  eta: 0:01:52  lr: 0.000000  loss: 0.6995 (0.6982)  BCE_Loss: 0.6995 (0.6982)  time: 2.8414  data: 2.2730  max mem: 6498\n",
      "Epoch: [1]  [22/56]  eta: 0:01:45  lr: 0.000000  loss: 0.6995 (0.6984)  BCE_Loss: 0.6995 (0.6984)  time: 2.8415  data: 2.2730  max mem: 6498\n",
      "Epoch: [1]  [23/56]  eta: 0:01:38  lr: 0.000000  loss: 0.6995 (0.6984)  BCE_Loss: 0.6995 (0.6984)  time: 2.8415  data: 2.2730  max mem: 6498\n",
      "Epoch: [1]  [24/56]  eta: 0:01:45  lr: 0.000000  loss: 0.6995 (0.6981)  BCE_Loss: 0.6995 (0.6981)  time: 2.8499  data: 2.2814  max mem: 6498\n",
      "Epoch: [1]  [25/56]  eta: 0:01:38  lr: 0.000000  loss: 0.6989 (0.6977)  BCE_Loss: 0.6989 (0.6977)  time: 2.8499  data: 2.2814  max mem: 6498\n",
      "Epoch: [1]  [26/56]  eta: 0:01:32  lr: 0.000000  loss: 0.6985 (0.6974)  BCE_Loss: 0.6985 (0.6974)  time: 2.8500  data: 2.2814  max mem: 6498\n",
      "Epoch: [1]  [27/56]  eta: 0:01:26  lr: 0.000000  loss: 0.6985 (0.6973)  BCE_Loss: 0.6985 (0.6973)  time: 2.8500  data: 2.2814  max mem: 6498\n",
      "Epoch: [1]  [28/56]  eta: 0:01:30  lr: 0.000000  loss: 0.6985 (0.6970)  BCE_Loss: 0.6985 (0.6970)  time: 2.8621  data: 2.2935  max mem: 6498\n",
      "Epoch: [1]  [29/56]  eta: 0:01:24  lr: 0.000000  loss: 0.6989 (0.6972)  BCE_Loss: 0.6989 (0.6972)  time: 2.8621  data: 2.2935  max mem: 6498\n",
      "Epoch: [1]  [30/56]  eta: 0:01:19  lr: 0.000000  loss: 0.6989 (0.6973)  BCE_Loss: 0.6989 (0.6973)  time: 2.8620  data: 2.2935  max mem: 6498\n",
      "Epoch: [1]  [31/56]  eta: 0:01:14  lr: 0.000000  loss: 0.6989 (0.6972)  BCE_Loss: 0.6989 (0.6972)  time: 2.8621  data: 2.2935  max mem: 6498\n",
      "Epoch: [1]  [32/56]  eta: 0:01:15  lr: 0.000000  loss: 0.6985 (0.6970)  BCE_Loss: 0.6985 (0.6970)  time: 2.8267  data: 2.2582  max mem: 6498\n",
      "Epoch: [1]  [33/56]  eta: 0:01:10  lr: 0.000000  loss: 0.6985 (0.6972)  BCE_Loss: 0.6985 (0.6972)  time: 2.8266  data: 2.2582  max mem: 6498\n",
      "Epoch: [1]  [34/56]  eta: 0:01:06  lr: 0.000000  loss: 0.6985 (0.6977)  BCE_Loss: 0.6985 (0.6977)  time: 2.8266  data: 2.2582  max mem: 6498\n",
      "Epoch: [1]  [35/56]  eta: 0:01:01  lr: 0.000000  loss: 0.6971 (0.6976)  BCE_Loss: 0.6971 (0.6976)  time: 2.8265  data: 2.2582  max mem: 6498\n",
      "Epoch: [1]  [36/56]  eta: 0:01:02  lr: 0.000000  loss: 0.6971 (0.6977)  BCE_Loss: 0.6971 (0.6977)  time: 2.8704  data: 2.3021  max mem: 6498\n",
      "Epoch: [1]  [37/56]  eta: 0:00:57  lr: 0.000000  loss: 0.6971 (0.6981)  BCE_Loss: 0.6971 (0.6981)  time: 2.8703  data: 2.3021  max mem: 6498\n",
      "Epoch: [1]  [38/56]  eta: 0:00:53  lr: 0.000000  loss: 0.6952 (0.6979)  BCE_Loss: 0.6952 (0.6979)  time: 2.8703  data: 2.3021  max mem: 6498\n",
      "Epoch: [1]  [39/56]  eta: 0:00:49  lr: 0.000000  loss: 0.6940 (0.6976)  BCE_Loss: 0.6940 (0.6976)  time: 2.8703  data: 2.3021  max mem: 6498\n",
      "Epoch: [1]  [40/56]  eta: 0:00:49  lr: 0.000000  loss: 0.6939 (0.6975)  BCE_Loss: 0.6939 (0.6975)  time: 2.8181  data: 2.2499  max mem: 6498\n",
      "Epoch: [1]  [41/56]  eta: 0:00:45  lr: 0.000000  loss: 0.6940 (0.6974)  BCE_Loss: 0.6940 (0.6974)  time: 2.8183  data: 2.2499  max mem: 6498\n",
      "Epoch: [1]  [42/56]  eta: 0:00:41  lr: 0.000000  loss: 0.6940 (0.6974)  BCE_Loss: 0.6940 (0.6974)  time: 2.8182  data: 2.2499  max mem: 6498\n",
      "Epoch: [1]  [43/56]  eta: 0:00:37  lr: 0.000000  loss: 0.6940 (0.6975)  BCE_Loss: 0.6940 (0.6975)  time: 2.8182  data: 2.2499  max mem: 6498\n",
      "Epoch: [1]  [44/56]  eta: 0:00:36  lr: 0.000000  loss: 0.6952 (0.6975)  BCE_Loss: 0.6952 (0.6975)  time: 2.7235  data: 2.1552  max mem: 6498\n",
      "Epoch: [1]  [45/56]  eta: 0:00:32  lr: 0.000000  loss: 0.6958 (0.6976)  BCE_Loss: 0.6958 (0.6976)  time: 2.7236  data: 2.1552  max mem: 6498\n",
      "Epoch: [1]  [46/56]  eta: 0:00:29  lr: 0.000000  loss: 0.6958 (0.6976)  BCE_Loss: 0.6958 (0.6976)  time: 2.7236  data: 2.1552  max mem: 6498\n",
      "Epoch: [1]  [47/56]  eta: 0:00:25  lr: 0.000000  loss: 0.6972 (0.6976)  BCE_Loss: 0.6972 (0.6976)  time: 2.7236  data: 2.1552  max mem: 6498\n",
      "Epoch: [1]  [48/56]  eta: 0:00:24  lr: 0.000000  loss: 0.6973 (0.6976)  BCE_Loss: 0.6973 (0.6976)  time: 2.7482  data: 2.1798  max mem: 6498\n",
      "Epoch: [1]  [49/56]  eta: 0:00:20  lr: 0.000000  loss: 0.6973 (0.6977)  BCE_Loss: 0.6973 (0.6977)  time: 2.7482  data: 2.1798  max mem: 6498\n",
      "Epoch: [1]  [50/56]  eta: 0:00:17  lr: 0.000000  loss: 0.6972 (0.6976)  BCE_Loss: 0.6972 (0.6976)  time: 2.7482  data: 2.1798  max mem: 6498\n",
      "Epoch: [1]  [51/56]  eta: 0:00:14  lr: 0.000000  loss: 0.6972 (0.6976)  BCE_Loss: 0.6972 (0.6976)  time: 2.7481  data: 2.1798  max mem: 6498\n",
      "Epoch: [1]  [52/56]  eta: 0:00:12  lr: 0.000000  loss: 0.6973 (0.6976)  BCE_Loss: 0.6973 (0.6976)  time: 2.7620  data: 2.1937  max mem: 6498\n",
      "Epoch: [1]  [53/56]  eta: 0:00:08  lr: 0.000000  loss: 0.6972 (0.6974)  BCE_Loss: 0.6972 (0.6974)  time: 2.7620  data: 2.1937  max mem: 6498\n",
      "Epoch: [1]  [54/56]  eta: 0:00:05  lr: 0.000000  loss: 0.6958 (0.6974)  BCE_Loss: 0.6958 (0.6974)  time: 2.7619  data: 2.1937  max mem: 6498\n",
      "Epoch: [1]  [55/56]  eta: 0:00:02  lr: 0.000000  loss: 0.6959 (0.6974)  BCE_Loss: 0.6959 (0.6974)  time: 2.7620  data: 2.1937  max mem: 6498\n",
      "Epoch: [1] Total time: 0:02:41 (2.8807 s / it)\n",
      "Averaged train_stats:  {'lr': 0.0, 'loss': 0.697353, 'BCE_Loss': 0.697353}\n",
      "Valid:  [0/6]  eta: 0:01:08  loss: 0.6807 (0.6807)  BCE_Loss: 0.6807 (0.6807)  time: 11.4706  data: 10.9348  max mem: 6498\n",
      "Valid:  [1/6]  eta: 0:00:30  loss: 0.6807 (0.6809)  BCE_Loss: 0.6807 (0.6809)  time: 6.0025  data: 5.4674  max mem: 6498\n",
      "Valid:  [2/6]  eta: 0:00:16  loss: 0.6812 (0.6894)  BCE_Loss: 0.6812 (0.6894)  time: 4.1792  data: 3.6450  max mem: 6498\n",
      "Valid:  [3/6]  eta: 0:00:09  loss: 0.6812 (0.6937)  BCE_Loss: 0.6812 (0.6937)  time: 3.2679  data: 2.7337  max mem: 6498\n",
      "Valid:  [4/6]  eta: 0:00:06  loss: 0.7064 (0.6966)  BCE_Loss: 0.7064 (0.6966)  time: 3.2758  data: 2.7421  max mem: 6498\n",
      "Valid:  [5/6]  eta: 0:00:02  loss: 0.7064 (0.6983)  BCE_Loss: 0.7064 (0.6983)  time: 2.7482  data: 2.2851  max mem: 6498\n",
      "Valid: Total time: 0:00:16 (2.7630 s / it)\n",
      "Averaged valid_stats:  {'loss': 0.6982528, 'BCE_Loss': 0.6982528, 'auc': 0.4990412, 'f1': 0.0203046, 'acc': 0.3814103, 'sen': 0.0102564, 'spe': 1.0}\n",
      "Freeze encoder ...!\n",
      "Epoch: [2]  [ 0/56]  eta: 0:12:03  lr: 0.000050  loss: 0.6893 (0.6893)  BCE_Loss: 0.6893 (0.6893)  time: 12.9214  data: 12.3494  max mem: 6498\n",
      "Epoch: [2]  [ 1/56]  eta: 0:06:10  lr: 0.000050  loss: 0.6828 (0.6861)  BCE_Loss: 0.6828 (0.6861)  time: 6.7450  data: 6.1747  max mem: 6498\n",
      "Epoch: [2]  [ 2/56]  eta: 0:04:13  lr: 0.000050  loss: 0.6828 (0.6847)  BCE_Loss: 0.6828 (0.6847)  time: 4.6874  data: 4.1165  max mem: 6498\n",
      "Epoch: [2]  [ 3/56]  eta: 0:03:13  lr: 0.000050  loss: 0.6828 (0.6853)  BCE_Loss: 0.6828 (0.6853)  time: 3.6590  data: 3.0875  max mem: 6498\n",
      "Epoch: [2]  [ 4/56]  eta: 0:04:05  lr: 0.000050  loss: 0.6828 (0.6831)  BCE_Loss: 0.6828 (0.6831)  time: 4.7290  data: 4.1583  max mem: 6498\n",
      "Epoch: [2]  [ 5/56]  eta: 0:03:28  lr: 0.000050  loss: 0.6828 (0.6852)  BCE_Loss: 0.6828 (0.6852)  time: 4.0952  data: 3.5251  max mem: 6498\n",
      "Epoch: [2]  [ 6/56]  eta: 0:02:59  lr: 0.000050  loss: 0.6872 (0.6889)  BCE_Loss: 0.6872 (0.6889)  time: 3.5915  data: 3.0215  max mem: 6498\n",
      "Epoch: [2]  [ 7/56]  eta: 0:02:37  lr: 0.000050  loss: 0.6872 (0.6918)  BCE_Loss: 0.6872 (0.6918)  time: 3.2138  data: 2.6438  max mem: 6498\n",
      "Epoch: [2]  [ 8/56]  eta: 0:03:03  lr: 0.000050  loss: 0.6872 (0.6905)  BCE_Loss: 0.6872 (0.6905)  time: 3.8194  data: 3.2497  max mem: 6498\n",
      "Epoch: [2]  [ 9/56]  eta: 0:02:45  lr: 0.000050  loss: 0.6828 (0.6888)  BCE_Loss: 0.6828 (0.6888)  time: 3.5280  data: 2.9584  max mem: 6498\n",
      "Epoch: [2]  [10/56]  eta: 0:02:29  lr: 0.000050  loss: 0.6872 (0.6903)  BCE_Loss: 0.6872 (0.6903)  time: 3.2590  data: 2.6895  max mem: 6498\n",
      "Epoch: [2]  [11/56]  eta: 0:02:16  lr: 0.000050  loss: 0.6847 (0.6898)  BCE_Loss: 0.6847 (0.6898)  time: 3.0348  data: 2.4653  max mem: 6498\n",
      "Epoch: [2]  [12/56]  eta: 0:02:35  lr: 0.000050  loss: 0.6872 (0.6908)  BCE_Loss: 0.6872 (0.6908)  time: 3.5229  data: 2.9535  max mem: 6498\n",
      "Epoch: [2]  [13/56]  eta: 0:02:24  lr: 0.000050  loss: 0.6849 (0.6904)  BCE_Loss: 0.6849 (0.6904)  time: 3.3500  data: 2.7806  max mem: 6498\n",
      "Epoch: [2]  [14/56]  eta: 0:02:12  lr: 0.000050  loss: 0.6856 (0.6901)  BCE_Loss: 0.6856 (0.6901)  time: 3.1647  data: 2.5952  max mem: 6498\n",
      "Epoch: [2]  [15/56]  eta: 0:02:03  lr: 0.000050  loss: 0.6849 (0.6897)  BCE_Loss: 0.6849 (0.6897)  time: 3.0025  data: 2.4330  max mem: 6498\n",
      "Epoch: [2]  [16/56]  eta: 0:02:15  lr: 0.000050  loss: 0.6856 (0.6903)  BCE_Loss: 0.6856 (0.6903)  time: 3.3845  data: 2.8151  max mem: 6498\n",
      "Epoch: [2]  [17/56]  eta: 0:02:06  lr: 0.000050  loss: 0.6856 (0.6905)  BCE_Loss: 0.6856 (0.6905)  time: 3.2335  data: 2.6642  max mem: 6498\n",
      "Epoch: [2]  [18/56]  eta: 0:01:57  lr: 0.000050  loss: 0.6856 (0.6898)  BCE_Loss: 0.6856 (0.6898)  time: 3.0932  data: 2.5239  max mem: 6498\n",
      "Epoch: [2]  [19/56]  eta: 0:01:49  lr: 0.000050  loss: 0.6856 (0.6901)  BCE_Loss: 0.6856 (0.6901)  time: 2.9670  data: 2.3978  max mem: 6498\n",
      "Epoch: [2]  [20/56]  eta: 0:01:57  lr: 0.000050  loss: 0.6856 (0.6899)  BCE_Loss: 0.6856 (0.6899)  time: 2.7854  data: 2.2161  max mem: 6498\n",
      "Epoch: [2]  [21/56]  eta: 0:01:51  lr: 0.000050  loss: 0.6858 (0.6902)  BCE_Loss: 0.6858 (0.6902)  time: 2.8351  data: 2.2658  max mem: 6498\n",
      "Epoch: [2]  [22/56]  eta: 0:01:44  lr: 0.000050  loss: 0.6858 (0.6899)  BCE_Loss: 0.6858 (0.6899)  time: 2.8348  data: 2.2658  max mem: 6498\n",
      "Epoch: [2]  [23/56]  eta: 0:01:38  lr: 0.000050  loss: 0.6858 (0.6909)  BCE_Loss: 0.6858 (0.6909)  time: 2.8346  data: 2.2658  max mem: 6498\n",
      "Epoch: [2]  [24/56]  eta: 0:01:41  lr: 0.000050  loss: 0.6858 (0.6906)  BCE_Loss: 0.6858 (0.6906)  time: 2.7912  data: 2.2223  max mem: 6498\n",
      "Epoch: [2]  [25/56]  eta: 0:01:36  lr: 0.000050  loss: 0.6856 (0.6903)  BCE_Loss: 0.6856 (0.6903)  time: 2.8339  data: 2.2650  max mem: 6498\n",
      "Epoch: [2]  [26/56]  eta: 0:01:31  lr: 0.000050  loss: 0.6849 (0.6901)  BCE_Loss: 0.6849 (0.6901)  time: 2.8418  data: 2.2729  max mem: 6498\n",
      "Epoch: [2]  [27/56]  eta: 0:01:25  lr: 0.000050  loss: 0.6849 (0.6908)  BCE_Loss: 0.6849 (0.6908)  time: 2.8418  data: 2.2729  max mem: 6498\n",
      "Epoch: [2]  [28/56]  eta: 0:01:27  lr: 0.000050  loss: 0.6853 (0.6906)  BCE_Loss: 0.6853 (0.6906)  time: 2.7957  data: 2.2268  max mem: 6498\n",
      "Epoch: [2]  [29/56]  eta: 0:01:23  lr: 0.000050  loss: 0.6856 (0.6912)  BCE_Loss: 0.6856 (0.6912)  time: 2.9026  data: 2.3338  max mem: 6498\n",
      "Epoch: [2]  [30/56]  eta: 0:01:18  lr: 0.000050  loss: 0.6856 (0.6914)  BCE_Loss: 0.6856 (0.6914)  time: 2.9027  data: 2.3338  max mem: 6498\n",
      "Epoch: [2]  [31/56]  eta: 0:01:13  lr: 0.000050  loss: 0.6856 (0.6908)  BCE_Loss: 0.6856 (0.6908)  time: 2.9026  data: 2.3338  max mem: 6498\n",
      "Epoch: [2]  [32/56]  eta: 0:01:13  lr: 0.000050  loss: 0.6853 (0.6903)  BCE_Loss: 0.6853 (0.6903)  time: 2.7493  data: 2.1804  max mem: 6498\n",
      "Epoch: [2]  [33/56]  eta: 0:01:11  lr: 0.000050  loss: 0.6856 (0.6905)  BCE_Loss: 0.6856 (0.6905)  time: 2.9082  data: 2.3394  max mem: 6498\n",
      "Epoch: [2]  [34/56]  eta: 0:01:06  lr: 0.000050  loss: 0.6858 (0.6905)  BCE_Loss: 0.6858 (0.6905)  time: 2.9082  data: 2.3394  max mem: 6498\n",
      "Epoch: [2]  [35/56]  eta: 0:01:01  lr: 0.000050  loss: 0.6858 (0.6904)  BCE_Loss: 0.6858 (0.6904)  time: 2.9081  data: 2.3394  max mem: 6498\n",
      "Epoch: [2]  [36/56]  eta: 0:01:00  lr: 0.000050  loss: 0.6853 (0.6901)  BCE_Loss: 0.6853 (0.6901)  time: 2.7197  data: 2.1511  max mem: 6498\n",
      "Epoch: [2]  [37/56]  eta: 0:00:58  lr: 0.000050  loss: 0.6850 (0.6899)  BCE_Loss: 0.6850 (0.6899)  time: 2.9471  data: 2.3785  max mem: 6498\n",
      "Epoch: [2]  [38/56]  eta: 0:00:54  lr: 0.000050  loss: 0.6853 (0.6899)  BCE_Loss: 0.6853 (0.6899)  time: 2.9471  data: 2.3785  max mem: 6498\n",
      "Epoch: [2]  [39/56]  eta: 0:00:50  lr: 0.000050  loss: 0.6850 (0.6897)  BCE_Loss: 0.6850 (0.6897)  time: 2.9471  data: 2.3785  max mem: 6498\n",
      "Epoch: [2]  [40/56]  eta: 0:00:47  lr: 0.000050  loss: 0.6850 (0.6897)  BCE_Loss: 0.6850 (0.6897)  time: 2.6782  data: 2.1099  max mem: 6498\n",
      "Epoch: [2]  [41/56]  eta: 0:00:45  lr: 0.000050  loss: 0.6850 (0.6900)  BCE_Loss: 0.6850 (0.6900)  time: 2.8905  data: 2.3221  max mem: 6498\n",
      "Epoch: [2]  [42/56]  eta: 0:00:41  lr: 0.000050  loss: 0.6853 (0.6901)  BCE_Loss: 0.6853 (0.6901)  time: 2.8907  data: 2.3221  max mem: 6498\n",
      "Epoch: [2]  [43/56]  eta: 0:00:38  lr: 0.000050  loss: 0.6850 (0.6899)  BCE_Loss: 0.6850 (0.6899)  time: 2.8907  data: 2.3221  max mem: 6498\n",
      "Epoch: [2]  [44/56]  eta: 0:00:35  lr: 0.000050  loss: 0.6853 (0.6901)  BCE_Loss: 0.6853 (0.6901)  time: 2.6717  data: 2.1031  max mem: 6498\n",
      "Epoch: [2]  [45/56]  eta: 0:00:33  lr: 0.000050  loss: 0.6853 (0.6897)  BCE_Loss: 0.6853 (0.6897)  time: 2.9346  data: 2.3659  max mem: 6498\n",
      "Epoch: [2]  [46/56]  eta: 0:00:29  lr: 0.000050  loss: 0.6895 (0.6903)  BCE_Loss: 0.6895 (0.6903)  time: 2.9266  data: 2.3580  max mem: 6498\n",
      "Epoch: [2]  [47/56]  eta: 0:00:26  lr: 0.000050  loss: 0.6895 (0.6906)  BCE_Loss: 0.6895 (0.6906)  time: 2.9267  data: 2.3580  max mem: 6498\n",
      "Epoch: [2]  [48/56]  eta: 0:00:23  lr: 0.000050  loss: 0.6906 (0.6909)  BCE_Loss: 0.6906 (0.6909)  time: 2.6575  data: 2.0888  max mem: 6498\n",
      "Epoch: [2]  [49/56]  eta: 0:00:21  lr: 0.000050  loss: 0.6906 (0.6911)  BCE_Loss: 0.6906 (0.6911)  time: 2.8654  data: 2.2966  max mem: 6498\n",
      "Epoch: [2]  [50/56]  eta: 0:00:17  lr: 0.000050  loss: 0.6895 (0.6910)  BCE_Loss: 0.6895 (0.6910)  time: 2.8654  data: 2.2966  max mem: 6498\n",
      "Epoch: [2]  [51/56]  eta: 0:00:14  lr: 0.000050  loss: 0.6895 (0.6906)  BCE_Loss: 0.6895 (0.6906)  time: 2.8654  data: 2.2966  max mem: 6498\n",
      "Epoch: [2]  [52/56]  eta: 0:00:11  lr: 0.000050  loss: 0.6895 (0.6905)  BCE_Loss: 0.6895 (0.6905)  time: 2.6757  data: 2.1068  max mem: 6498\n",
      "Epoch: [2]  [53/56]  eta: 0:00:08  lr: 0.000050  loss: 0.6895 (0.6905)  BCE_Loss: 0.6895 (0.6905)  time: 2.7984  data: 2.2295  max mem: 6498\n",
      "Epoch: [2]  [54/56]  eta: 0:00:05  lr: 0.000050  loss: 0.6895 (0.6905)  BCE_Loss: 0.6895 (0.6905)  time: 2.8056  data: 2.2367  max mem: 6498\n",
      "Epoch: [2]  [55/56]  eta: 0:00:02  lr: 0.000050  loss: 0.6895 (0.6904)  BCE_Loss: 0.6895 (0.6904)  time: 2.8056  data: 2.2367  max mem: 6498\n",
      "Epoch: [2] Total time: 0:02:42 (2.9008 s / it)\n",
      "Averaged train_stats:  {'lr': 5e-05, 'loss': 0.6904379, 'BCE_Loss': 0.6904379}\n",
      "Valid:  [0/6]  eta: 0:01:07  loss: 0.6519 (0.6519)  BCE_Loss: 0.6519 (0.6519)  time: 11.3067  data: 10.7727  max mem: 6498\n",
      "Valid:  [1/6]  eta: 0:00:29  loss: 0.6519 (0.6540)  BCE_Loss: 0.6519 (0.6540)  time: 5.9196  data: 5.3864  max mem: 6498\n",
      "Valid:  [2/6]  eta: 0:00:16  loss: 0.6561 (0.6733)  BCE_Loss: 0.6561 (0.6733)  time: 4.1240  data: 3.5909  max mem: 6498\n",
      "Valid:  [3/6]  eta: 0:00:09  loss: 0.6561 (0.6827)  BCE_Loss: 0.6561 (0.6827)  time: 3.2264  data: 2.6932  max mem: 6498\n",
      "Valid:  [4/6]  eta: 0:00:06  loss: 0.7059 (0.6873)  BCE_Loss: 0.7059 (0.6873)  time: 3.2457  data: 2.7126  max mem: 6498\n",
      "Valid:  [5/6]  eta: 0:00:02  loss: 0.7001 (0.6895)  BCE_Loss: 0.7001 (0.6895)  time: 2.7231  data: 2.2605  max mem: 6498\n",
      "Valid: Total time: 0:00:16 (2.7332 s / it)\n",
      "Averaged valid_stats:  {'loss': 0.6894615, 'BCE_Loss': 0.6894615, 'auc': 0.7050186, 'f1': 0.4503817, 'acc': 0.5384616, 'sen': 0.3025641, 'spe': 0.9316239}\n",
      "Freeze encoder ...!\n",
      "Epoch: [3]  [ 0/56]  eta: 0:11:14  lr: 0.000100  loss: 0.6791 (0.6791)  BCE_Loss: 0.6791 (0.6791)  time: 12.0360  data: 11.4640  max mem: 6498\n",
      "Epoch: [3]  [ 1/56]  eta: 0:05:46  lr: 0.000100  loss: 0.6744 (0.6768)  BCE_Loss: 0.6744 (0.6768)  time: 6.3032  data: 5.7321  max mem: 6498\n",
      "Epoch: [3]  [ 2/56]  eta: 0:03:57  lr: 0.000100  loss: 0.6791 (0.6855)  BCE_Loss: 0.6791 (0.6855)  time: 4.3913  data: 3.8214  max mem: 6498\n",
      "Epoch: [3]  [ 3/56]  eta: 0:03:02  lr: 0.000100  loss: 0.6791 (0.6889)  BCE_Loss: 0.6791 (0.6889)  time: 3.4356  data: 2.8661  max mem: 6498\n",
      "Epoch: [3]  [ 4/56]  eta: 0:04:04  lr: 0.000100  loss: 0.6980 (0.6907)  BCE_Loss: 0.6980 (0.6907)  time: 4.6953  data: 4.1259  max mem: 6498\n",
      "Epoch: [3]  [ 5/56]  eta: 0:03:24  lr: 0.000100  loss: 0.6936 (0.6912)  BCE_Loss: 0.6936 (0.6912)  time: 4.0075  data: 3.4383  max mem: 6498\n",
      "Epoch: [3]  [ 6/56]  eta: 0:02:55  lr: 0.000100  loss: 0.6938 (0.6915)  BCE_Loss: 0.6938 (0.6915)  time: 3.5162  data: 2.9471  max mem: 6498\n",
      "Epoch: [3]  [ 7/56]  eta: 0:02:34  lr: 0.000100  loss: 0.6936 (0.6905)  BCE_Loss: 0.6936 (0.6905)  time: 3.1478  data: 2.5787  max mem: 6498\n",
      "Epoch: [3]  [ 8/56]  eta: 0:03:00  lr: 0.000100  loss: 0.6936 (0.6903)  BCE_Loss: 0.6936 (0.6903)  time: 3.7588  data: 3.1895  max mem: 6498\n",
      "Epoch: [3]  [ 9/56]  eta: 0:02:41  lr: 0.000100  loss: 0.6892 (0.6900)  BCE_Loss: 0.6892 (0.6900)  time: 3.4397  data: 2.8706  max mem: 6498\n",
      "Epoch: [3]  [10/56]  eta: 0:02:26  lr: 0.000100  loss: 0.6936 (0.6907)  BCE_Loss: 0.6936 (0.6907)  time: 3.1788  data: 2.6096  max mem: 6498\n",
      "Epoch: [3]  [11/56]  eta: 0:02:13  lr: 0.000100  loss: 0.6936 (0.6916)  BCE_Loss: 0.6936 (0.6916)  time: 2.9613  data: 2.3922  max mem: 6498\n",
      "Epoch: [3]  [12/56]  eta: 0:02:30  lr: 0.000100  loss: 0.6936 (0.6907)  BCE_Loss: 0.6936 (0.6907)  time: 3.4144  data: 2.8454  max mem: 6498\n",
      "Epoch: [3]  [13/56]  eta: 0:02:18  lr: 0.000100  loss: 0.6892 (0.6891)  BCE_Loss: 0.6892 (0.6891)  time: 3.2112  data: 2.6422  max mem: 6498\n",
      "Epoch: [3]  [14/56]  eta: 0:02:07  lr: 0.000100  loss: 0.6936 (0.6909)  BCE_Loss: 0.6936 (0.6909)  time: 3.0350  data: 2.4660  max mem: 6498\n",
      "Epoch: [3]  [15/56]  eta: 0:01:58  lr: 0.000100  loss: 0.6905 (0.6909)  BCE_Loss: 0.6905 (0.6909)  time: 2.8809  data: 2.3119  max mem: 6498\n",
      "Epoch: [3]  [16/56]  eta: 0:02:08  lr: 0.000100  loss: 0.6905 (0.6900)  BCE_Loss: 0.6905 (0.6900)  time: 3.2221  data: 2.6532  max mem: 6498\n",
      "Epoch: [3]  [17/56]  eta: 0:02:01  lr: 0.000100  loss: 0.6905 (0.6911)  BCE_Loss: 0.6905 (0.6911)  time: 3.1220  data: 2.5531  max mem: 6498\n",
      "Epoch: [3]  [18/56]  eta: 0:01:53  lr: 0.000100  loss: 0.6905 (0.6908)  BCE_Loss: 0.6905 (0.6908)  time: 2.9876  data: 2.4187  max mem: 6498\n",
      "Epoch: [3]  [19/56]  eta: 0:01:46  lr: 0.000100  loss: 0.6905 (0.6912)  BCE_Loss: 0.6905 (0.6912)  time: 2.8779  data: 2.3090  max mem: 6498\n",
      "Epoch: [3]  [20/56]  eta: 0:01:53  lr: 0.000100  loss: 0.6905 (0.6911)  BCE_Loss: 0.6905 (0.6911)  time: 2.7150  data: 2.1463  max mem: 6498\n",
      "Epoch: [3]  [21/56]  eta: 0:01:46  lr: 0.000100  loss: 0.6936 (0.6915)  BCE_Loss: 0.6936 (0.6915)  time: 2.7150  data: 2.1463  max mem: 6498\n",
      "Epoch: [3]  [22/56]  eta: 0:01:39  lr: 0.000100  loss: 0.6905 (0.6910)  BCE_Loss: 0.6905 (0.6910)  time: 2.7150  data: 2.1463  max mem: 6498\n",
      "Epoch: [3]  [23/56]  eta: 0:01:34  lr: 0.000100  loss: 0.6905 (0.6910)  BCE_Loss: 0.6905 (0.6910)  time: 2.7352  data: 2.1666  max mem: 6498\n",
      "Epoch: [3]  [24/56]  eta: 0:01:38  lr: 0.000100  loss: 0.6905 (0.6912)  BCE_Loss: 0.6905 (0.6912)  time: 2.6740  data: 2.1054  max mem: 6498\n",
      "Epoch: [3]  [25/56]  eta: 0:01:32  lr: 0.000100  loss: 0.6899 (0.6904)  BCE_Loss: 0.6899 (0.6904)  time: 2.6740  data: 2.1054  max mem: 6498\n",
      "Epoch: [3]  [26/56]  eta: 0:01:27  lr: 0.000100  loss: 0.6892 (0.6899)  BCE_Loss: 0.6892 (0.6899)  time: 2.7178  data: 2.1492  max mem: 6498\n",
      "Epoch: [3]  [27/56]  eta: 0:01:22  lr: 0.000100  loss: 0.6892 (0.6892)  BCE_Loss: 0.6892 (0.6892)  time: 2.7294  data: 2.1608  max mem: 6498\n",
      "Epoch: [3]  [28/56]  eta: 0:01:25  lr: 0.000100  loss: 0.6899 (0.6896)  BCE_Loss: 0.6899 (0.6896)  time: 2.7182  data: 2.1497  max mem: 6498\n",
      "Epoch: [3]  [29/56]  eta: 0:01:19  lr: 0.000100  loss: 0.6905 (0.6902)  BCE_Loss: 0.6905 (0.6902)  time: 2.7182  data: 2.1497  max mem: 6498\n",
      "Epoch: [3]  [30/56]  eta: 0:01:16  lr: 0.000100  loss: 0.6905 (0.6903)  BCE_Loss: 0.6905 (0.6903)  time: 2.7958  data: 2.2273  max mem: 6498\n",
      "Epoch: [3]  [31/56]  eta: 0:01:11  lr: 0.000100  loss: 0.6905 (0.6905)  BCE_Loss: 0.6905 (0.6905)  time: 2.7958  data: 2.2273  max mem: 6498\n",
      "Epoch: [3]  [32/56]  eta: 0:01:11  lr: 0.000100  loss: 0.6924 (0.6911)  BCE_Loss: 0.6924 (0.6911)  time: 2.7304  data: 2.1617  max mem: 6498\n",
      "Epoch: [3]  [33/56]  eta: 0:01:07  lr: 0.000100  loss: 0.6943 (0.6914)  BCE_Loss: 0.6943 (0.6914)  time: 2.7303  data: 2.1617  max mem: 6498\n",
      "Epoch: [3]  [34/56]  eta: 0:01:04  lr: 0.000100  loss: 0.6924 (0.6913)  BCE_Loss: 0.6924 (0.6913)  time: 2.8536  data: 2.2851  max mem: 6498\n",
      "Epoch: [3]  [35/56]  eta: 0:01:00  lr: 0.000100  loss: 0.6924 (0.6913)  BCE_Loss: 0.6924 (0.6913)  time: 2.8535  data: 2.2851  max mem: 6498\n"
     ]
    }
   ],
   "source": [
    "!python train.py \\\n",
    "--data_set 'Pneumonia' \\\n",
    "--model_name 'Downtask_Pneumonia' \\\n",
    "--data_folder_dir \"/mnt/nas125_vol2/kanggilpark/child/jyp_child/data\" \\\n",
    "--batch-size 120 \\\n",
    "--num_workers 4 \\\n",
    "--multi-gpu-mode 'Single' \\\n",
    "--cuda-visible-devices '1' \\\n",
    "--gradual_unfreeze true \\\n",
    "--print-freq 1\\\n",
    "--output_dir outputssss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************************\n",
      "*           Training Mode is  Upstream        *\n",
      "***********************************************\n",
      "Dataset Folder Path:  /home/pkg777774/child_xray/fracture/jykim\n",
      "---------- Model ----------\n",
      "Resume From:  \n",
      "Output Save To:  outputs\n",
      "Visible GPUs:  2\n",
      "---------- Optimizer ----------\n",
      "Learning Rate:  0.0005\n",
      "Batchsize:  72\n",
      "Loading dataset ....\n",
      "Train [Total]  number =  1172545\n",
      "Valid [Total]  number =  390849\n",
      "Creating model  : Uptask_Sup_Classifier\n",
      "Pretrained model: \n",
      "Number of Learnable Params: 23534544\n",
      "ResNet(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=16, bias=True)\n",
      ")\n",
      "Start training for 500 epochs\n",
      "Epoch: [0]  [    0/16285]  eta: 3 days, 2:56:24  lr: 0.000000  loss: 2.6602 (2.6602)  CE_Loss: 2.6602 (2.6602)  time: 16.5664  data: 14.5702  max mem: 7830\n",
      "Epoch: [0]  [    1/16285]  eta: 1 day, 14:40:02  lr: 0.000000  loss: 2.6602 (2.7138)  CE_Loss: 2.6602 (2.7138)  time: 8.5484  data: 7.2852  max mem: 8106\n",
      "Epoch: [0]  [    2/16285]  eta: 1 day, 2:41:58  lr: 0.000000  loss: 2.6602 (2.6468)  CE_Loss: 2.6602 (2.6468)  time: 5.9030  data: 4.8569  max mem: 8106\n",
      "Epoch: [0]  [    3/16285]  eta: 20:42:25  lr: 0.000000  loss: 2.6334 (2.6435)  CE_Loss: 2.6334 (2.6435)  time: 4.5784  data: 3.6427  max mem: 8106\n",
      "Epoch: [0]  [    4/16285]  eta: 17:06:48  lr: 0.000000  loss: 2.6334 (2.6296)  CE_Loss: 2.6334 (2.6296)  time: 3.7841  data: 2.9142  max mem: 8106\n",
      "Epoch: [0]  [    5/16285]  eta: 14:42:08  lr: 0.000000  loss: 2.6334 (2.6466)  CE_Loss: 2.6334 (2.6466)  time: 3.2511  data: 2.4285  max mem: 8106\n",
      "Epoch: [0]  [    6/16285]  eta: 12:55:47  lr: 0.000000  loss: 2.6602 (2.6578)  CE_Loss: 2.6602 (2.6578)  time: 2.8594  data: 2.0816  max mem: 8106\n",
      "Epoch: [0]  [    7/16285]  eta: 11:39:53  lr: 0.000000  loss: 2.6408 (2.6556)  CE_Loss: 2.6408 (2.6556)  time: 2.5798  data: 1.8214  max mem: 8106\n"
     ]
    }
   ],
   "source": [
    "!python train.py \\\n",
    "--data_set 'PedXnet_Sup_16class' \\\n",
    "--training-stream 'Upstream' \\\n",
    "--model_name 'Uptask_Sup_Classifier' \\\n",
    "--data_folder_dir '/home/pkg777774/child_xray/fracture/jykim' \\\n",
    "--batch-size 72 \\\n",
    "--num_workers 8 \\\n",
    "--multi-gpu-mode 'Single' \\\n",
    "--cuda-visible-devices '2' \\\n",
    "--print-freq 1\\\n",
    "--output_dir outputs"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bdaf8225ca8361a4501e29f7f35fc36f6baec4997917c7c75ec5999a985d7c37"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('child_x')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
